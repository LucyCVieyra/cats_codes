{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMkCQfys0LI0cylritOUK/U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LucyCVieyra/cats_codes/blob/main/cats_pose_v03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Ruta al dataset en Google Drive\n",
        "DRIVE_DATASET_ROOT = \"/content/drive/MyDrive/cats_dataset\"\n",
        "print(\"DRIVE_DATASET_ROOT =\", DRIVE_DATASET_ROOT)\n"
      ],
      "metadata": {
        "id": "E5r9J_UISvUR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "337d78c6-da3a-4fc9-a1e0-bc269a4b3599"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "DRIVE_DATASET_ROOT = /content/drive/MyDrive/cats_dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "from typing import Tuple\n",
        "import shutil\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "\n",
        "# Copiar el dataset de Drive a disco local SOLO si no existe todavía\n",
        "LOCAL_DATASET_ROOT = \"/content/cats_dataset\"\n",
        "\n",
        "if not os.path.exists(LOCAL_DATASET_ROOT):\n",
        "    print(\"Copiando dataset de Drive a disco local (solo se hace una vez)...\")\n",
        "    shutil.copytree(DRIVE_DATASET_ROOT, LOCAL_DATASET_ROOT)\n",
        "    print(\"Copia terminada.\")\n",
        "else:\n",
        "    print(\"Ya existe copia local en\", LOCAL_DATASET_ROOT)\n",
        "\n",
        "# A partir de aquí usamos SIEMPRE la copia local\n",
        "DATASET_ROOT = LOCAL_DATASET_ROOT\n",
        "print(\"DATASET_ROOT =\", DATASET_ROOT)\n",
        "\n",
        "# Ruta al CSV dentro del dataset\n",
        "CSV_PATH = os.path.join(DATASET_ROOT, \"dataset.csv\")\n",
        "print(\"CSV_PATH:\", CSV_PATH)\n",
        "\n",
        "# Hiperparámetros\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 50\n",
        "LEARNING_RATE = 1e-4\n",
        "BETA_ROT = 100.0      # peso de la orientación en la pérdida\n",
        "IMAGE_SIZE = 224      # tamaño de entrada esperado para ResNet\n",
        "NUM_WORKERS = 0       # importante: 0 en Colab para evitar cuelgues\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Usando dispositivo:\", DEVICE)\n",
        "\n"
      ],
      "metadata": {
        "id": "a_iXwZ84PeaQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "478c0a95-141b-4acf-c3d5-762d1704a36b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copiando dataset de Drive a disco local (solo se hace una vez)...\n",
            "Copia terminada.\n",
            "DATASET_ROOT = /content/cats_dataset\n",
            "CSV_PATH: /content/cats_dataset/dataset.csv\n",
            "Usando dispositivo: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PoseDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Lee dataset.csv y carga imágenes + vector de pose (x,y,z,q0,q1,q2,q3)\n",
        "    Filtra por split: 'train', 'val' o 'test'.\n",
        "    \"\"\"\n",
        "    def __init__(self, csv_path: str, root_dir: str, split: str = \"train\", transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.samples = []\n",
        "\n",
        "        with open(csv_path, \"r\") as f:\n",
        "            reader = csv.DictReader(f)\n",
        "            for row in reader:\n",
        "                if row[\"split\"] != split:\n",
        "                    continue\n",
        "\n",
        "                image_path = row[\"image_path\"]               # p.ej. trayectoria_1/images/image_000001.png\n",
        "                full_image_path = os.path.join(root_dir, image_path)\n",
        "\n",
        "                pose = [\n",
        "                    float(row[\"x\"]),\n",
        "                    float(row[\"y\"]),\n",
        "                    float(row[\"z\"]),\n",
        "                    float(row[\"q0\"]),\n",
        "                    float(row[\"q1\"]),\n",
        "                    float(row[\"q2\"]),\n",
        "                    float(row[\"q3\"]),\n",
        "                ]\n",
        "\n",
        "                self.samples.append((full_image_path, pose))\n",
        "\n",
        "        if len(self.samples) == 0:\n",
        "            raise RuntimeError(f\"No se encontraron muestras para split={split}. Revisa dataset.csv\")\n",
        "\n",
        "        print(f\"Split '{split}': {len(self.samples)} muestras\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        img_path, pose = self.samples[idx]\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        pose = torch.tensor(pose, dtype=torch.float32)\n",
        "\n",
        "        return img, pose\n",
        "\n",
        "\n",
        "# Transforms (train y val)\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n"
      ],
      "metadata": {
        "id": "AxbgRFwZPmaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear datasets\n",
        "train_dataset = PoseDataset(CSV_PATH, DATASET_ROOT, split=\"train\", transform=train_transform)\n",
        "val_dataset   = PoseDataset(CSV_PATH, DATASET_ROOT, split=\"val\",   transform=val_transform)\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=False,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=False,\n",
        ")\n",
        "\n",
        "# Sanity check: probar que carga algunos batches\n",
        "import time\n",
        "\n",
        "start = time.time()\n",
        "for i, (images, poses) in enumerate(train_loader):\n",
        "    print(f\"Batch {i} -> images {images.shape}, poses {poses.shape}\")\n",
        "    if i == 2:\n",
        "        break\n",
        "end = time.time()\n",
        "print(\"Sanity check OK, tomó\", end - start, \"s\")\n",
        "\n",
        "# Ver un ejemplo de ruta y cargar una imagen para asegurarnos de que todo está bien\n",
        "example_path, _ = train_dataset.samples[0]\n",
        "print(\"Ejemplo de ruta de imagen:\", example_path)\n",
        "\n",
        "img = Image.open(example_path).convert(\"RGB\")\n",
        "display(img.resize((256, 256)))\n"
      ],
      "metadata": {
        "id": "g7I0y0caPrLE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "187334da-f54f-41f6-bb0b-1f390ef17328"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split 'train': 7209 muestras\n",
            "Split 'val': 2057 muestras\n",
            "Batch 0 -> images torch.Size([32, 3, 224, 224]), poses torch.Size([32, 7])\n",
            "Batch 1 -> images torch.Size([32, 3, 224, 224]), poses torch.Size([32, 7])\n",
            "Batch 2 -> images torch.Size([32, 3, 224, 224]), poses torch.Size([32, 7])\n",
            "Sanity check OK, tomó 0.8005256652832031 s\n",
            "Ejemplo de ruta de imagen: /content/cats_dataset/trayectoria_1/images/image_000000.png\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=256x256>"
            ],
            "image/png": "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\n",
            "image/jpeg": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PoseNetLight(nn.Module):\n",
        "    \"\"\"\n",
        "    PoseNet ligero:\n",
        "    - Backbone: ResNet18 preentrenada en ImageNet\n",
        "    - Cabeza de regresión: 7D (x,y,z,q0,q1,q2,q3)\n",
        "    \"\"\"\n",
        "    def __init__(self, pretrained=True):\n",
        "        super().__init__()\n",
        "        # ResNet18 con API nueva de torchvision\n",
        "        weights = models.ResNet18_Weights.DEFAULT if pretrained else None\n",
        "        backbone = models.resnet18(weights=weights)\n",
        "\n",
        "        modules = list(backbone.children())[:-1]  # quitar la FC\n",
        "        self.backbone = nn.Sequential(*modules)\n",
        "        self.backbone_out_dim = backbone.fc.in_features  # 512\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.backbone_out_dim, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(128, 7)  # x,y,z,q0,q1,q2,q3\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)        # (B, 512, 1, 1)\n",
        "        x = torch.flatten(x, 1)     # (B, 512)\n",
        "        pose = self.fc(x)           # (B, 7)\n",
        "        return pose\n",
        "\n",
        "\n",
        "def pose_loss(pred: torch.Tensor, target: torch.Tensor, beta: float = BETA_ROT) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    pred:   (B,7) [x,y,z,q0,q1,q2,q3]\n",
        "    target: (B,7)\n",
        "    \"\"\"\n",
        "    t_pred = pred[:, :3]\n",
        "    q_pred = pred[:, 3:]\n",
        "    t_gt   = target[:, :3]\n",
        "    q_gt   = target[:, 3:]\n",
        "\n",
        "    # Normalizar cuaternión predicho\n",
        "    q_pred = q_pred / (torch.norm(q_pred, p=2, dim=1, keepdim=True) + 1e-8)\n",
        "\n",
        "    t_loss = nn.functional.mse_loss(t_pred, t_gt)\n",
        "    q_loss = nn.functional.mse_loss(q_pred, q_gt)\n",
        "\n",
        "    return t_loss + beta * q_loss\n",
        "\n",
        "\n",
        "model = PoseNetLight(pretrained=True).to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
        "\n",
        "print(model)\n"
      ],
      "metadata": {
        "id": "7DWhiSdePxID",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02cc93b7-ba9a-423e-92dc-4ded57543859"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 144MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PoseNetLight(\n",
            "  (backbone): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (4): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (5): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (6): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (7): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  )\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Linear(in_features=128, out_features=7, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def train_one_epoch(model, dataloader, optimizer, device, epoch_idx=0, num_epochs=1):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    n_batches = len(dataloader)\n",
        "    start = time.time()\n",
        "\n",
        "    for i, (images, poses) in enumerate(dataloader):\n",
        "        images = images.to(device)\n",
        "        poses  = poses.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = pose_loss(outputs, poses)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        # Print cada 10 batches\n",
        "        if (i % 10) == 0:\n",
        "            elapsed = time.time() - start\n",
        "            print(f\"[Train][Época {epoch_idx}/{num_epochs}] \"\n",
        "                  f\"Batch {i}/{n_batches}  Loss batch: {loss.item():.6f}  \"\n",
        "                  f\"Tiempo transcurrido: {elapsed:.1f}s\")\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    total_time = time.time() - start\n",
        "    print(f\"[Train] Época {epoch_idx} terminada en {total_time:.1f}s  Loss media: {epoch_loss:.6f}\")\n",
        "    return epoch_loss\n",
        "\n",
        "\n",
        "def eval_one_epoch(model, dataloader, device, epoch_idx=0, num_epochs=1):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    n_batches = len(dataloader)\n",
        "    start = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (images, poses) in enumerate(dataloader):\n",
        "            images = images.to(device)\n",
        "            poses  = poses.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = pose_loss(outputs, poses)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "            if (i % 10) == 0:\n",
        "                elapsed = time.time() - start\n",
        "                print(f\"[Val][Época {epoch_idx}/{num_epochs}] \"\n",
        "                      f\"Batch {i}/{n_batches}  Loss batch: {loss.item():.6f}  \"\n",
        "                      f\"Tiempo transcurrido: {elapsed:.1f}s\")\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    total_time = time.time() - start\n",
        "    print(f\"[Val] Época {epoch_idx} terminada en {total_time:.1f}s  Loss media: {epoch_loss:.6f}\")\n",
        "    return epoch_loss\n"
      ],
      "metadata": {
        "id": "wVeOh4kSUvFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 50\n",
        "\n",
        "best_val_loss = float(\"inf\")\n",
        "best_model_path = os.path.join(DATASET_ROOT, \"posenet_light_best.pth\")\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Comenzando época {epoch}/{NUM_EPOCHS}\")\n",
        "\n",
        "    train_loss = train_one_epoch(model, train_loader, optimizer, DEVICE,\n",
        "                                 epoch_idx=epoch, num_epochs=NUM_EPOCHS)\n",
        "    val_loss   = eval_one_epoch(model, val_loader, DEVICE,\n",
        "                                epoch_idx=epoch, num_epochs=NUM_EPOCHS)\n",
        "\n",
        "    print(f\"Época [{epoch}/{NUM_EPOCHS}] \"\n",
        "          f\"Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save({\n",
        "            \"epoch\": epoch,\n",
        "            \"model_state_dict\": model.state_dict(),\n",
        "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "            \"val_loss\": val_loss,\n",
        "        }, best_model_path)\n",
        "        print(f\"  🟢 Nuevo mejor modelo guardado en: {best_model_path}\")\n",
        "\n",
        "print(\"Entrenamiento terminado. Mejor val_loss:\", best_val_loss)\n"
      ],
      "metadata": {
        "id": "zA0jZfZGU1FS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1aa83343-9059-48ec-a0f4-49e2505d2515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Comenzando época 1/50\n",
            "[Train][Época 1/50] Batch 0/226  Loss batch: 1.145146  Tiempo transcurrido: 0.3s\n",
            "[Train][Época 1/50] Batch 10/226  Loss batch: 1.377313  Tiempo transcurrido: 3.6s\n",
            "[Train][Época 1/50] Batch 20/226  Loss batch: 1.175146  Tiempo transcurrido: 7.1s\n",
            "[Train][Época 1/50] Batch 30/226  Loss batch: 1.153405  Tiempo transcurrido: 10.1s\n",
            "[Train][Época 1/50] Batch 40/226  Loss batch: 0.959599  Tiempo transcurrido: 13.2s\n",
            "[Train][Época 1/50] Batch 50/226  Loss batch: 0.923191  Tiempo transcurrido: 16.3s\n",
            "[Train][Época 1/50] Batch 60/226  Loss batch: 0.961839  Tiempo transcurrido: 20.0s\n",
            "[Train][Época 1/50] Batch 70/226  Loss batch: 0.964079  Tiempo transcurrido: 23.2s\n",
            "[Train][Época 1/50] Batch 80/226  Loss batch: 0.741173  Tiempo transcurrido: 26.2s\n",
            "[Train][Época 1/50] Batch 90/226  Loss batch: 0.989624  Tiempo transcurrido: 29.3s\n",
            "[Train][Época 1/50] Batch 100/226  Loss batch: 1.089430  Tiempo transcurrido: 33.0s\n",
            "[Train][Época 1/50] Batch 110/226  Loss batch: 0.758077  Tiempo transcurrido: 36.1s\n",
            "[Train][Época 1/50] Batch 120/226  Loss batch: 0.911495  Tiempo transcurrido: 39.1s\n",
            "[Train][Época 1/50] Batch 130/226  Loss batch: 1.172991  Tiempo transcurrido: 42.1s\n",
            "[Train][Época 1/50] Batch 140/226  Loss batch: 1.428859  Tiempo transcurrido: 46.0s\n",
            "[Train][Época 1/50] Batch 150/226  Loss batch: 0.994160  Tiempo transcurrido: 49.0s\n",
            "[Train][Época 1/50] Batch 160/226  Loss batch: 1.035761  Tiempo transcurrido: 52.6s\n",
            "[Train][Época 1/50] Batch 170/226  Loss batch: 0.975036  Tiempo transcurrido: 55.8s\n",
            "[Train][Época 1/50] Batch 180/226  Loss batch: 0.726066  Tiempo transcurrido: 59.4s\n",
            "[Train][Época 1/50] Batch 190/226  Loss batch: 0.798804  Tiempo transcurrido: 62.5s\n",
            "[Train][Época 1/50] Batch 200/226  Loss batch: 0.739815  Tiempo transcurrido: 65.6s\n",
            "[Train][Época 1/50] Batch 210/226  Loss batch: 1.060755  Tiempo transcurrido: 68.8s\n",
            "[Train][Época 1/50] Batch 220/226  Loss batch: 3.888996  Tiempo transcurrido: 72.4s\n",
            "[Train] Época 1 terminada en 73.7s  Loss media: 0.997445\n",
            "[Val][Época 1/50] Batch 0/65  Loss batch: 6.489799  Tiempo transcurrido: 0.2s\n",
            "[Val][Época 1/50] Batch 10/65  Loss batch: 6.472812  Tiempo transcurrido: 2.6s\n",
            "[Val][Época 1/50] Batch 20/65  Loss batch: 14.678942  Tiempo transcurrido: 4.9s\n",
            "[Val][Época 1/50] Batch 30/65  Loss batch: 63.500706  Tiempo transcurrido: 7.7s\n",
            "[Val][Época 1/50] Batch 40/65  Loss batch: 80.648743  Tiempo transcurrido: 11.3s\n",
            "[Val][Época 1/50] Batch 50/65  Loss batch: 112.793030  Tiempo transcurrido: 13.8s\n",
            "[Val][Época 1/50] Batch 60/65  Loss batch: 98.672058  Tiempo transcurrido: 16.4s\n",
            "[Val] Época 1 terminada en 17.2s  Loss media: 65.996282\n",
            "Época [1/50] Train Loss: 0.997445 | Val Loss: 65.996282\n",
            "  🟢 Nuevo mejor modelo guardado en: /content/cats_dataset/posenet_light_best.pth\n",
            "============================================================\n",
            "Comenzando época 2/50\n",
            "[Train][Época 2/50] Batch 0/226  Loss batch: 0.468247  Tiempo transcurrido: 0.3s\n",
            "[Train][Época 2/50] Batch 10/226  Loss batch: 0.949893  Tiempo transcurrido: 3.5s\n",
            "[Train][Época 2/50] Batch 20/226  Loss batch: 0.655777  Tiempo transcurrido: 7.2s\n",
            "[Train][Época 2/50] Batch 30/226  Loss batch: 1.054881  Tiempo transcurrido: 10.2s\n",
            "[Train][Época 2/50] Batch 40/226  Loss batch: 0.882089  Tiempo transcurrido: 13.3s\n",
            "[Train][Época 2/50] Batch 50/226  Loss batch: 0.784085  Tiempo transcurrido: 16.3s\n",
            "[Train][Época 2/50] Batch 60/226  Loss batch: 1.055897  Tiempo transcurrido: 20.1s\n",
            "[Train][Época 2/50] Batch 70/226  Loss batch: 0.927021  Tiempo transcurrido: 23.1s\n",
            "[Train][Época 2/50] Batch 80/226  Loss batch: 0.678316  Tiempo transcurrido: 26.1s\n",
            "[Train][Época 2/50] Batch 90/226  Loss batch: 1.289218  Tiempo transcurrido: 29.2s\n",
            "[Train][Época 2/50] Batch 100/226  Loss batch: 0.740472  Tiempo transcurrido: 33.0s\n",
            "[Train][Época 2/50] Batch 110/226  Loss batch: 4.020634  Tiempo transcurrido: 36.0s\n",
            "[Train][Época 2/50] Batch 120/226  Loss batch: 0.960479  Tiempo transcurrido: 39.0s\n",
            "[Train][Época 2/50] Batch 130/226  Loss batch: 0.708731  Tiempo transcurrido: 42.1s\n",
            "[Train][Época 2/50] Batch 140/226  Loss batch: 0.645458  Tiempo transcurrido: 45.9s\n",
            "[Train][Época 2/50] Batch 150/226  Loss batch: 0.747880  Tiempo transcurrido: 48.9s\n",
            "[Train][Época 2/50] Batch 160/226  Loss batch: 0.686219  Tiempo transcurrido: 51.9s\n",
            "[Train][Época 2/50] Batch 170/226  Loss batch: 0.601975  Tiempo transcurrido: 55.0s\n",
            "[Train][Época 2/50] Batch 180/226  Loss batch: 0.773340  Tiempo transcurrido: 58.8s\n",
            "[Train][Época 2/50] Batch 190/226  Loss batch: 1.022791  Tiempo transcurrido: 61.9s\n",
            "[Train][Época 2/50] Batch 200/226  Loss batch: 0.589716  Tiempo transcurrido: 64.9s\n",
            "[Train][Época 2/50] Batch 210/226  Loss batch: 1.064904  Tiempo transcurrido: 67.9s\n",
            "[Train][Época 2/50] Batch 220/226  Loss batch: 0.520743  Tiempo transcurrido: 71.7s\n",
            "[Train] Época 2 terminada en 73.0s  Loss media: 1.239407\n",
            "[Val][Época 2/50] Batch 0/65  Loss batch: 6.804864  Tiempo transcurrido: 0.2s\n",
            "[Val][Época 2/50] Batch 10/65  Loss batch: 5.745737  Tiempo transcurrido: 2.6s\n",
            "[Val][Época 2/50] Batch 20/65  Loss batch: 14.722786  Tiempo transcurrido: 5.0s\n",
            "[Val][Época 2/50] Batch 30/65  Loss batch: 61.156197  Tiempo transcurrido: 7.6s\n",
            "[Val][Época 2/50] Batch 40/65  Loss batch: 104.013382  Tiempo transcurrido: 11.1s\n",
            "[Val][Época 2/50] Batch 50/65  Loss batch: 106.452766  Tiempo transcurrido: 13.8s\n",
            "[Val][Época 2/50] Batch 60/65  Loss batch: 103.893555  Tiempo transcurrido: 16.4s\n",
            "[Val] Época 2 terminada en 17.3s  Loss media: 66.719619\n",
            "Época [2/50] Train Loss: 1.239407 | Val Loss: 66.719619\n",
            "============================================================\n",
            "Comenzando época 3/50\n",
            "[Train][Época 3/50] Batch 0/226  Loss batch: 0.787851  Tiempo transcurrido: 0.3s\n",
            "[Train][Época 3/50] Batch 10/226  Loss batch: 0.859206  Tiempo transcurrido: 3.4s\n",
            "[Train][Época 3/50] Batch 20/226  Loss batch: 0.648521  Tiempo transcurrido: 7.2s\n",
            "[Train][Época 3/50] Batch 30/226  Loss batch: 0.790049  Tiempo transcurrido: 10.3s\n",
            "[Train][Época 3/50] Batch 40/226  Loss batch: 0.662554  Tiempo transcurrido: 13.4s\n",
            "[Train][Época 3/50] Batch 50/226  Loss batch: 3.271953  Tiempo transcurrido: 16.4s\n",
            "[Train][Época 3/50] Batch 60/226  Loss batch: 0.930981  Tiempo transcurrido: 20.0s\n",
            "[Train][Época 3/50] Batch 70/226  Loss batch: 1.079456  Tiempo transcurrido: 23.3s\n",
            "[Train][Época 3/50] Batch 80/226  Loss batch: 1.453764  Tiempo transcurrido: 26.3s\n",
            "[Train][Época 3/50] Batch 90/226  Loss batch: 0.705568  Tiempo transcurrido: 29.4s\n",
            "[Train][Época 3/50] Batch 100/226  Loss batch: 0.715617  Tiempo transcurrido: 33.0s\n",
            "[Train][Época 3/50] Batch 110/226  Loss batch: 1.314542  Tiempo transcurrido: 36.2s\n",
            "[Train][Época 3/50] Batch 120/226  Loss batch: 0.834600  Tiempo transcurrido: 39.2s\n",
            "[Train][Época 3/50] Batch 130/226  Loss batch: 0.511139  Tiempo transcurrido: 42.3s\n",
            "[Train][Época 3/50] Batch 140/226  Loss batch: 1.082879  Tiempo transcurrido: 45.8s\n",
            "[Train][Época 3/50] Batch 150/226  Loss batch: 0.778126  Tiempo transcurrido: 49.1s\n",
            "[Train][Época 3/50] Batch 160/226  Loss batch: 0.610040  Tiempo transcurrido: 52.2s\n",
            "[Train][Época 3/50] Batch 170/226  Loss batch: 0.791436  Tiempo transcurrido: 55.2s\n",
            "[Train][Época 3/50] Batch 180/226  Loss batch: 0.612899  Tiempo transcurrido: 58.7s\n",
            "[Train][Época 3/50] Batch 190/226  Loss batch: 0.689097  Tiempo transcurrido: 62.0s\n",
            "[Train][Época 3/50] Batch 200/226  Loss batch: 0.783228  Tiempo transcurrido: 65.1s\n",
            "[Train][Época 3/50] Batch 210/226  Loss batch: 0.628505  Tiempo transcurrido: 68.1s\n",
            "[Train][Época 3/50] Batch 220/226  Loss batch: 3.732537  Tiempo transcurrido: 71.5s\n",
            "[Train] Época 3 terminada en 73.2s  Loss media: 0.979513\n",
            "[Val][Época 3/50] Batch 0/65  Loss batch: 6.475422  Tiempo transcurrido: 0.2s\n",
            "[Val][Época 3/50] Batch 10/65  Loss batch: 11.527519  Tiempo transcurrido: 2.6s\n",
            "[Val][Época 3/50] Batch 20/65  Loss batch: 54.719810  Tiempo transcurrido: 5.0s\n",
            "[Val][Época 3/50] Batch 30/65  Loss batch: 61.776081  Tiempo transcurrido: 7.6s\n",
            "[Val][Época 3/50] Batch 40/65  Loss batch: 73.174423  Tiempo transcurrido: 10.7s\n",
            "[Val][Época 3/50] Batch 50/65  Loss batch: 111.719559  Tiempo transcurrido: 13.8s\n",
            "[Val][Época 3/50] Batch 60/65  Loss batch: 103.924080  Tiempo transcurrido: 16.5s\n",
            "[Val] Época 3 terminada en 17.3s  Loss media: 69.993262\n",
            "Época [3/50] Train Loss: 0.979513 | Val Loss: 69.993262\n",
            "============================================================\n",
            "Comenzando época 4/50\n",
            "[Train][Época 4/50] Batch 0/226  Loss batch: 0.711509  Tiempo transcurrido: 0.3s\n",
            "[Train][Época 4/50] Batch 10/226  Loss batch: 0.486752  Tiempo transcurrido: 3.4s\n",
            "[Train][Época 4/50] Batch 20/226  Loss batch: 3.087076  Tiempo transcurrido: 6.7s\n",
            "[Train][Época 4/50] Batch 30/226  Loss batch: 0.827156  Tiempo transcurrido: 10.2s\n",
            "[Train][Época 4/50] Batch 40/226  Loss batch: 0.641514  Tiempo transcurrido: 13.3s\n",
            "[Train][Época 4/50] Batch 50/226  Loss batch: 0.904197  Tiempo transcurrido: 16.3s\n",
            "[Train][Época 4/50] Batch 60/226  Loss batch: 4.076520  Tiempo transcurrido: 19.5s\n",
            "[Train][Época 4/50] Batch 70/226  Loss batch: 0.733755  Tiempo transcurrido: 23.1s\n",
            "[Train][Época 4/50] Batch 80/226  Loss batch: 0.794897  Tiempo transcurrido: 26.2s\n",
            "[Train][Época 4/50] Batch 90/226  Loss batch: 3.517424  Tiempo transcurrido: 29.2s\n",
            "[Train][Época 4/50] Batch 100/226  Loss batch: 0.679213  Tiempo transcurrido: 32.3s\n",
            "[Train][Época 4/50] Batch 110/226  Loss batch: 1.043367  Tiempo transcurrido: 36.0s\n",
            "[Train][Época 4/50] Batch 120/226  Loss batch: 0.717562  Tiempo transcurrido: 39.0s\n",
            "[Train][Época 4/50] Batch 130/226  Loss batch: 0.553830  Tiempo transcurrido: 42.0s\n",
            "[Train][Época 4/50] Batch 140/226  Loss batch: 3.828083  Tiempo transcurrido: 45.1s\n",
            "[Train][Época 4/50] Batch 150/226  Loss batch: 0.645553  Tiempo transcurrido: 48.9s\n",
            "[Train][Época 4/50] Batch 160/226  Loss batch: 0.917034  Tiempo transcurrido: 51.9s\n",
            "[Train][Época 4/50] Batch 170/226  Loss batch: 0.658951  Tiempo transcurrido: 55.0s\n",
            "[Train][Época 4/50] Batch 180/226  Loss batch: 0.608874  Tiempo transcurrido: 58.0s\n",
            "[Train][Época 4/50] Batch 190/226  Loss batch: 0.455232  Tiempo transcurrido: 61.8s\n",
            "[Train][Época 4/50] Batch 200/226  Loss batch: 0.536652  Tiempo transcurrido: 64.9s\n",
            "[Train][Época 4/50] Batch 210/226  Loss batch: 3.950773  Tiempo transcurrido: 67.9s\n",
            "[Train][Época 4/50] Batch 220/226  Loss batch: 3.196926  Tiempo transcurrido: 71.0s\n",
            "[Train] Época 4 terminada en 72.7s  Loss media: 1.389486\n",
            "[Val][Época 4/50] Batch 0/65  Loss batch: 11.045414  Tiempo transcurrido: 0.3s\n",
            "[Val][Época 4/50] Batch 10/65  Loss batch: 9.941528  Tiempo transcurrido: 3.0s\n",
            "[Val][Época 4/50] Batch 20/65  Loss batch: 66.216522  Tiempo transcurrido: 5.4s\n",
            "[Val][Época 4/50] Batch 30/65  Loss batch: 63.008141  Tiempo transcurrido: 8.0s\n",
            "[Val][Época 4/50] Batch 40/65  Loss batch: 71.838402  Tiempo transcurrido: 10.8s\n",
            "[Val][Época 4/50] Batch 50/65  Loss batch: 112.471909  Tiempo transcurrido: 14.0s\n",
            "[Val][Época 4/50] Batch 60/65  Loss batch: 105.804939  Tiempo transcurrido: 16.8s\n",
            "[Val] Época 4 terminada en 17.6s  Loss media: 79.338553\n",
            "Época [4/50] Train Loss: 1.389486 | Val Loss: 79.338553\n",
            "============================================================\n",
            "Comenzando época 5/50\n",
            "[Train][Época 5/50] Batch 0/226  Loss batch: 0.726005  Tiempo transcurrido: 0.3s\n",
            "[Train][Época 5/50] Batch 10/226  Loss batch: 0.641433  Tiempo transcurrido: 3.4s\n",
            "[Train][Época 5/50] Batch 20/226  Loss batch: 0.513165  Tiempo transcurrido: 6.4s\n",
            "[Train][Época 5/50] Batch 30/226  Loss batch: 1.672458  Tiempo transcurrido: 10.1s\n",
            "[Train][Época 5/50] Batch 40/226  Loss batch: 0.587980  Tiempo transcurrido: 13.1s\n",
            "[Train][Época 5/50] Batch 50/226  Loss batch: 0.602118  Tiempo transcurrido: 16.2s\n",
            "[Train][Época 5/50] Batch 60/226  Loss batch: 0.760817  Tiempo transcurrido: 19.2s\n",
            "[Train][Época 5/50] Batch 70/226  Loss batch: 0.505110  Tiempo transcurrido: 22.9s\n",
            "[Train][Época 5/50] Batch 80/226  Loss batch: 0.734298  Tiempo transcurrido: 26.0s\n",
            "[Train][Época 5/50] Batch 90/226  Loss batch: 0.503531  Tiempo transcurrido: 29.1s\n",
            "[Train][Época 5/50] Batch 100/226  Loss batch: 0.517236  Tiempo transcurrido: 32.1s\n",
            "[Train][Época 5/50] Batch 110/226  Loss batch: 1.000727  Tiempo transcurrido: 35.8s\n",
            "[Train][Época 5/50] Batch 120/226  Loss batch: 0.443984  Tiempo transcurrido: 38.9s\n",
            "[Train][Época 5/50] Batch 130/226  Loss batch: 3.214708  Tiempo transcurrido: 41.9s\n",
            "[Train][Época 5/50] Batch 140/226  Loss batch: 0.578591  Tiempo transcurrido: 44.9s\n",
            "[Train][Época 5/50] Batch 150/226  Loss batch: 0.854649  Tiempo transcurrido: 48.5s\n",
            "[Train][Época 5/50] Batch 160/226  Loss batch: 0.801386  Tiempo transcurrido: 51.7s\n",
            "[Train][Época 5/50] Batch 170/226  Loss batch: 0.585170  Tiempo transcurrido: 54.7s\n",
            "[Train][Época 5/50] Batch 180/226  Loss batch: 0.512349  Tiempo transcurrido: 57.7s\n",
            "[Train][Época 5/50] Batch 190/226  Loss batch: 0.475054  Tiempo transcurrido: 61.2s\n",
            "[Train][Época 5/50] Batch 200/226  Loss batch: 0.712996  Tiempo transcurrido: 64.5s\n",
            "[Train][Época 5/50] Batch 210/226  Loss batch: 0.534488  Tiempo transcurrido: 67.5s\n",
            "[Train][Época 5/50] Batch 220/226  Loss batch: 0.375030  Tiempo transcurrido: 70.5s\n",
            "[Train] Época 5 terminada en 71.8s  Loss media: 0.782827\n",
            "[Val][Época 5/50] Batch 0/65  Loss batch: 8.000198  Tiempo transcurrido: 0.2s\n",
            "[Val][Época 5/50] Batch 10/65  Loss batch: 6.124110  Tiempo transcurrido: 3.4s\n",
            "[Val][Época 5/50] Batch 20/65  Loss batch: 53.369965  Tiempo transcurrido: 5.8s\n",
            "[Val][Época 5/50] Batch 30/65  Loss batch: 61.034325  Tiempo transcurrido: 8.5s\n",
            "[Val][Época 5/50] Batch 40/65  Loss batch: 70.917198  Tiempo transcurrido: 11.2s\n",
            "[Val][Época 5/50] Batch 50/65  Loss batch: 111.234192  Tiempo transcurrido: 13.8s\n",
            "[Val][Época 5/50] Batch 60/65  Loss batch: 106.858521  Tiempo transcurrido: 17.3s\n",
            "[Val] Época 5 terminada en 18.1s  Loss media: 74.872235\n",
            "Época [5/50] Train Loss: 0.782827 | Val Loss: 74.872235\n",
            "============================================================\n",
            "Comenzando época 6/50\n",
            "[Train][Época 6/50] Batch 0/226  Loss batch: 0.476552  Tiempo transcurrido: 0.3s\n",
            "[Train][Época 6/50] Batch 10/226  Loss batch: 0.511073  Tiempo transcurrido: 3.4s\n",
            "[Train][Época 6/50] Batch 20/226  Loss batch: 0.463940  Tiempo transcurrido: 6.4s\n",
            "[Train][Época 6/50] Batch 30/226  Loss batch: 0.655627  Tiempo transcurrido: 9.6s\n",
            "[Train][Época 6/50] Batch 40/226  Loss batch: 0.526484  Tiempo transcurrido: 13.2s\n",
            "[Train][Época 6/50] Batch 50/226  Loss batch: 0.564765  Tiempo transcurrido: 16.2s\n",
            "[Train][Época 6/50] Batch 60/226  Loss batch: 0.402734  Tiempo transcurrido: 19.2s\n",
            "[Train][Época 6/50] Batch 70/226  Loss batch: 0.381265  Tiempo transcurrido: 22.4s\n",
            "[Train][Época 6/50] Batch 80/226  Loss batch: 0.435887  Tiempo transcurrido: 26.0s\n",
            "[Train][Época 6/50] Batch 90/226  Loss batch: 0.346428  Tiempo transcurrido: 29.0s\n",
            "[Train][Época 6/50] Batch 100/226  Loss batch: 0.452080  Tiempo transcurrido: 32.0s\n",
            "[Train][Época 6/50] Batch 110/226  Loss batch: 0.499284  Tiempo transcurrido: 35.2s\n",
            "[Train][Época 6/50] Batch 120/226  Loss batch: 0.796315  Tiempo transcurrido: 38.9s\n",
            "[Train][Época 6/50] Batch 130/226  Loss batch: 0.376005  Tiempo transcurrido: 41.9s\n",
            "[Train][Época 6/50] Batch 140/226  Loss batch: 0.404717  Tiempo transcurrido: 44.9s\n",
            "[Train][Época 6/50] Batch 150/226  Loss batch: 0.424930  Tiempo transcurrido: 47.9s\n",
            "[Train][Época 6/50] Batch 160/226  Loss batch: 0.304120  Tiempo transcurrido: 51.7s\n",
            "[Train][Época 6/50] Batch 170/226  Loss batch: 0.321904  Tiempo transcurrido: 54.7s\n",
            "[Train][Época 6/50] Batch 180/226  Loss batch: 0.529606  Tiempo transcurrido: 57.8s\n",
            "[Train][Época 6/50] Batch 190/226  Loss batch: 0.404533  Tiempo transcurrido: 60.8s\n",
            "[Train][Época 6/50] Batch 200/226  Loss batch: 0.406071  Tiempo transcurrido: 64.5s\n",
            "[Train][Época 6/50] Batch 210/226  Loss batch: 0.638830  Tiempo transcurrido: 67.5s\n",
            "[Train][Época 6/50] Batch 220/226  Loss batch: 0.460114  Tiempo transcurrido: 70.5s\n",
            "[Train] Época 6 terminada en 71.8s  Loss media: 0.537028\n",
            "[Val][Época 6/50] Batch 0/65  Loss batch: 7.905569  Tiempo transcurrido: 0.2s\n",
            "[Val][Época 6/50] Batch 10/65  Loss batch: 6.934072  Tiempo transcurrido: 2.7s\n",
            "[Val][Época 6/50] Batch 20/65  Loss batch: 53.293274  Tiempo transcurrido: 5.8s\n",
            "[Val][Época 6/50] Batch 30/65  Loss batch: 60.240227  Tiempo transcurrido: 8.4s\n",
            "[Val][Época 6/50] Batch 40/65  Loss batch: 80.594185  Tiempo transcurrido: 11.1s\n",
            "[Val][Época 6/50] Batch 50/65  Loss batch: 111.426926  Tiempo transcurrido: 13.6s\n",
            "[Val][Época 6/50] Batch 60/65  Loss batch: 107.062424  Tiempo transcurrido: 16.7s\n",
            "[Val] Época 6 terminada en 17.8s  Loss media: 74.378359\n",
            "Época [6/50] Train Loss: 0.537028 | Val Loss: 74.378359\n",
            "============================================================\n",
            "Comenzando época 7/50\n",
            "[Train][Época 7/50] Batch 0/226  Loss batch: 0.366553  Tiempo transcurrido: 0.4s\n",
            "[Train][Época 7/50] Batch 10/226  Loss batch: 0.493100  Tiempo transcurrido: 3.5s\n",
            "[Train][Época 7/50] Batch 20/226  Loss batch: 0.427901  Tiempo transcurrido: 6.5s\n",
            "[Train][Época 7/50] Batch 30/226  Loss batch: 0.717486  Tiempo transcurrido: 9.5s\n",
            "[Train][Época 7/50] Batch 40/226  Loss batch: 0.521811  Tiempo transcurrido: 13.2s\n",
            "[Train][Época 7/50] Batch 50/226  Loss batch: 0.602823  Tiempo transcurrido: 16.3s\n",
            "[Train][Época 7/50] Batch 60/226  Loss batch: 0.508527  Tiempo transcurrido: 19.4s\n",
            "[Train][Época 7/50] Batch 70/226  Loss batch: 0.383004  Tiempo transcurrido: 22.4s\n",
            "[Train][Época 7/50] Batch 80/226  Loss batch: 0.557257  Tiempo transcurrido: 26.0s\n",
            "[Train][Época 7/50] Batch 90/226  Loss batch: 0.484652  Tiempo transcurrido: 29.2s\n",
            "[Train][Época 7/50] Batch 100/226  Loss batch: 0.874913  Tiempo transcurrido: 32.2s\n",
            "[Train][Época 7/50] Batch 110/226  Loss batch: 0.569817  Tiempo transcurrido: 35.3s\n",
            "[Train][Época 7/50] Batch 120/226  Loss batch: 0.412274  Tiempo transcurrido: 38.8s\n",
            "[Train][Época 7/50] Batch 130/226  Loss batch: 0.324237  Tiempo transcurrido: 42.0s\n",
            "[Train][Época 7/50] Batch 140/226  Loss batch: 0.541777  Tiempo transcurrido: 45.0s\n",
            "[Train][Época 7/50] Batch 150/226  Loss batch: 0.400014  Tiempo transcurrido: 48.1s\n",
            "[Train][Época 7/50] Batch 160/226  Loss batch: 0.299558  Tiempo transcurrido: 51.6s\n",
            "[Train][Época 7/50] Batch 170/226  Loss batch: 0.481386  Tiempo transcurrido: 54.9s\n",
            "[Train][Época 7/50] Batch 180/226  Loss batch: 0.328780  Tiempo transcurrido: 57.9s\n",
            "[Train][Época 7/50] Batch 190/226  Loss batch: 0.306630  Tiempo transcurrido: 60.9s\n",
            "[Train][Época 7/50] Batch 200/226  Loss batch: 0.553909  Tiempo transcurrido: 64.2s\n",
            "[Train][Época 7/50] Batch 210/226  Loss batch: 0.445563  Tiempo transcurrido: 67.6s\n",
            "[Train][Época 7/50] Batch 220/226  Loss batch: 0.329206  Tiempo transcurrido: 70.6s\n",
            "[Train] Época 7 terminada en 71.9s  Loss media: 0.498202\n",
            "[Val][Época 7/50] Batch 0/65  Loss batch: 8.404201  Tiempo transcurrido: 0.2s\n",
            "[Val][Época 7/50] Batch 10/65  Loss batch: 7.174380  Tiempo transcurrido: 2.5s\n",
            "[Val][Época 7/50] Batch 20/65  Loss batch: 56.174358  Tiempo transcurrido: 5.3s\n",
            "[Val][Época 7/50] Batch 30/65  Loss batch: 59.401924  Tiempo transcurrido: 8.4s\n",
            "[Val][Época 7/50] Batch 40/65  Loss batch: 83.146584  Tiempo transcurrido: 11.1s\n",
            "[Val][Época 7/50] Batch 50/65  Loss batch: 111.052406  Tiempo transcurrido: 13.7s\n",
            "[Val][Época 7/50] Batch 60/65  Loss batch: 108.430763  Tiempo transcurrido: 16.3s\n",
            "[Val] Época 7 terminada en 17.2s  Loss media: 74.959318\n",
            "Época [7/50] Train Loss: 0.498202 | Val Loss: 74.959318\n",
            "============================================================\n",
            "Comenzando época 8/50\n",
            "[Train][Época 8/50] Batch 0/226  Loss batch: 0.314249  Tiempo transcurrido: 0.4s\n",
            "[Train][Época 8/50] Batch 10/226  Loss batch: 0.441917  Tiempo transcurrido: 4.1s\n",
            "[Train][Época 8/50] Batch 20/226  Loss batch: 0.382963  Tiempo transcurrido: 7.1s\n",
            "[Train][Época 8/50] Batch 30/226  Loss batch: 0.473492  Tiempo transcurrido: 10.1s\n",
            "[Train][Época 8/50] Batch 40/226  Loss batch: 0.300161  Tiempo transcurrido: 13.2s\n",
            "[Train][Época 8/50] Batch 50/226  Loss batch: 0.425367  Tiempo transcurrido: 16.9s\n",
            "[Train][Época 8/50] Batch 60/226  Loss batch: 0.299846  Tiempo transcurrido: 20.0s\n",
            "[Train][Época 8/50] Batch 70/226  Loss batch: 0.441036  Tiempo transcurrido: 23.0s\n",
            "[Train][Época 8/50] Batch 80/226  Loss batch: 0.335034  Tiempo transcurrido: 26.0s\n",
            "[Train][Época 8/50] Batch 90/226  Loss batch: 0.338113  Tiempo transcurrido: 29.8s\n",
            "[Train][Época 8/50] Batch 100/226  Loss batch: 0.514390  Tiempo transcurrido: 32.8s\n",
            "[Train][Época 8/50] Batch 110/226  Loss batch: 0.341572  Tiempo transcurrido: 35.9s\n",
            "[Train][Época 8/50] Batch 120/226  Loss batch: 0.487116  Tiempo transcurrido: 38.9s\n",
            "[Train][Época 8/50] Batch 130/226  Loss batch: 0.512632  Tiempo transcurrido: 42.6s\n",
            "[Train][Época 8/50] Batch 140/226  Loss batch: 0.285709  Tiempo transcurrido: 45.7s\n",
            "[Train][Época 8/50] Batch 150/226  Loss batch: 0.482382  Tiempo transcurrido: 48.7s\n",
            "[Train][Época 8/50] Batch 160/226  Loss batch: 0.646238  Tiempo transcurrido: 51.7s\n",
            "[Train][Época 8/50] Batch 170/226  Loss batch: 0.447104  Tiempo transcurrido: 55.5s\n",
            "[Train][Época 8/50] Batch 180/226  Loss batch: 0.784910  Tiempo transcurrido: 58.6s\n",
            "[Train][Época 8/50] Batch 190/226  Loss batch: 0.334357  Tiempo transcurrido: 61.6s\n",
            "[Train][Época 8/50] Batch 200/226  Loss batch: 0.453784  Tiempo transcurrido: 64.6s\n",
            "[Train][Época 8/50] Batch 210/226  Loss batch: 0.355691  Tiempo transcurrido: 68.4s\n",
            "[Train][Época 8/50] Batch 220/226  Loss batch: 0.387491  Tiempo transcurrido: 71.4s\n",
            "[Train] Época 8 terminada en 72.7s  Loss media: 0.448648\n",
            "[Val][Época 8/50] Batch 0/65  Loss batch: 7.871925  Tiempo transcurrido: 0.2s\n",
            "[Val][Época 8/50] Batch 10/65  Loss batch: 8.018558  Tiempo transcurrido: 2.6s\n",
            "[Val][Época 8/50] Batch 20/65  Loss batch: 35.199760  Tiempo transcurrido: 4.9s\n",
            "[Val][Época 8/50] Batch 30/65  Loss batch: 59.891556  Tiempo transcurrido: 8.3s\n",
            "[Val][Época 8/50] Batch 40/65  Loss batch: 78.782852  Tiempo transcurrido: 11.2s\n",
            "[Val][Época 8/50] Batch 50/65  Loss batch: 111.272606  Tiempo transcurrido: 13.7s\n",
            "[Val][Época 8/50] Batch 60/65  Loss batch: 106.212715  Tiempo transcurrido: 16.3s\n",
            "[Val] Época 8 terminada en 17.1s  Loss media: 67.980855\n",
            "Época [8/50] Train Loss: 0.448648 | Val Loss: 67.980855\n",
            "============================================================\n",
            "Comenzando época 9/50\n",
            "[Train][Época 9/50] Batch 0/226  Loss batch: 0.781799  Tiempo transcurrido: 0.3s\n",
            "[Train][Época 9/50] Batch 10/226  Loss batch: 0.377345  Tiempo transcurrido: 3.9s\n",
            "[Train][Época 9/50] Batch 20/226  Loss batch: 0.370387  Tiempo transcurrido: 7.1s\n",
            "[Train][Época 9/50] Batch 30/226  Loss batch: 0.345211  Tiempo transcurrido: 10.1s\n",
            "[Train][Época 9/50] Batch 40/226  Loss batch: 0.337170  Tiempo transcurrido: 13.2s\n",
            "[Train][Época 9/50] Batch 50/226  Loss batch: 0.459922  Tiempo transcurrido: 16.7s\n",
            "[Train][Época 9/50] Batch 60/226  Loss batch: 0.355273  Tiempo transcurrido: 20.0s\n",
            "[Train][Época 9/50] Batch 70/226  Loss batch: 0.420492  Tiempo transcurrido: 23.0s\n",
            "[Train][Época 9/50] Batch 80/226  Loss batch: 0.418468  Tiempo transcurrido: 26.0s\n",
            "[Train][Época 9/50] Batch 90/226  Loss batch: 0.321473  Tiempo transcurrido: 29.5s\n",
            "[Train][Época 9/50] Batch 100/226  Loss batch: 0.328747  Tiempo transcurrido: 32.8s\n",
            "[Train][Época 9/50] Batch 110/226  Loss batch: 0.335402  Tiempo transcurrido: 35.8s\n",
            "[Train][Época 9/50] Batch 120/226  Loss batch: 0.416615  Tiempo transcurrido: 38.8s\n",
            "[Train][Época 9/50] Batch 130/226  Loss batch: 0.440128  Tiempo transcurrido: 42.2s\n",
            "[Train][Época 9/50] Batch 140/226  Loss batch: 0.495570  Tiempo transcurrido: 45.6s\n",
            "[Train][Época 9/50] Batch 150/226  Loss batch: 0.349307  Tiempo transcurrido: 48.6s\n",
            "[Train][Época 9/50] Batch 160/226  Loss batch: 0.565915  Tiempo transcurrido: 51.6s\n",
            "[Train][Época 9/50] Batch 170/226  Loss batch: 0.373413  Tiempo transcurrido: 54.9s\n",
            "[Train][Época 9/50] Batch 180/226  Loss batch: 0.427820  Tiempo transcurrido: 58.4s\n",
            "[Train][Época 9/50] Batch 190/226  Loss batch: 3.599995  Tiempo transcurrido: 61.4s\n",
            "[Train][Época 9/50] Batch 200/226  Loss batch: 0.686613  Tiempo transcurrido: 64.4s\n",
            "[Train][Época 9/50] Batch 210/226  Loss batch: 0.212793  Tiempo transcurrido: 67.6s\n",
            "[Train][Época 9/50] Batch 220/226  Loss batch: 0.256893  Tiempo transcurrido: 71.3s\n",
            "[Train] Época 9 terminada en 72.7s  Loss media: 0.533818\n",
            "[Val][Época 9/50] Batch 0/65  Loss batch: 7.282699  Tiempo transcurrido: 0.2s\n",
            "[Val][Época 9/50] Batch 10/65  Loss batch: 6.520240  Tiempo transcurrido: 2.5s\n",
            "[Val][Época 9/50] Batch 20/65  Loss batch: 44.101215  Tiempo transcurrido: 4.9s\n",
            "[Val][Época 9/50] Batch 30/65  Loss batch: 58.940899  Tiempo transcurrido: 7.6s\n",
            "[Val][Época 9/50] Batch 40/65  Loss batch: 72.766563  Tiempo transcurrido: 11.2s\n",
            "[Val][Época 9/50] Batch 50/65  Loss batch: 111.187790  Tiempo transcurrido: 13.8s\n",
            "[Val][Época 9/50] Batch 60/65  Loss batch: 106.410851  Tiempo transcurrido: 16.4s\n",
            "[Val] Época 9 terminada en 17.2s  Loss media: 67.694984\n",
            "Época [9/50] Train Loss: 0.533818 | Val Loss: 67.694984\n",
            "============================================================\n",
            "Comenzando época 10/50\n",
            "[Train][Época 10/50] Batch 0/226  Loss batch: 0.893647  Tiempo transcurrido: 0.3s\n",
            "[Train][Época 10/50] Batch 10/226  Loss batch: 0.400579  Tiempo transcurrido: 3.4s\n",
            "[Train][Época 10/50] Batch 20/226  Loss batch: 0.372319  Tiempo transcurrido: 7.2s\n",
            "[Train][Época 10/50] Batch 30/226  Loss batch: 0.406650  Tiempo transcurrido: 10.2s\n",
            "[Train][Época 10/50] Batch 40/226  Loss batch: 0.383563  Tiempo transcurrido: 13.1s\n",
            "[Train][Época 10/50] Batch 50/226  Loss batch: 0.275321  Tiempo transcurrido: 16.1s\n",
            "[Train][Época 10/50] Batch 60/226  Loss batch: 0.377210  Tiempo transcurrido: 19.9s\n",
            "[Train][Época 10/50] Batch 70/226  Loss batch: 0.363629  Tiempo transcurrido: 22.9s\n",
            "[Train][Época 10/50] Batch 80/226  Loss batch: 0.418954  Tiempo transcurrido: 25.9s\n",
            "[Train][Época 10/50] Batch 90/226  Loss batch: 0.445954  Tiempo transcurrido: 28.9s\n",
            "[Train][Época 10/50] Batch 100/226  Loss batch: 0.375368  Tiempo transcurrido: 32.7s\n",
            "[Train][Época 10/50] Batch 110/226  Loss batch: 0.359898  Tiempo transcurrido: 35.7s\n",
            "[Train][Época 10/50] Batch 120/226  Loss batch: 0.397411  Tiempo transcurrido: 38.7s\n",
            "[Train][Época 10/50] Batch 130/226  Loss batch: 0.302163  Tiempo transcurrido: 41.7s\n",
            "[Train][Época 10/50] Batch 140/226  Loss batch: 0.348412  Tiempo transcurrido: 45.4s\n",
            "[Train][Época 10/50] Batch 150/226  Loss batch: 0.412652  Tiempo transcurrido: 48.5s\n",
            "[Train][Época 10/50] Batch 160/226  Loss batch: 0.392959  Tiempo transcurrido: 51.5s\n",
            "[Train][Época 10/50] Batch 170/226  Loss batch: 0.408767  Tiempo transcurrido: 54.5s\n",
            "[Train][Época 10/50] Batch 180/226  Loss batch: 0.272497  Tiempo transcurrido: 58.2s\n",
            "[Train][Época 10/50] Batch 190/226  Loss batch: 0.259212  Tiempo transcurrido: 61.4s\n",
            "[Train][Época 10/50] Batch 200/226  Loss batch: 0.451029  Tiempo transcurrido: 64.4s\n",
            "[Train][Época 10/50] Batch 210/226  Loss batch: 0.343539  Tiempo transcurrido: 67.4s\n",
            "[Train][Época 10/50] Batch 220/226  Loss batch: 0.355148  Tiempo transcurrido: 71.0s\n",
            "[Train] Época 10 terminada en 72.5s  Loss media: 0.517935\n",
            "[Val][Época 10/50] Batch 0/65  Loss batch: 8.404906  Tiempo transcurrido: 0.2s\n",
            "[Val][Época 10/50] Batch 10/65  Loss batch: 8.862789  Tiempo transcurrido: 2.5s\n",
            "[Val][Época 10/50] Batch 20/65  Loss batch: 50.868198  Tiempo transcurrido: 4.9s\n",
            "[Val][Época 10/50] Batch 30/65  Loss batch: 59.611778  Tiempo transcurrido: 7.5s\n",
            "[Val][Época 10/50] Batch 40/65  Loss batch: 77.313339  Tiempo transcurrido: 10.6s\n",
            "[Val][Época 10/50] Batch 50/65  Loss batch: 111.319901  Tiempo transcurrido: 13.6s\n",
            "[Val][Época 10/50] Batch 60/65  Loss batch: 104.850853  Tiempo transcurrido: 16.3s\n",
            "[Val] Época 10 terminada en 17.1s  Loss media: 72.887778\n",
            "Época [10/50] Train Loss: 0.517935 | Val Loss: 72.887778\n",
            "============================================================\n",
            "Comenzando época 11/50\n",
            "[Train][Época 11/50] Batch 0/226  Loss batch: 0.232478  Tiempo transcurrido: 0.3s\n",
            "[Train][Época 11/50] Batch 10/226  Loss batch: 0.319230  Tiempo transcurrido: 3.4s\n",
            "[Train][Época 11/50] Batch 20/226  Loss batch: 0.282584  Tiempo transcurrido: 6.8s\n",
            "[Train][Época 11/50] Batch 30/226  Loss batch: 0.386230  Tiempo transcurrido: 10.2s\n",
            "[Train][Época 11/50] Batch 40/226  Loss batch: 0.468172  Tiempo transcurrido: 13.1s\n",
            "[Train][Época 11/50] Batch 50/226  Loss batch: 0.305064  Tiempo transcurrido: 16.2s\n",
            "[Train][Época 11/50] Batch 60/226  Loss batch: 0.344222  Tiempo transcurrido: 19.4s\n",
            "[Train][Época 11/50] Batch 70/226  Loss batch: 0.331251  Tiempo transcurrido: 23.0s\n",
            "[Train][Época 11/50] Batch 80/226  Loss batch: 0.195259  Tiempo transcurrido: 26.0s\n",
            "[Train][Época 11/50] Batch 90/226  Loss batch: 0.458708  Tiempo transcurrido: 29.0s\n",
            "[Train][Época 11/50] Batch 100/226  Loss batch: 0.414792  Tiempo transcurrido: 32.3s\n",
            "[Train][Época 11/50] Batch 110/226  Loss batch: 0.494943  Tiempo transcurrido: 35.9s\n",
            "[Train][Época 11/50] Batch 120/226  Loss batch: 0.216151  Tiempo transcurrido: 38.8s\n",
            "[Train][Época 11/50] Batch 130/226  Loss batch: 0.357370  Tiempo transcurrido: 41.9s\n",
            "[Train][Época 11/50] Batch 140/226  Loss batch: 0.327296  Tiempo transcurrido: 45.0s\n",
            "[Train][Época 11/50] Batch 150/226  Loss batch: 0.319923  Tiempo transcurrido: 48.6s\n",
            "[Train][Época 11/50] Batch 160/226  Loss batch: 0.265844  Tiempo transcurrido: 51.7s\n",
            "[Train][Época 11/50] Batch 170/226  Loss batch: 0.313747  Tiempo transcurrido: 54.7s\n",
            "[Train][Época 11/50] Batch 180/226  Loss batch: 0.399514  Tiempo transcurrido: 57.8s\n",
            "[Train][Época 11/50] Batch 190/226  Loss batch: 0.402157  Tiempo transcurrido: 61.5s\n",
            "[Train][Época 11/50] Batch 200/226  Loss batch: 0.339197  Tiempo transcurrido: 64.5s\n",
            "[Train][Época 11/50] Batch 210/226  Loss batch: 0.508250  Tiempo transcurrido: 67.6s\n",
            "[Train][Época 11/50] Batch 220/226  Loss batch: 0.422069  Tiempo transcurrido: 70.6s\n",
            "[Train] Época 11 terminada en 72.4s  Loss media: 0.401309\n",
            "[Val][Época 11/50] Batch 0/65  Loss batch: 6.249503  Tiempo transcurrido: 0.3s\n",
            "[Val][Época 11/50] Batch 10/65  Loss batch: 7.551911  Tiempo transcurrido: 2.9s\n",
            "[Val][Época 11/50] Batch 20/65  Loss batch: 23.611254  Tiempo transcurrido: 5.3s\n",
            "[Val][Época 11/50] Batch 30/65  Loss batch: 59.837837  Tiempo transcurrido: 7.9s\n",
            "[Val][Época 11/50] Batch 40/65  Loss batch: 80.259644  Tiempo transcurrido: 10.6s\n",
            "[Val][Época 11/50] Batch 50/65  Loss batch: 111.474548  Tiempo transcurrido: 13.9s\n",
            "[Val][Época 11/50] Batch 60/65  Loss batch: 105.306824  Tiempo transcurrido: 16.7s\n",
            "[Val] Época 11 terminada en 17.5s  Loss media: 67.515910\n",
            "Época [11/50] Train Loss: 0.401309 | Val Loss: 67.515910\n",
            "============================================================\n",
            "Comenzando época 12/50\n",
            "[Train][Época 12/50] Batch 0/226  Loss batch: 0.348617  Tiempo transcurrido: 0.3s\n",
            "[Train][Época 12/50] Batch 10/226  Loss batch: 0.297760  Tiempo transcurrido: 3.4s\n",
            "[Train][Época 12/50] Batch 20/226  Loss batch: 0.269151  Tiempo transcurrido: 6.4s\n",
            "[Train][Época 12/50] Batch 30/226  Loss batch: 0.382985  Tiempo transcurrido: 10.1s\n",
            "[Train][Época 12/50] Batch 40/226  Loss batch: 0.290049  Tiempo transcurrido: 13.1s\n",
            "[Train][Época 12/50] Batch 50/226  Loss batch: 0.500460  Tiempo transcurrido: 16.1s\n",
            "[Train][Época 12/50] Batch 60/226  Loss batch: 0.312351  Tiempo transcurrido: 19.1s\n",
            "[Train][Época 12/50] Batch 70/226  Loss batch: 0.433762  Tiempo transcurrido: 22.8s\n",
            "[Train][Época 12/50] Batch 80/226  Loss batch: 0.264995  Tiempo transcurrido: 25.9s\n",
            "[Train][Época 12/50] Batch 90/226  Loss batch: 0.405667  Tiempo transcurrido: 28.9s\n",
            "[Train][Época 12/50] Batch 100/226  Loss batch: 0.360522  Tiempo transcurrido: 31.9s\n",
            "[Train][Época 12/50] Batch 110/226  Loss batch: 0.358247  Tiempo transcurrido: 35.5s\n",
            "[Train][Época 12/50] Batch 120/226  Loss batch: 0.503154  Tiempo transcurrido: 38.7s\n",
            "[Train][Época 12/50] Batch 130/226  Loss batch: 0.354086  Tiempo transcurrido: 41.7s\n",
            "[Train][Época 12/50] Batch 140/226  Loss batch: 0.367001  Tiempo transcurrido: 44.7s\n",
            "[Train][Época 12/50] Batch 150/226  Loss batch: 0.340481  Tiempo transcurrido: 48.4s\n",
            "[Train][Época 12/50] Batch 160/226  Loss batch: 0.466031  Tiempo transcurrido: 51.6s\n",
            "[Train][Época 12/50] Batch 170/226  Loss batch: 0.449489  Tiempo transcurrido: 54.6s\n",
            "[Train][Época 12/50] Batch 180/226  Loss batch: 0.261115  Tiempo transcurrido: 57.6s\n",
            "[Train][Época 12/50] Batch 190/226  Loss batch: 0.296649  Tiempo transcurrido: 61.2s\n",
            "[Train][Época 12/50] Batch 200/226  Loss batch: 0.266110  Tiempo transcurrido: 64.5s\n",
            "[Train][Época 12/50] Batch 210/226  Loss batch: 0.265886  Tiempo transcurrido: 67.5s\n",
            "[Train][Época 12/50] Batch 220/226  Loss batch: 0.339149  Tiempo transcurrido: 70.5s\n",
            "[Train] Época 12 terminada en 71.8s  Loss media: 0.370516\n",
            "[Val][Época 12/50] Batch 0/65  Loss batch: 6.562390  Tiempo transcurrido: 0.2s\n",
            "[Val][Época 12/50] Batch 10/65  Loss batch: 7.217009  Tiempo transcurrido: 3.5s\n",
            "[Val][Época 12/50] Batch 20/65  Loss batch: 16.475344  Tiempo transcurrido: 5.8s\n",
            "[Val][Época 12/50] Batch 30/65  Loss batch: 58.692871  Tiempo transcurrido: 8.5s\n",
            "[Val][Época 12/50] Batch 40/65  Loss batch: 71.822815  Tiempo transcurrido: 11.2s\n",
            "[Val][Época 12/50] Batch 50/65  Loss batch: 111.035690  Tiempo transcurrido: 13.8s\n",
            "[Val][Época 12/50] Batch 60/65  Loss batch: 106.202385  Tiempo transcurrido: 17.2s\n",
            "[Val] Época 12 terminada en 18.1s  Loss media: 64.612865\n",
            "Época [12/50] Train Loss: 0.370516 | Val Loss: 64.612865\n",
            "  🟢 Nuevo mejor modelo guardado en: /content/cats_dataset/posenet_light_best.pth\n",
            "============================================================\n",
            "Comenzando época 13/50\n",
            "[Train][Época 13/50] Batch 0/226  Loss batch: 0.448850  Tiempo transcurrido: 0.3s\n",
            "[Train][Época 13/50] Batch 10/226  Loss batch: 0.242745  Tiempo transcurrido: 3.5s\n",
            "[Train][Época 13/50] Batch 20/226  Loss batch: 0.289498  Tiempo transcurrido: 6.5s\n",
            "[Train][Época 13/50] Batch 30/226  Loss batch: 0.305955  Tiempo transcurrido: 9.9s\n",
            "[Train][Época 13/50] Batch 40/226  Loss batch: 0.286874  Tiempo transcurrido: 13.3s\n",
            "[Train][Época 13/50] Batch 50/226  Loss batch: 0.373135  Tiempo transcurrido: 16.4s\n",
            "[Train][Época 13/50] Batch 60/226  Loss batch: 0.306252  Tiempo transcurrido: 19.4s\n",
            "[Train][Época 13/50] Batch 70/226  Loss batch: 0.495172  Tiempo transcurrido: 22.8s\n",
            "[Train][Época 13/50] Batch 80/226  Loss batch: 0.373065  Tiempo transcurrido: 26.2s\n",
            "[Train][Época 13/50] Batch 90/226  Loss batch: 0.251807  Tiempo transcurrido: 29.2s\n",
            "[Train][Época 13/50] Batch 100/226  Loss batch: 0.409317  Tiempo transcurrido: 32.2s\n",
            "[Train][Época 13/50] Batch 110/226  Loss batch: 0.292562  Tiempo transcurrido: 35.5s\n",
            "[Train][Época 13/50] Batch 120/226  Loss batch: 0.277495  Tiempo transcurrido: 39.1s\n",
            "[Train][Época 13/50] Batch 130/226  Loss batch: 0.511108  Tiempo transcurrido: 42.1s\n",
            "[Train][Época 13/50] Batch 140/226  Loss batch: 0.434680  Tiempo transcurrido: 45.1s\n",
            "[Train][Época 13/50] Batch 150/226  Loss batch: 0.285076  Tiempo transcurrido: 48.3s\n",
            "[Train][Época 13/50] Batch 160/226  Loss batch: 0.236157  Tiempo transcurrido: 51.9s\n",
            "[Train][Época 13/50] Batch 170/226  Loss batch: 0.303505  Tiempo transcurrido: 54.9s\n",
            "[Train][Época 13/50] Batch 180/226  Loss batch: 0.299094  Tiempo transcurrido: 57.9s\n",
            "[Train][Época 13/50] Batch 190/226  Loss batch: 0.565849  Tiempo transcurrido: 61.1s\n",
            "[Train][Época 13/50] Batch 200/226  Loss batch: 0.433229  Tiempo transcurrido: 64.8s\n",
            "[Train][Época 13/50] Batch 210/226  Loss batch: 0.255898  Tiempo transcurrido: 67.8s\n",
            "[Train][Época 13/50] Batch 220/226  Loss batch: 0.311561  Tiempo transcurrido: 70.8s\n",
            "[Train] Época 13 terminada en 72.1s  Loss media: 0.331780\n",
            "[Val][Época 13/50] Batch 0/65  Loss batch: 7.689986  Tiempo transcurrido: 0.2s\n",
            "[Val][Época 13/50] Batch 10/65  Loss batch: 7.018446  Tiempo transcurrido: 2.9s\n",
            "[Val][Época 13/50] Batch 20/65  Loss batch: 23.311810  Tiempo transcurrido: 5.7s\n",
            "[Val][Época 13/50] Batch 30/65  Loss batch: 59.189041  Tiempo transcurrido: 8.4s\n",
            "[Val][Época 13/50] Batch 40/65  Loss batch: 74.922600  Tiempo transcurrido: 11.1s\n",
            "[Val][Época 13/50] Batch 50/65  Loss batch: 111.286415  Tiempo transcurrido: 13.6s\n",
            "[Val][Época 13/50] Batch 60/65  Loss batch: 106.698105  Tiempo transcurrido: 16.9s\n",
            "[Val] Época 13 terminada en 18.0s  Loss media: 66.869404\n",
            "Época [13/50] Train Loss: 0.331780 | Val Loss: 66.869404\n",
            "============================================================\n",
            "Comenzando época 14/50\n",
            "[Train][Época 14/50] Batch 0/226  Loss batch: 0.270255  Tiempo transcurrido: 0.3s\n",
            "[Train][Época 14/50] Batch 10/226  Loss batch: 0.518745  Tiempo transcurrido: 3.3s\n",
            "[Train][Época 14/50] Batch 20/226  Loss batch: 0.329749  Tiempo transcurrido: 6.3s\n",
            "[Train][Época 14/50] Batch 30/226  Loss batch: 0.287471  Tiempo transcurrido: 9.3s\n",
            "[Train][Época 14/50] Batch 40/226  Loss batch: 0.276972  Tiempo transcurrido: 13.1s\n",
            "[Train][Época 14/50] Batch 50/226  Loss batch: 0.330678  Tiempo transcurrido: 16.1s\n",
            "[Train][Época 14/50] Batch 60/226  Loss batch: 0.234740  Tiempo transcurrido: 19.1s\n",
            "[Train][Época 14/50] Batch 70/226  Loss batch: 0.231885  Tiempo transcurrido: 22.1s\n",
            "[Train][Época 14/50] Batch 80/226  Loss batch: 0.307103  Tiempo transcurrido: 25.8s\n",
            "[Train][Época 14/50] Batch 90/226  Loss batch: 0.337395  Tiempo transcurrido: 29.0s\n",
            "[Train][Época 14/50] Batch 100/226  Loss batch: 0.424264  Tiempo transcurrido: 32.0s\n",
            "[Train][Época 14/50] Batch 110/226  Loss batch: 0.215267  Tiempo transcurrido: 35.0s\n",
            "[Train][Época 14/50] Batch 120/226  Loss batch: 0.296868  Tiempo transcurrido: 38.6s\n",
            "[Train][Época 14/50] Batch 130/226  Loss batch: 0.309932  Tiempo transcurrido: 41.8s\n",
            "[Train][Época 14/50] Batch 140/226  Loss batch: 0.378049  Tiempo transcurrido: 44.9s\n",
            "[Train][Época 14/50] Batch 150/226  Loss batch: 0.324901  Tiempo transcurrido: 48.0s\n",
            "[Train][Época 14/50] Batch 160/226  Loss batch: 0.389417  Tiempo transcurrido: 51.7s\n",
            "[Train][Época 14/50] Batch 170/226  Loss batch: 0.331621  Tiempo transcurrido: 54.8s\n",
            "[Train][Época 14/50] Batch 180/226  Loss batch: 0.299208  Tiempo transcurrido: 57.8s\n",
            "[Train][Época 14/50] Batch 190/226  Loss batch: 0.331058  Tiempo transcurrido: 60.9s\n",
            "[Train][Época 14/50] Batch 200/226  Loss batch: 0.240356  Tiempo transcurrido: 64.6s\n",
            "[Train][Época 14/50] Batch 210/226  Loss batch: 0.224524  Tiempo transcurrido: 67.8s\n",
            "[Train][Época 14/50] Batch 220/226  Loss batch: 0.252408  Tiempo transcurrido: 70.8s\n",
            "[Train] Época 14 terminada en 72.1s  Loss media: 0.321427\n",
            "[Val][Época 14/50] Batch 0/65  Loss batch: 7.485622  Tiempo transcurrido: 0.2s\n",
            "[Val][Época 14/50] Batch 10/65  Loss batch: 7.491475  Tiempo transcurrido: 2.6s\n",
            "[Val][Época 14/50] Batch 20/65  Loss batch: 30.967340  Tiempo transcurrido: 5.7s\n",
            "[Val][Época 14/50] Batch 30/65  Loss batch: 60.016621  Tiempo transcurrido: 8.4s\n",
            "[Val][Época 14/50] Batch 40/65  Loss batch: 77.062134  Tiempo transcurrido: 11.2s\n",
            "[Val][Época 14/50] Batch 50/65  Loss batch: 111.767822  Tiempo transcurrido: 13.7s\n",
            "[Val][Época 14/50] Batch 60/65  Loss batch: 106.257454  Tiempo transcurrido: 16.3s\n",
            "[Val] Época 14 terminada en 17.5s  Loss media: 67.009298\n",
            "Época [14/50] Train Loss: 0.321427 | Val Loss: 67.009298\n",
            "============================================================\n",
            "Comenzando época 15/50\n",
            "[Train][Época 15/50] Batch 0/226  Loss batch: 0.337743  Tiempo transcurrido: 0.4s\n",
            "[Train][Época 15/50] Batch 10/226  Loss batch: 0.326606  Tiempo transcurrido: 3.8s\n",
            "[Train][Época 15/50] Batch 20/226  Loss batch: 0.277301  Tiempo transcurrido: 6.9s\n",
            "[Train][Época 15/50] Batch 30/226  Loss batch: 0.386653  Tiempo transcurrido: 9.9s\n",
            "[Train][Época 15/50] Batch 40/226  Loss batch: 0.351342  Tiempo transcurrido: 13.4s\n",
            "[Train][Época 15/50] Batch 50/226  Loss batch: 0.298817  Tiempo transcurrido: 16.8s\n",
            "[Train][Época 15/50] Batch 60/226  Loss batch: 0.222943  Tiempo transcurrido: 19.9s\n",
            "[Train][Época 15/50] Batch 70/226  Loss batch: 0.415087  Tiempo transcurrido: 22.9s\n",
            "[Train][Época 15/50] Batch 80/226  Loss batch: 0.296184  Tiempo transcurrido: 26.3s\n",
            "[Train][Época 15/50] Batch 90/226  Loss batch: 0.236622  Tiempo transcurrido: 29.8s\n",
            "[Train][Época 15/50] Batch 100/226  Loss batch: 0.228429  Tiempo transcurrido: 32.8s\n",
            "[Train][Época 15/50] Batch 110/226  Loss batch: 0.296251  Tiempo transcurrido: 35.8s\n",
            "[Train][Época 15/50] Batch 120/226  Loss batch: 0.282648  Tiempo transcurrido: 39.1s\n",
            "[Train][Época 15/50] Batch 130/226  Loss batch: 0.271718  Tiempo transcurrido: 42.7s\n",
            "[Train][Época 15/50] Batch 140/226  Loss batch: 0.221671  Tiempo transcurrido: 45.7s\n",
            "[Train][Época 15/50] Batch 150/226  Loss batch: 0.287235  Tiempo transcurrido: 48.7s\n",
            "[Train][Época 15/50] Batch 160/226  Loss batch: 0.247250  Tiempo transcurrido: 52.0s\n",
            "[Train][Época 15/50] Batch 170/226  Loss batch: 0.357863  Tiempo transcurrido: 55.5s\n",
            "[Train][Época 15/50] Batch 180/226  Loss batch: 0.287481  Tiempo transcurrido: 58.6s\n",
            "[Train][Época 15/50] Batch 190/226  Loss batch: 0.292176  Tiempo transcurrido: 61.6s\n",
            "[Train][Época 15/50] Batch 200/226  Loss batch: 0.304197  Tiempo transcurrido: 64.7s\n",
            "[Train][Época 15/50] Batch 210/226  Loss batch: 0.301757  Tiempo transcurrido: 68.4s\n",
            "[Train][Época 15/50] Batch 220/226  Loss batch: 0.334891  Tiempo transcurrido: 71.4s\n",
            "[Train] Época 15 terminada en 72.7s  Loss media: 0.308068\n",
            "[Val][Época 15/50] Batch 0/65  Loss batch: 7.282875  Tiempo transcurrido: 0.2s\n",
            "[Val][Época 15/50] Batch 10/65  Loss batch: 8.464470  Tiempo transcurrido: 2.6s\n",
            "[Val][Época 15/50] Batch 20/65  Loss batch: 22.693007  Tiempo transcurrido: 5.1s\n",
            "[Val][Época 15/50] Batch 30/65  Loss batch: 59.837074  Tiempo transcurrido: 8.5s\n",
            "[Val][Época 15/50] Batch 40/65  Loss batch: 78.665985  Tiempo transcurrido: 11.3s\n",
            "[Val][Época 15/50] Batch 50/65  Loss batch: 111.349747  Tiempo transcurrido: 13.8s\n",
            "[Val][Época 15/50] Batch 60/65  Loss batch: 106.283325  Tiempo transcurrido: 16.4s\n",
            "[Val] Época 15 terminada en 17.3s  Loss media: 66.628414\n",
            "Época [15/50] Train Loss: 0.308068 | Val Loss: 66.628414\n",
            "============================================================\n",
            "Comenzando época 16/50\n",
            "[Train][Época 16/50] Batch 0/226  Loss batch: 0.268347  Tiempo transcurrido: 0.3s\n",
            "[Train][Época 16/50] Batch 10/226  Loss batch: 0.249982  Tiempo transcurrido: 4.1s\n",
            "[Train][Época 16/50] Batch 20/226  Loss batch: 0.203411  Tiempo transcurrido: 7.1s\n",
            "[Train][Época 16/50] Batch 30/226  Loss batch: 0.278269  Tiempo transcurrido: 10.2s\n",
            "[Train][Época 16/50] Batch 40/226  Loss batch: 0.264255  Tiempo transcurrido: 13.2s\n",
            "[Train][Época 16/50] Batch 50/226  Loss batch: 0.335519  Tiempo transcurrido: 17.0s\n",
            "[Train][Época 16/50] Batch 60/226  Loss batch: 0.239723  Tiempo transcurrido: 20.0s\n",
            "[Train][Época 16/50] Batch 70/226  Loss batch: 0.343473  Tiempo transcurrido: 23.0s\n",
            "[Train][Época 16/50] Batch 80/226  Loss batch: 0.418007  Tiempo transcurrido: 26.0s\n",
            "[Train][Época 16/50] Batch 90/226  Loss batch: 0.222732  Tiempo transcurrido: 29.8s\n",
            "[Train][Época 16/50] Batch 100/226  Loss batch: 0.263418  Tiempo transcurrido: 32.9s\n",
            "[Train][Época 16/50] Batch 110/226  Loss batch: 0.310872  Tiempo transcurrido: 35.9s\n",
            "[Train][Época 16/50] Batch 120/226  Loss batch: 0.312517  Tiempo transcurrido: 38.9s\n",
            "[Train][Época 16/50] Batch 130/226  Loss batch: 0.217632  Tiempo transcurrido: 42.6s\n",
            "[Train][Época 16/50] Batch 140/226  Loss batch: 0.290502  Tiempo transcurrido: 45.6s\n",
            "[Train][Época 16/50] Batch 150/226  Loss batch: 0.289176  Tiempo transcurrido: 48.7s\n",
            "[Train][Época 16/50] Batch 160/226  Loss batch: 0.181864  Tiempo transcurrido: 51.7s\n",
            "[Train][Época 16/50] Batch 170/226  Loss batch: 0.246479  Tiempo transcurrido: 55.4s\n",
            "[Train][Época 16/50] Batch 180/226  Loss batch: 0.243809  Tiempo transcurrido: 58.5s\n",
            "[Train][Época 16/50] Batch 190/226  Loss batch: 0.215263  Tiempo transcurrido: 61.6s\n",
            "[Train][Época 16/50] Batch 200/226  Loss batch: 0.271995  Tiempo transcurrido: 64.6s\n",
            "[Train][Época 16/50] Batch 210/226  Loss batch: 0.432132  Tiempo transcurrido: 68.2s\n",
            "[Train][Época 16/50] Batch 220/226  Loss batch: 0.237181  Tiempo transcurrido: 71.4s\n",
            "[Train] Época 16 terminada en 72.7s  Loss media: 0.292649\n",
            "[Val][Época 16/50] Batch 0/65  Loss batch: 7.371468  Tiempo transcurrido: 0.2s\n",
            "[Val][Época 16/50] Batch 10/65  Loss batch: 7.509897  Tiempo transcurrido: 2.6s\n",
            "[Val][Época 16/50] Batch 20/65  Loss batch: 27.032101  Tiempo transcurrido: 4.9s\n",
            "[Val][Época 16/50] Batch 30/65  Loss batch: 59.570644  Tiempo transcurrido: 8.1s\n",
            "[Val][Época 16/50] Batch 40/65  Loss batch: 81.843987  Tiempo transcurrido: 11.2s\n",
            "[Val][Época 16/50] Batch 50/65  Loss batch: 111.122322  Tiempo transcurrido: 13.7s\n",
            "[Val][Época 16/50] Batch 60/65  Loss batch: 106.290512  Tiempo transcurrido: 16.4s\n",
            "[Val] Época 16 terminada en 17.2s  Loss media: 67.095122\n",
            "Época [16/50] Train Loss: 0.292649 | Val Loss: 67.095122\n",
            "============================================================\n",
            "Comenzando época 17/50\n",
            "[Train][Época 17/50] Batch 0/226  Loss batch: 0.211383  Tiempo transcurrido: 0.3s\n",
            "[Train][Época 17/50] Batch 10/226  Loss batch: 0.372285  Tiempo transcurrido: 3.8s\n",
            "[Train][Época 17/50] Batch 20/226  Loss batch: 0.209666  Tiempo transcurrido: 7.2s\n",
            "[Train][Época 17/50] Batch 30/226  Loss batch: 0.290037  Tiempo transcurrido: 10.2s\n",
            "[Train][Época 17/50] Batch 40/226  Loss batch: 0.213133  Tiempo transcurrido: 13.2s\n",
            "[Train][Época 17/50] Batch 50/226  Loss batch: 0.399310  Tiempo transcurrido: 16.5s\n",
            "[Train][Época 17/50] Batch 60/226  Loss batch: 0.172753  Tiempo transcurrido: 19.9s\n",
            "[Train][Época 17/50] Batch 70/226  Loss batch: 0.200063  Tiempo transcurrido: 23.0s\n",
            "[Train][Época 17/50] Batch 80/226  Loss batch: 0.333854  Tiempo transcurrido: 26.0s\n",
            "[Train][Época 17/50] Batch 90/226  Loss batch: 0.230710  Tiempo transcurrido: 29.3s\n",
            "[Train][Época 17/50] Batch 100/226  Loss batch: 0.294020  Tiempo transcurrido: 32.8s\n",
            "[Train][Época 17/50] Batch 110/226  Loss batch: 0.225404  Tiempo transcurrido: 35.8s\n",
            "[Train][Época 17/50] Batch 120/226  Loss batch: 0.187393  Tiempo transcurrido: 38.9s\n",
            "[Train][Época 17/50] Batch 130/226  Loss batch: 0.248217  Tiempo transcurrido: 42.0s\n",
            "[Train][Época 17/50] Batch 140/226  Loss batch: 0.300013  Tiempo transcurrido: 45.6s\n",
            "[Train][Época 17/50] Batch 150/226  Loss batch: 0.313334  Tiempo transcurrido: 48.6s\n",
            "[Train][Época 17/50] Batch 160/226  Loss batch: 0.219294  Tiempo transcurrido: 51.6s\n",
            "[Train][Época 17/50] Batch 170/226  Loss batch: 0.344638  Tiempo transcurrido: 54.7s\n",
            "[Train][Época 17/50] Batch 180/226  Loss batch: 0.335232  Tiempo transcurrido: 58.4s\n",
            "[Train][Época 17/50] Batch 190/226  Loss batch: 0.225446  Tiempo transcurrido: 61.4s\n",
            "[Train][Época 17/50] Batch 200/226  Loss batch: 0.205154  Tiempo transcurrido: 64.5s\n",
            "[Train][Época 17/50] Batch 210/226  Loss batch: 0.219087  Tiempo transcurrido: 67.5s\n",
            "[Train][Época 17/50] Batch 220/226  Loss batch: 0.354366  Tiempo transcurrido: 71.2s\n",
            "[Train] Época 17 terminada en 72.5s  Loss media: 0.280233\n",
            "[Val][Época 17/50] Batch 0/65  Loss batch: 6.958620  Tiempo transcurrido: 0.2s\n",
            "[Val][Época 17/50] Batch 10/65  Loss batch: 7.843487  Tiempo transcurrido: 2.5s\n",
            "[Val][Época 17/50] Batch 20/65  Loss batch: 28.498877  Tiempo transcurrido: 4.9s\n",
            "[Val][Época 17/50] Batch 30/65  Loss batch: 61.088394  Tiempo transcurrido: 7.5s\n",
            "[Val][Época 17/50] Batch 40/65  Loss batch: 90.209167  Tiempo transcurrido: 11.1s\n",
            "[Val][Época 17/50] Batch 50/65  Loss batch: 111.220200  Tiempo transcurrido: 13.6s\n",
            "[Val][Época 17/50] Batch 60/65  Loss batch: 106.349541  Tiempo transcurrido: 16.2s\n",
            "[Val] Época 17 terminada en 17.0s  Loss media: 69.615080\n",
            "Época [17/50] Train Loss: 0.280233 | Val Loss: 69.615080\n",
            "============================================================\n",
            "Comenzando época 18/50\n",
            "[Train][Época 18/50] Batch 0/226  Loss batch: 0.283133  Tiempo transcurrido: 0.3s\n",
            "[Train][Época 18/50] Batch 10/226  Loss batch: 0.268747  Tiempo transcurrido: 3.4s\n",
            "[Train][Época 18/50] Batch 20/226  Loss batch: 0.313039  Tiempo transcurrido: 7.2s\n",
            "[Train][Época 18/50] Batch 30/226  Loss batch: 0.256008  Tiempo transcurrido: 10.2s\n",
            "[Train][Época 18/50] Batch 40/226  Loss batch: 0.242451  Tiempo transcurrido: 13.2s\n",
            "[Train][Época 18/50] Batch 50/226  Loss batch: 0.311349  Tiempo transcurrido: 16.2s\n",
            "[Train][Época 18/50] Batch 60/226  Loss batch: 0.374266  Tiempo transcurrido: 19.9s\n",
            "[Train][Época 18/50] Batch 70/226  Loss batch: 0.239919  Tiempo transcurrido: 23.0s\n",
            "[Train][Época 18/50] Batch 80/226  Loss batch: 0.222906  Tiempo transcurrido: 26.0s\n",
            "[Train][Época 18/50] Batch 90/226  Loss batch: 0.243513  Tiempo transcurrido: 29.0s\n",
            "[Train][Época 18/50] Batch 100/226  Loss batch: 0.205429  Tiempo transcurrido: 32.7s\n",
            "[Train][Época 18/50] Batch 110/226  Loss batch: 0.359478  Tiempo transcurrido: 35.8s\n",
            "[Train][Época 18/50] Batch 120/226  Loss batch: 0.268977  Tiempo transcurrido: 38.9s\n",
            "[Train][Época 18/50] Batch 130/226  Loss batch: 0.377636  Tiempo transcurrido: 41.8s\n",
            "[Train][Época 18/50] Batch 140/226  Loss batch: 0.285088  Tiempo transcurrido: 45.3s\n",
            "[Train][Época 18/50] Batch 150/226  Loss batch: 0.229350  Tiempo transcurrido: 48.6s\n",
            "[Train][Época 18/50] Batch 160/226  Loss batch: 0.328618  Tiempo transcurrido: 51.6s\n",
            "[Train][Época 18/50] Batch 170/226  Loss batch: 0.298514  Tiempo transcurrido: 54.7s\n",
            "[Train][Época 18/50] Batch 180/226  Loss batch: 0.194277  Tiempo transcurrido: 58.2s\n",
            "[Train][Época 18/50] Batch 190/226  Loss batch: 0.264746  Tiempo transcurrido: 61.5s\n",
            "[Train][Época 18/50] Batch 200/226  Loss batch: 0.216591  Tiempo transcurrido: 64.6s\n",
            "[Train][Época 18/50] Batch 210/226  Loss batch: 0.277234  Tiempo transcurrido: 67.6s\n",
            "[Train][Época 18/50] Batch 220/226  Loss batch: 0.269196  Tiempo transcurrido: 71.0s\n",
            "[Train] Época 18 terminada en 72.6s  Loss media: 0.272728\n",
            "[Val][Época 18/50] Batch 0/65  Loss batch: 6.351525  Tiempo transcurrido: 0.2s\n",
            "[Val][Época 18/50] Batch 10/65  Loss batch: 7.468277  Tiempo transcurrido: 2.6s\n",
            "[Val][Época 18/50] Batch 20/65  Loss batch: 20.151691  Tiempo transcurrido: 4.9s\n",
            "[Val][Época 18/50] Batch 30/65  Loss batch: 60.472523  Tiempo transcurrido: 7.5s\n",
            "[Val][Época 18/50] Batch 40/65  Loss batch: 75.593811  Tiempo transcurrido: 10.4s\n",
            "[Val][Época 18/50] Batch 50/65  Loss batch: 111.262741  Tiempo transcurrido: 13.7s\n",
            "[Val][Época 18/50] Batch 60/65  Loss batch: 105.054794  Tiempo transcurrido: 16.3s\n",
            "[Val] Época 18 terminada en 17.1s  Loss media: 65.406731\n",
            "Época [18/50] Train Loss: 0.272728 | Val Loss: 65.406731\n",
            "============================================================\n",
            "Comenzando época 19/50\n",
            "[Train][Época 19/50] Batch 0/226  Loss batch: 0.195422  Tiempo transcurrido: 0.3s\n",
            "[Train][Época 19/50] Batch 10/226  Loss batch: 0.235130  Tiempo transcurrido: 3.4s\n",
            "[Train][Época 19/50] Batch 20/226  Loss batch: 0.206306  Tiempo transcurrido: 6.5s\n",
            "[Train][Época 19/50] Batch 30/226  Loss batch: 0.268307  Tiempo transcurrido: 10.1s\n",
            "[Train][Época 19/50] Batch 40/226  Loss batch: 0.279550  Tiempo transcurrido: 13.1s\n",
            "[Train][Época 19/50] Batch 50/226  Loss batch: 0.215571  Tiempo transcurrido: 16.2s\n",
            "[Train][Época 19/50] Batch 60/226  Loss batch: 0.217957  Tiempo transcurrido: 19.3s\n",
            "[Train][Época 19/50] Batch 70/226  Loss batch: 0.279225  Tiempo transcurrido: 22.9s\n",
            "[Train][Época 19/50] Batch 80/226  Loss batch: 0.230504  Tiempo transcurrido: 26.0s\n",
            "[Train][Época 19/50] Batch 90/226  Loss batch: 0.240810  Tiempo transcurrido: 29.0s\n",
            "[Train][Época 19/50] Batch 100/226  Loss batch: 0.224914  Tiempo transcurrido: 32.0s\n",
            "[Train][Época 19/50] Batch 110/226  Loss batch: 0.274620  Tiempo transcurrido: 35.8s\n",
            "[Train][Época 19/50] Batch 120/226  Loss batch: 0.363103  Tiempo transcurrido: 38.8s\n",
            "[Train][Época 19/50] Batch 130/226  Loss batch: 0.258104  Tiempo transcurrido: 41.8s\n",
            "[Train][Época 19/50] Batch 140/226  Loss batch: 0.245860  Tiempo transcurrido: 44.9s\n",
            "[Train][Época 19/50] Batch 150/226  Loss batch: 0.225287  Tiempo transcurrido: 48.6s\n",
            "[Train][Época 19/50] Batch 160/226  Loss batch: 0.306384  Tiempo transcurrido: 51.7s\n",
            "[Train][Época 19/50] Batch 170/226  Loss batch: 0.251934  Tiempo transcurrido: 54.6s\n",
            "[Train][Época 19/50] Batch 180/226  Loss batch: 0.236942  Tiempo transcurrido: 57.7s\n",
            "[Train][Época 19/50] Batch 190/226  Loss batch: 0.167195  Tiempo transcurrido: 61.4s\n",
            "[Train][Época 19/50] Batch 200/226  Loss batch: 0.274096  Tiempo transcurrido: 64.4s\n",
            "[Train][Época 19/50] Batch 210/226  Loss batch: 0.268869  Tiempo transcurrido: 67.4s\n",
            "[Train][Época 19/50] Batch 220/226  Loss batch: 0.201157  Tiempo transcurrido: 70.4s\n",
            "[Train] Época 19 terminada en 71.9s  Loss media: 0.265003\n",
            "[Val][Época 19/50] Batch 0/65  Loss batch: 7.773981  Tiempo transcurrido: 0.3s\n",
            "[Val][Época 19/50] Batch 10/65  Loss batch: 8.035414  Tiempo transcurrido: 3.3s\n",
            "[Val][Época 19/50] Batch 20/65  Loss batch: 27.147751  Tiempo transcurrido: 5.7s\n",
            "[Val][Época 19/50] Batch 30/65  Loss batch: 61.301170  Tiempo transcurrido: 8.3s\n",
            "[Val][Época 19/50] Batch 40/65  Loss batch: 82.487610  Tiempo transcurrido: 11.0s\n",
            "[Val][Época 19/50] Batch 50/65  Loss batch: 111.194206  Tiempo transcurrido: 13.9s\n",
            "[Val][Época 19/50] Batch 60/65  Loss batch: 106.131958  Tiempo transcurrido: 17.0s\n",
            "[Val] Época 19 terminada en 17.8s  Loss media: 68.888626\n",
            "Época [19/50] Train Loss: 0.265003 | Val Loss: 68.888626\n",
            "============================================================\n",
            "Comenzando época 20/50\n",
            "[Train][Época 20/50] Batch 0/226  Loss batch: 0.175374  Tiempo transcurrido: 0.3s\n",
            "[Train][Época 20/50] Batch 10/226  Loss batch: 0.320665  Tiempo transcurrido: 3.4s\n",
            "[Train][Época 20/50] Batch 20/226  Loss batch: 0.210023  Tiempo transcurrido: 6.4s\n",
            "[Train][Época 20/50] Batch 30/226  Loss batch: 0.309896  Tiempo transcurrido: 10.0s\n",
            "[Train][Época 20/50] Batch 40/226  Loss batch: 0.388093  Tiempo transcurrido: 13.2s\n",
            "[Train][Época 20/50] Batch 50/226  Loss batch: 0.242885  Tiempo transcurrido: 16.2s\n",
            "[Train][Época 20/50] Batch 60/226  Loss batch: 0.261078  Tiempo transcurrido: 19.2s\n",
            "[Train][Época 20/50] Batch 70/226  Loss batch: 0.274158  Tiempo transcurrido: 22.6s\n",
            "[Train][Época 20/50] Batch 80/226  Loss batch: 0.214215  Tiempo transcurrido: 25.9s\n",
            "[Train][Época 20/50] Batch 90/226  Loss batch: 0.287888  Tiempo transcurrido: 28.9s\n",
            "[Train][Época 20/50] Batch 100/226  Loss batch: 0.243056  Tiempo transcurrido: 31.9s\n",
            "[Train][Época 20/50] Batch 110/226  Loss batch: 0.225776  Tiempo transcurrido: 35.3s\n",
            "[Train][Época 20/50] Batch 120/226  Loss batch: 0.176423  Tiempo transcurrido: 38.8s\n",
            "[Train][Época 20/50] Batch 130/226  Loss batch: 0.218435  Tiempo transcurrido: 41.8s\n",
            "[Train][Época 20/50] Batch 140/226  Loss batch: 0.238195  Tiempo transcurrido: 44.8s\n",
            "[Train][Época 20/50] Batch 150/226  Loss batch: 0.268004  Tiempo transcurrido: 48.1s\n",
            "[Train][Época 20/50] Batch 160/226  Loss batch: 0.181698  Tiempo transcurrido: 51.6s\n",
            "[Train][Época 20/50] Batch 170/226  Loss batch: 0.201481  Tiempo transcurrido: 54.7s\n",
            "[Train][Época 20/50] Batch 180/226  Loss batch: 0.306071  Tiempo transcurrido: 57.7s\n",
            "[Train][Época 20/50] Batch 190/226  Loss batch: 0.257990  Tiempo transcurrido: 61.0s\n",
            "[Train][Época 20/50] Batch 200/226  Loss batch: 0.334893  Tiempo transcurrido: 64.5s\n",
            "[Train][Época 20/50] Batch 210/226  Loss batch: 0.242806  Tiempo transcurrido: 67.5s\n",
            "[Train][Época 20/50] Batch 220/226  Loss batch: 0.226695  Tiempo transcurrido: 70.5s\n",
            "[Train] Época 20 terminada en 71.8s  Loss media: 0.255660\n",
            "[Val][Época 20/50] Batch 0/65  Loss batch: 8.531634  Tiempo transcurrido: 0.2s\n",
            "[Val][Época 20/50] Batch 10/65  Loss batch: 8.116058  Tiempo transcurrido: 3.1s\n",
            "[Val][Época 20/50] Batch 20/65  Loss batch: 21.049532  Tiempo transcurrido: 5.8s\n",
            "[Val][Época 20/50] Batch 30/65  Loss batch: 60.029991  Tiempo transcurrido: 8.4s\n",
            "[Val][Época 20/50] Batch 40/65  Loss batch: 88.068039  Tiempo transcurrido: 11.2s\n",
            "[Val][Época 20/50] Batch 50/65  Loss batch: 111.685440  Tiempo transcurrido: 13.7s\n",
            "[Val][Época 20/50] Batch 60/65  Loss batch: 106.798805  Tiempo transcurrido: 17.1s\n",
            "[Val] Época 20 terminada en 18.1s  Loss media: 68.495173\n",
            "Época [20/50] Train Loss: 0.255660 | Val Loss: 68.495173\n",
            "============================================================\n",
            "Comenzando época 21/50\n",
            "[Train][Época 21/50] Batch 0/226  Loss batch: 0.431803  Tiempo transcurrido: 0.3s\n",
            "[Train][Época 21/50] Batch 10/226  Loss batch: 0.238496  Tiempo transcurrido: 3.4s\n",
            "[Train][Época 21/50] Batch 20/226  Loss batch: 0.184211  Tiempo transcurrido: 6.4s\n",
            "[Train][Época 21/50] Batch 30/226  Loss batch: 0.342177  Tiempo transcurrido: 9.5s\n",
            "[Train][Época 21/50] Batch 40/226  Loss batch: 0.257147  Tiempo transcurrido: 13.3s\n",
            "[Train][Época 21/50] Batch 50/226  Loss batch: 0.180497  Tiempo transcurrido: 16.3s\n",
            "[Train][Época 21/50] Batch 60/226  Loss batch: 0.285423  Tiempo transcurrido: 19.3s\n",
            "[Train][Época 21/50] Batch 70/226  Loss batch: 0.261292  Tiempo transcurrido: 22.3s\n",
            "[Train][Época 21/50] Batch 80/226  Loss batch: 0.272816  Tiempo transcurrido: 26.1s\n",
            "[Train][Época 21/50] Batch 90/226  Loss batch: 0.141516  Tiempo transcurrido: 29.1s\n",
            "[Train][Época 21/50] Batch 100/226  Loss batch: 0.272424  Tiempo transcurrido: 32.1s\n",
            "[Train][Época 21/50] Batch 110/226  Loss batch: 0.239708  Tiempo transcurrido: 35.2s\n",
            "[Train][Época 21/50] Batch 120/226  Loss batch: 0.224670  Tiempo transcurrido: 38.9s\n",
            "[Train][Época 21/50] Batch 130/226  Loss batch: 0.284459  Tiempo transcurrido: 41.9s\n",
            "[Train][Época 21/50] Batch 140/226  Loss batch: 0.226139  Tiempo transcurrido: 45.0s\n",
            "[Train][Época 21/50] Batch 150/226  Loss batch: 0.243568  Tiempo transcurrido: 48.0s\n",
            "[Train][Época 21/50] Batch 160/226  Loss batch: 3.345371  Tiempo transcurrido: 51.8s\n",
            "[Train][Época 21/50] Batch 170/226  Loss batch: 0.189659  Tiempo transcurrido: 54.8s\n",
            "[Train][Época 21/50] Batch 180/226  Loss batch: 0.207463  Tiempo transcurrido: 57.8s\n",
            "[Train][Época 21/50] Batch 190/226  Loss batch: 0.232347  Tiempo transcurrido: 60.8s\n",
            "[Train][Época 21/50] Batch 200/226  Loss batch: 0.240194  Tiempo transcurrido: 64.7s\n",
            "[Train][Época 21/50] Batch 210/226  Loss batch: 0.237091  Tiempo transcurrido: 67.7s\n",
            "[Train][Época 21/50] Batch 220/226  Loss batch: 0.323260  Tiempo transcurrido: 70.7s\n",
            "[Train] Época 21 terminada en 72.0s  Loss media: 0.252085\n",
            "[Val][Época 21/50] Batch 0/65  Loss batch: 7.598524  Tiempo transcurrido: 0.2s\n",
            "[Val][Época 21/50] Batch 10/65  Loss batch: 7.225109  Tiempo transcurrido: 2.5s\n",
            "[Val][Época 21/50] Batch 20/65  Loss batch: 23.890108  Tiempo transcurrido: 5.8s\n",
            "[Val][Época 21/50] Batch 30/65  Loss batch: 60.040592  Tiempo transcurrido: 8.4s\n",
            "[Val][Época 21/50] Batch 40/65  Loss batch: 80.349060  Tiempo transcurrido: 11.2s\n",
            "[Val][Época 21/50] Batch 50/65  Loss batch: 111.193161  Tiempo transcurrido: 13.7s\n",
            "[Val][Época 21/50] Batch 60/65  Loss batch: 106.733566  Tiempo transcurrido: 16.5s\n",
            "[Val] Época 21 terminada en 17.7s  Loss media: 68.153786\n",
            "Época [21/50] Train Loss: 0.252085 | Val Loss: 68.153786\n",
            "============================================================\n",
            "Comenzando época 22/50\n",
            "[Train][Época 22/50] Batch 0/226  Loss batch: 0.176797  Tiempo transcurrido: 0.5s\n",
            "[Train][Época 22/50] Batch 10/226  Loss batch: 0.128565  Tiempo transcurrido: 3.7s\n",
            "[Train][Época 22/50] Batch 20/226  Loss batch: 0.194446  Tiempo transcurrido: 6.8s\n",
            "[Train][Época 22/50] Batch 30/226  Loss batch: 0.221966  Tiempo transcurrido: 9.8s\n",
            "[Train][Época 22/50] Batch 40/226  Loss batch: 0.225028  Tiempo transcurrido: 13.3s\n",
            "[Train][Época 22/50] Batch 50/226  Loss batch: 0.220041  Tiempo transcurrido: 16.6s\n",
            "[Train][Época 22/50] Batch 60/226  Loss batch: 0.201346  Tiempo transcurrido: 19.6s\n",
            "[Train][Época 22/50] Batch 70/226  Loss batch: 0.244341  Tiempo transcurrido: 22.6s\n",
            "[Train][Época 22/50] Batch 80/226  Loss batch: 0.242770  Tiempo transcurrido: 26.1s\n",
            "[Train][Época 22/50] Batch 90/226  Loss batch: 0.278936  Tiempo transcurrido: 29.4s\n",
            "[Train][Época 22/50] Batch 100/226  Loss batch: 0.211986  Tiempo transcurrido: 32.4s\n",
            "[Train][Época 22/50] Batch 110/226  Loss batch: 0.163380  Tiempo transcurrido: 35.5s\n",
            "[Train][Época 22/50] Batch 120/226  Loss batch: 0.251128  Tiempo transcurrido: 38.9s\n",
            "[Train][Época 22/50] Batch 130/226  Loss batch: 0.276996  Tiempo transcurrido: 42.3s\n",
            "[Train][Época 22/50] Batch 140/226  Loss batch: 0.261337  Tiempo transcurrido: 45.3s\n",
            "[Train][Época 22/50] Batch 150/226  Loss batch: 0.265342  Tiempo transcurrido: 48.3s\n",
            "[Train][Época 22/50] Batch 160/226  Loss batch: 0.324405  Tiempo transcurrido: 51.7s\n",
            "[Train][Época 22/50] Batch 170/226  Loss batch: 0.208277  Tiempo transcurrido: 55.2s\n",
            "[Train][Época 22/50] Batch 180/226  Loss batch: 0.273723  Tiempo transcurrido: 58.3s\n",
            "[Train][Época 22/50] Batch 190/226  Loss batch: 0.183846  Tiempo transcurrido: 61.3s\n",
            "[Train][Época 22/50] Batch 200/226  Loss batch: 0.133174  Tiempo transcurrido: 64.6s\n",
            "[Train][Época 22/50] Batch 210/226  Loss batch: 0.191100  Tiempo transcurrido: 68.2s\n",
            "[Train][Época 22/50] Batch 220/226  Loss batch: 0.298688  Tiempo transcurrido: 71.2s\n",
            "[Train] Época 22 terminada en 72.5s  Loss media: 0.243166\n",
            "[Val][Época 22/50] Batch 0/65  Loss batch: 9.009456  Tiempo transcurrido: 0.2s\n",
            "[Val][Época 22/50] Batch 10/65  Loss batch: 8.278972  Tiempo transcurrido: 2.6s\n",
            "[Val][Época 22/50] Batch 20/65  Loss batch: 22.483337  Tiempo transcurrido: 5.3s\n",
            "[Val][Época 22/50] Batch 30/65  Loss batch: 60.656731  Tiempo transcurrido: 8.4s\n",
            "[Val][Época 22/50] Batch 40/65  Loss batch: 82.933945  Tiempo transcurrido: 11.2s\n",
            "[Val][Época 22/50] Batch 50/65  Loss batch: 110.909309  Tiempo transcurrido: 13.7s\n",
            "[Val][Época 22/50] Batch 60/65  Loss batch: 103.188065  Tiempo transcurrido: 16.3s\n",
            "[Val] Época 22 terminada en 17.1s  Loss media: 69.105318\n",
            "Época [22/50] Train Loss: 0.243166 | Val Loss: 69.105318\n",
            "============================================================\n",
            "Comenzando época 23/50\n",
            "[Train][Época 23/50] Batch 0/226  Loss batch: 0.211865  Tiempo transcurrido: 0.4s\n",
            "[Train][Época 23/50] Batch 10/226  Loss batch: 0.303569  Tiempo transcurrido: 4.1s\n",
            "[Train][Época 23/50] Batch 20/226  Loss batch: 0.182046  Tiempo transcurrido: 7.1s\n",
            "[Train][Época 23/50] Batch 30/226  Loss batch: 0.297136  Tiempo transcurrido: 10.1s\n",
            "[Train][Época 23/50] Batch 40/226  Loss batch: 0.200730  Tiempo transcurrido: 13.1s\n",
            "[Train][Época 23/50] Batch 50/226  Loss batch: 0.214308  Tiempo transcurrido: 16.9s\n",
            "[Train][Época 23/50] Batch 60/226  Loss batch: 0.165995  Tiempo transcurrido: 19.9s\n",
            "[Train][Época 23/50] Batch 70/226  Loss batch: 0.222722  Tiempo transcurrido: 22.9s\n",
            "[Train][Época 23/50] Batch 80/226  Loss batch: 0.239893  Tiempo transcurrido: 25.9s\n",
            "[Train][Época 23/50] Batch 90/226  Loss batch: 0.280710  Tiempo transcurrido: 29.7s\n",
            "[Train][Época 23/50] Batch 100/226  Loss batch: 0.201926  Tiempo transcurrido: 32.8s\n",
            "[Train][Época 23/50] Batch 110/226  Loss batch: 0.268572  Tiempo transcurrido: 35.8s\n",
            "[Train][Época 23/50] Batch 120/226  Loss batch: 0.166429  Tiempo transcurrido: 38.8s\n",
            "[Train][Época 23/50] Batch 130/226  Loss batch: 0.198840  Tiempo transcurrido: 42.6s\n",
            "[Train][Época 23/50] Batch 140/226  Loss batch: 0.291266  Tiempo transcurrido: 45.6s\n",
            "[Train][Época 23/50] Batch 150/226  Loss batch: 0.188437  Tiempo transcurrido: 48.6s\n",
            "[Train][Época 23/50] Batch 160/226  Loss batch: 0.189656  Tiempo transcurrido: 51.7s\n",
            "[Train][Época 23/50] Batch 170/226  Loss batch: 0.222607  Tiempo transcurrido: 55.5s\n",
            "[Train][Época 23/50] Batch 180/226  Loss batch: 0.339839  Tiempo transcurrido: 58.5s\n",
            "[Train][Época 23/50] Batch 190/226  Loss batch: 0.226210  Tiempo transcurrido: 61.5s\n",
            "[Train][Época 23/50] Batch 200/226  Loss batch: 0.233925  Tiempo transcurrido: 64.5s\n",
            "[Train][Época 23/50] Batch 210/226  Loss batch: 0.303085  Tiempo transcurrido: 68.3s\n",
            "[Train][Época 23/50] Batch 220/226  Loss batch: 0.193465  Tiempo transcurrido: 71.3s\n",
            "[Train] Época 23 terminada en 72.6s  Loss media: 0.245793\n",
            "[Val][Época 23/50] Batch 0/65  Loss batch: 8.912032  Tiempo transcurrido: 0.2s\n",
            "[Val][Época 23/50] Batch 10/65  Loss batch: 7.947424  Tiempo transcurrido: 2.6s\n",
            "[Val][Época 23/50] Batch 20/65  Loss batch: 15.978157  Tiempo transcurrido: 4.9s\n",
            "[Val][Época 23/50] Batch 30/65  Loss batch: 60.768288  Tiempo transcurrido: 8.2s\n",
            "[Val][Época 23/50] Batch 40/65  Loss batch: 91.111084  Tiempo transcurrido: 11.2s\n",
            "[Val][Época 23/50] Batch 50/65  Loss batch: 111.133102  Tiempo transcurrido: 13.7s\n",
            "[Val][Época 23/50] Batch 60/65  Loss batch: 107.720558  Tiempo transcurrido: 16.3s\n",
            "[Val] Época 23 terminada en 17.1s  Loss media: 66.234319\n",
            "Época [23/50] Train Loss: 0.245793 | Val Loss: 66.234319\n",
            "============================================================\n",
            "Comenzando época 24/50\n",
            "[Train][Época 24/50] Batch 0/226  Loss batch: 0.256643  Tiempo transcurrido: 0.3s\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3452228010.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Comenzando época {epoch}/{NUM_EPOCHS}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     train_loss = train_one_epoch(model, train_loader, optimizer, DEVICE,\n\u001b[0m\u001b[1;32m     11\u001b[0m                                  epoch_idx=epoch, num_epochs=NUM_EPOCHS)\n\u001b[1;32m     12\u001b[0m     val_loss   = eval_one_epoch(model, val_loader, DEVICE,\n",
            "\u001b[0;32m/tmp/ipython-input-2282917816.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, optimizer, device, epoch_idx, num_epochs)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mposes\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mposes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-817705759.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mpose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \"\"\"\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_float_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# ============================================================\n",
        "# Utilidades\n",
        "# ============================================================\n",
        "\n",
        "def quaternion_to_angle_deg(q1, q2):\n",
        "    \"\"\"\n",
        "    Calcula el ángulo (en grados) entre dos cuaterniones.\n",
        "    q1 y q2 deben ser arrays de 4 elementos normalizados.\n",
        "    \"\"\"\n",
        "    q1 = q1 / (np.linalg.norm(q1) + 1e-8)\n",
        "    q2 = q2 / (np.linalg.norm(q2) + 1e-8)\n",
        "\n",
        "    dot = np.clip(np.dot(q1, q2), -1.0, 1.0)\n",
        "\n",
        "    # ángulo entre cuaterniones\n",
        "    angle_rad = 2 * np.arccos(abs(dot))\n",
        "    angle_deg = np.degrees(angle_rad)\n",
        "    return angle_deg\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Cargar el mejor modelo guardado\n",
        "# ============================================================\n",
        "\n",
        "checkpoint = torch.load(best_model_path, map_location=DEVICE)\n",
        "best_model = PoseNetLight(pretrained=False).to(DEVICE)\n",
        "best_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "best_model.eval()\n",
        "\n",
        "print(\"Mejor modelo cargado:\")\n",
        "print(\"  Época:\", checkpoint[\"epoch\"])\n",
        "print(\"  Val Loss:\", checkpoint[\"val_loss\"])\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Seleccionar dataset a evaluar\n",
        "# ============================================================\n",
        "\n",
        "# Puedes cambiar val_dataset por test_dataset si después haces split test.\n",
        "eval_dataset = val_dataset\n",
        "eval_loader = DataLoader(eval_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
        "\n",
        "print(f\"Evaluando en split con {len(eval_dataset)} muestras...\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Evaluación\n",
        "# ============================================================\n",
        "\n",
        "transl_errors = []       # distancia euclidiana |t_pred - t_gt|\n",
        "transl_err_x = []\n",
        "transl_err_y = []\n",
        "transl_err_z = []\n",
        "\n",
        "rot_errors_deg = []      # error angular entre cuaterniones\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, poses_gt in eval_loader:\n",
        "        images = images.to(DEVICE)\n",
        "        poses_gt = poses_gt.to(DEVICE)\n",
        "\n",
        "        poses_pred = best_model(images)\n",
        "\n",
        "        # separar posición y cuaterniones\n",
        "        t_pred = poses_pred[:, :3].cpu().numpy()\n",
        "        q_pred = poses_pred[:, 3:].cpu().numpy()\n",
        "        t_gt   = poses_gt[:, :3].cpu().numpy()\n",
        "        q_gt   = poses_gt[:, 3:].cpu().numpy()\n",
        "\n",
        "        # normalizar cuaterniones predichos\n",
        "        q_pred = q_pred / (np.linalg.norm(q_pred, axis=1, keepdims=True) + 1e-8)\n",
        "        q_gt   = q_gt / (np.linalg.norm(q_gt, axis=1, keepdims=True) + 1e-8)\n",
        "\n",
        "        # errores de traslación\n",
        "        diff = t_pred - t_gt\n",
        "        dist = np.linalg.norm(diff, axis=1)\n",
        "\n",
        "        transl_errors.extend(dist)\n",
        "        transl_err_x.extend(np.abs(diff[:, 0]))\n",
        "        transl_err_y.extend(np.abs(diff[:, 1]))\n",
        "        transl_err_z.extend(np.abs(diff[:, 2]))\n",
        "\n",
        "        # errores de rotación\n",
        "        for qp, qg in zip(q_pred, q_gt):\n",
        "            rot_errors_deg.append(quaternion_to_angle_deg(qp, qg))\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Estadísticas finales\n",
        "# ============================================================\n",
        "\n",
        "transl_errors = np.array(transl_errors)\n",
        "transl_err_x = np.array(transl_err_x)\n",
        "transl_err_y = np.array(transl_err_y)\n",
        "transl_err_z = np.array(transl_err_z)\n",
        "rot_errors_deg = np.array(rot_errors_deg)\n",
        "\n",
        "print(\"\\n====================================================\")\n",
        "print(\"          📊 RESULTADOS DE EVALUACIÓN\")\n",
        "print(\"====================================================\\n\")\n",
        "\n",
        "print(f\"Total muestras evaluadas: {len(transl_errors)}\")\n",
        "\n",
        "print(\"\\n--- TRASLACIÓN (m) ---\")\n",
        "print(f\"Error medio (L2):        {np.mean(transl_errors):.4f} m\")\n",
        "print(f\"Error mediano:           {np.median(transl_errors):.4f} m\")\n",
        "print(f\"RMSE:                    {np.sqrt(np.mean(transl_errors**2)):.4f} m\")\n",
        "print(f\"Error medio |dx|:        {np.mean(transl_err_x):.4f} m\")\n",
        "print(f\"Error medio |dy|:        {np.mean(transl_err_y):.4f} m\")\n",
        "print(f\"Error medio |dz|:        {np.mean(transl_err_z):.4f} m\")\n",
        "\n",
        "print(\"\\n--- ORIENTACIÓN (grados) ---\")\n",
        "print(f\"Error angular medio:     {np.mean(rot_errors_deg):.3f}°\")\n",
        "print(f\"Error angular mediano:   {np.median(rot_errors_deg):.3f}°\")\n",
        "print(f\"Error angular RMS:       {np.sqrt(np.mean(rot_errors_deg**2)):.3f}°\")\n",
        "\n",
        "print(\"\\n====================================================\")\n",
        "print(\"Evaluación completada.\")\n",
        "print(\"====================================================\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XlxXJvyokVC",
        "outputId": "2580236e-2e6a-4733-84a4-6dcdfea510bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mejor modelo cargado:\n",
            "  Época: 12\n",
            "  Val Loss: 64.61286463387452\n",
            "Evaluando en split con 2057 muestras...\n",
            "\n",
            "====================================================\n",
            "          📊 RESULTADOS DE EVALUACIÓN\n",
            "====================================================\n",
            "\n",
            "Total muestras evaluadas: 2057\n",
            "\n",
            "--- TRASLACIÓN (m) ---\n",
            "Error medio (L2):        5.6638 m\n",
            "Error mediano:           5.4697 m\n",
            "RMSE:                    5.9123 m\n",
            "Error medio |dx|:        5.1818 m\n",
            "Error medio |dy|:        1.8157 m\n",
            "Error medio |dz|:        0.2429 m\n",
            "\n",
            "--- ORIENTACIÓN (grados) ---\n",
            "Error angular medio:     65.186°\n",
            "Error angular mediano:   31.132°\n",
            "Error angular RMS:       91.815°\n",
            "\n",
            "====================================================\n",
            "Evaluación completada.\n",
            "====================================================\n"
          ]
        }
      ]
    }
  ]
}