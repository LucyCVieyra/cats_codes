{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPjW92oapLFuyfwip6k2G4t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LucyCVieyra/cats_codes/blob/main/cats_pose_v04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 1\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# dataset\n",
        "DRIVE_DATASET_ROOT = \"/content/drive/MyDrive/cats_dataset\"\n",
        "print(\"DRIVE_DATASET_ROOT =\", DRIVE_DATASET_ROOT)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHF1F-FgGKfx",
        "outputId": "90a600d0-1e5b-4c42-9867-96762a223d3b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "DRIVE_DATASET_ROOT = /content/drive/MyDrive/cats_dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 2\n",
        "\n",
        "import os\n",
        "import csv\n",
        "from typing import Tuple\n",
        "import shutil\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "\n",
        "# copia el dataset de Drive a disco local\n",
        "LOCAL_DATASET_ROOT = \"/content/cats_dataset\"\n",
        "\n",
        "if not os.path.exists(LOCAL_DATASET_ROOT):\n",
        "    print(\"Copiando dataset de Drive a disco local (solo se hace una vez)...\")\n",
        "    shutil.copytree(DRIVE_DATASET_ROOT, LOCAL_DATASET_ROOT)\n",
        "    print(\"Copia terminada.\")\n",
        "else:\n",
        "    print(\"Ya existe copia local en\", LOCAL_DATASET_ROOT)\n",
        "\n",
        "# A partir de aquí se usa la copia local\n",
        "DATASET_ROOT = LOCAL_DATASET_ROOT\n",
        "print(\"DATASET_ROOT =\", DATASET_ROOT)\n",
        "\n",
        "# Ruta al CSV dentro del dataset\n",
        "CSV_PATH = os.path.join(DATASET_ROOT, \"dataset.csv\")\n",
        "print(\"CSV_PATH:\", CSV_PATH)\n",
        "\n",
        "# Normalización de posición\n",
        "# Escalas en metros para normalizar x,y,z dentro del Dataset\n",
        "POS_SCALE = torch.tensor([5.0, 5.0, 3.0], dtype=torch.float32)  # [sx, sy, sz]\n",
        "print(\"POS_SCALE (m):\", POS_SCALE.tolist())\n",
        "\n",
        "# Hiperparámetros\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 50\n",
        "LEARNING_RATE = 1e-4\n",
        "BETA_ROT = 10.0 # 100.0\n",
        "IMAGE_SIZE = 224\n",
        "NUM_WORKERS = 0\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Usando dispositivo:\", DEVICE)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtP27V3_GQmI",
        "outputId": "81047f11-bf04-40fb-b1f6-0767c53f15fa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ya existe copia local en /content/cats_dataset\n",
            "DATASET_ROOT = /content/cats_dataset\n",
            "CSV_PATH: /content/cats_dataset/dataset.csv\n",
            "POS_SCALE (m): [5.0, 5.0, 3.0]\n",
            "Usando dispositivo: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 3\n",
        "\n",
        "class PoseDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Lee dataset.csv y carga imágenes + vector de pose (x,y,z,q0,q1,q2,q3)\n",
        "    Filtra por split: 'train', 'val' o 'test'.\n",
        "    Normaliza posición (x,y,z) dividiendo entre POS_SCALE.\n",
        "    \"\"\"\n",
        "    def __init__(self, csv_path: str, root_dir: str, split: str = \"train\", transform=None,\n",
        "                 pos_scale: torch.Tensor = POS_SCALE, normalize_pos: bool = True):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.pos_scale = pos_scale\n",
        "        self.normalize_pos = normalize_pos\n",
        "        self.samples = []\n",
        "\n",
        "        with open(csv_path, \"r\") as f:\n",
        "            reader = csv.DictReader(f)\n",
        "            for row in reader:\n",
        "                if row[\"split\"] != split:\n",
        "                    continue\n",
        "\n",
        "                image_path = row[\"image_path\"]\n",
        "                full_image_path = os.path.join(root_dir, image_path)\n",
        "\n",
        "                pose = [\n",
        "                    float(row[\"x\"]),\n",
        "                    float(row[\"y\"]),\n",
        "                    float(row[\"z\"]),\n",
        "                    float(row[\"q0\"]),\n",
        "                    float(row[\"q1\"]),\n",
        "                    float(row[\"q2\"]),\n",
        "                    float(row[\"q3\"]),\n",
        "                ]\n",
        "\n",
        "                self.samples.append((full_image_path, pose))\n",
        "\n",
        "        if len(self.samples) == 0:\n",
        "            raise RuntimeError(f\"No se encontraron muestras para split={split}. Revisa dataset.csv\")\n",
        "\n",
        "        print(f\"Split '{split}': {len(self.samples)} muestras\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        img_path, pose = self.samples[idx]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        pose = torch.tensor(pose, dtype=torch.float32)\n",
        "\n",
        "        # Normalizar solo traslación\n",
        "        if self.normalize_pos:\n",
        "            pose[:3] = pose[:3] / self.pos_scale\n",
        "\n",
        "        return img, pose\n",
        "\n",
        "\n",
        "# Transforms\n",
        "# Train: augmentation fotométrico + blur + ruido leve\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "\n",
        "    transforms.ColorJitter(brightness=0.25, contrast=0.25, saturation=0.20, hue=0.05),\n",
        "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 1.2)),\n",
        "\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: x + 0.02 * torch.randn_like(x)),  # ruido\n",
        "    transforms.Lambda(lambda x: torch.clamp(x, 0.0, 1.0)),\n",
        "\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Val: sin augment\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n"
      ],
      "metadata": {
        "id": "5iI25HsBG0ks"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 4\n",
        "\n",
        "# Crear datasets\n",
        "train_dataset = PoseDataset(CSV_PATH, DATASET_ROOT, split=\"train\", transform=train_transform)\n",
        "val_dataset   = PoseDataset(CSV_PATH, DATASET_ROOT, split=\"val\",   transform=val_transform)\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=False,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=False,\n",
        ")\n",
        "\n",
        "# Sanity check\n",
        "import time\n",
        "\n",
        "start = time.time()\n",
        "for i, (images, poses) in enumerate(train_loader):\n",
        "    print(f\"Batch {i} -> images {images.shape}, poses {poses.shape}\")\n",
        "    print(\"Ejemplo pose normalizada [x,y,z,q...]:\", poses[0].tolist())\n",
        "    if i == 2:\n",
        "        break\n",
        "end = time.time()\n",
        "print(\"Sanity check OK, tomó\", end - start, \"s\")\n",
        "\n",
        "# Ver una imagen\n",
        "example_path, _ = train_dataset.samples[0]\n",
        "print(\"Ejemplo de ruta de imagen:\", example_path)\n",
        "\n",
        "img = Image.open(example_path).convert(\"RGB\")\n",
        "display(img.resize((256, 256)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "F-wHZ1fqHRlo",
        "outputId": "4b5bc4a4-7bd5-4214-d7bb-903c6335fa02"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split 'train': 7209 muestras\n",
            "Split 'val': 2057 muestras\n",
            "Batch 0 -> images torch.Size([32, 3, 224, 224]), poses torch.Size([32, 7])\n",
            "Ejemplo pose normalizada [x,y,z,q...]: [-0.3422645926475525, -0.05936681479215622, -0.27305373549461365, -0.9961836338043213, 0.0014830853324383497, 0.001762723783031106, 0.08725260943174362]\n",
            "Batch 1 -> images torch.Size([32, 3, 224, 224]), poses torch.Size([32, 7])\n",
            "Ejemplo pose normalizada [x,y,z,q...]: [-1.1077326536178589, 0.30240944027900696, -0.5331458449363708, -0.9339587688446045, 0.002432357519865036, 0.004922534804791212, 0.35733869671821594]\n",
            "Batch 2 -> images torch.Size([32, 3, 224, 224]), poses torch.Size([32, 7])\n",
            "Ejemplo pose normalizada [x,y,z,q...]: [-1.1578714847564697, 0.34630221128463745, -0.8682782053947449, -0.9663085341453552, -0.015420353971421719, -0.006095589138567448, 0.2568519115447998]\n",
            "Sanity check OK, tomó 2.0072085857391357 s\n",
            "Ejemplo de ruta de imagen: /content/cats_dataset/trayectoria_1/images/image_000000.png\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=256x256>"
            ],
            "image/png": "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\n",
            "image/jpeg": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 5\n",
        "\n",
        "class PoseNetLight(nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        super().__init__()\n",
        "        weights = models.ResNet18_Weights.DEFAULT if pretrained else None\n",
        "        backbone = models.resnet18(weights=weights)\n",
        "\n",
        "        modules = list(backbone.children())[:-1]\n",
        "        self.backbone = nn.Sequential(*modules)\n",
        "        self.backbone_out_dim = backbone.fc.in_features\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.backbone_out_dim, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(128, 7)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return self.fc(x)\n",
        "\n",
        "\n",
        "def pose_loss(pred: torch.Tensor, target: torch.Tensor, beta: float = BETA_ROT) -> torch.Tensor:\n",
        "    t_pred = pred[:, :3]\n",
        "    q_pred = pred[:, 3:]\n",
        "    t_gt   = target[:, :3]\n",
        "    q_gt   = target[:, 3:]\n",
        "\n",
        "    # Normalizar cuaternión predicho\n",
        "    q_pred = q_pred / (torch.norm(q_pred, p=2, dim=1, keepdim=True) + 1e-8)\n",
        "\n",
        "    t_loss = nn.functional.mse_loss(t_pred, t_gt)\n",
        "    q_loss = nn.functional.mse_loss(q_pred, q_gt)\n",
        "\n",
        "    return t_loss + beta * q_loss\n",
        "\n",
        "\n",
        "model = PoseNetLight(pretrained=True).to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
        "\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Wf7gtHZHhd4",
        "outputId": "779417ba-e94c-436d-ba2e-5dbb0ebdbb7a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PoseNetLight(\n",
            "  (backbone): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (4): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (5): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (6): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (7): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  )\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Linear(in_features=128, out_features=7, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 6\n",
        "\n",
        "import time\n",
        "\n",
        "def train_one_epoch(model, dataloader, optimizer, device, epoch_idx=0, num_epochs=1):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    n_batches = len(dataloader)\n",
        "    start = time.time()\n",
        "\n",
        "    for i, (images, poses) in enumerate(dataloader):\n",
        "        images = images.to(device)\n",
        "        poses  = poses.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = pose_loss(outputs, poses)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        if (i % 10) == 0:\n",
        "            elapsed = time.time() - start\n",
        "            print(f\"[Train][Época {epoch_idx}/{num_epochs}] \"\n",
        "                  f\"Batch {i}/{n_batches}  Loss batch: {loss.item():.6f}  \"\n",
        "                  f\"Tiempo transcurrido: {elapsed:.1f}s\")\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    total_time = time.time() - start\n",
        "    print(f\"[Train] Época {epoch_idx} terminada en {total_time:.1f}s  Loss media: {epoch_loss:.6f}\")\n",
        "    return epoch_loss\n",
        "\n",
        "\n",
        "def eval_one_epoch(model, dataloader, device, epoch_idx=0, num_epochs=1):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    n_batches = len(dataloader)\n",
        "    start = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (images, poses) in enumerate(dataloader):\n",
        "            images = images.to(device)\n",
        "            poses  = poses.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = pose_loss(outputs, poses)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "            if (i % 10) == 0:\n",
        "                elapsed = time.time() - start\n",
        "                print(f\"[Val][Época {epoch_idx}/{num_epochs}] \"\n",
        "                      f\"Batch {i}/{n_batches}  Loss batch: {loss.item():.6f}  \"\n",
        "                      f\"Tiempo transcurrido: {elapsed:.1f}s\")\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    total_time = time.time() - start\n",
        "    print(f\"[Val] Época {epoch_idx} terminada en {total_time:.1f}s  Loss media: {epoch_loss:.6f}\")\n",
        "    return epoch_loss\n"
      ],
      "metadata": {
        "id": "ANzII6KpHpzV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 7\n",
        "\n",
        "NUM_EPOCHS = 30\n",
        "\n",
        "best_val_loss = float(\"inf\")\n",
        "best_model_path = os.path.join(DATASET_ROOT, \"posenet_light_best.pth\")\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Comenzando época {epoch}/{NUM_EPOCHS}\")\n",
        "\n",
        "    train_loss = train_one_epoch(model, train_loader, optimizer, DEVICE,\n",
        "                                 epoch_idx=epoch, num_epochs=NUM_EPOCHS)\n",
        "    val_loss   = eval_one_epoch(model, val_loader, DEVICE,\n",
        "                                epoch_idx=epoch, num_epochs=NUM_EPOCHS)\n",
        "\n",
        "    print(f\"Época [{epoch}/{NUM_EPOCHS}] \"\n",
        "          f\"Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save({\n",
        "            \"epoch\": epoch,\n",
        "            \"model_state_dict\": model.state_dict(),\n",
        "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "            \"val_loss\": val_loss,\n",
        "        }, best_model_path)\n",
        "        print(f\"  🟢 Nuevo mejor modelo guardado en: {best_model_path}\")\n",
        "\n",
        "print(\"Entrenamiento terminado. Mejor val_loss:\", best_val_loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLwhxHhUHurX",
        "outputId": "01d8f085-7a18-4ade-bd31-4382680a5efb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Comenzando época 1/30\n",
            "[Train][Época 1/30] Batch 0/226  Loss batch: 5.676468  Tiempo transcurrido: 2.0s\n",
            "[Train][Época 1/30] Batch 10/226  Loss batch: 1.547019  Tiempo transcurrido: 7.5s\n",
            "[Train][Época 1/30] Batch 20/226  Loss batch: 1.219752  Tiempo transcurrido: 12.3s\n",
            "[Train][Época 1/30] Batch 30/226  Loss batch: 0.727963  Tiempo transcurrido: 18.1s\n",
            "[Train][Época 1/30] Batch 40/226  Loss batch: 0.475738  Tiempo transcurrido: 23.0s\n",
            "[Train][Época 1/30] Batch 50/226  Loss batch: 0.366861  Tiempo transcurrido: 28.5s\n",
            "[Train][Época 1/30] Batch 60/226  Loss batch: 0.424339  Tiempo transcurrido: 33.7s\n",
            "[Train][Época 1/30] Batch 70/226  Loss batch: 0.289831  Tiempo transcurrido: 38.6s\n",
            "[Train][Época 1/30] Batch 80/226  Loss batch: 0.187684  Tiempo transcurrido: 44.4s\n",
            "[Train][Época 1/30] Batch 90/226  Loss batch: 0.546801  Tiempo transcurrido: 49.2s\n",
            "[Train][Época 1/30] Batch 100/226  Loss batch: 0.504127  Tiempo transcurrido: 54.6s\n",
            "[Train][Época 1/30] Batch 110/226  Loss batch: 0.173969  Tiempo transcurrido: 59.9s\n",
            "[Train][Época 1/30] Batch 120/226  Loss batch: 0.232103  Tiempo transcurrido: 64.8s\n",
            "[Train][Época 1/30] Batch 130/226  Loss batch: 0.343323  Tiempo transcurrido: 70.6s\n",
            "[Train][Época 1/30] Batch 140/226  Loss batch: 0.176236  Tiempo transcurrido: 75.9s\n",
            "[Train][Época 1/30] Batch 150/226  Loss batch: 0.482384  Tiempo transcurrido: 81.5s\n",
            "[Train][Época 1/30] Batch 160/226  Loss batch: 0.139087  Tiempo transcurrido: 86.7s\n",
            "[Train][Época 1/30] Batch 170/226  Loss batch: 0.361158  Tiempo transcurrido: 91.6s\n",
            "[Train][Época 1/30] Batch 180/226  Loss batch: 0.127310  Tiempo transcurrido: 97.4s\n",
            "[Train][Época 1/30] Batch 190/226  Loss batch: 0.490626  Tiempo transcurrido: 102.3s\n",
            "[Train][Época 1/30] Batch 200/226  Loss batch: 0.108750  Tiempo transcurrido: 107.7s\n",
            "[Train][Época 1/30] Batch 210/226  Loss batch: 0.126108  Tiempo transcurrido: 112.9s\n",
            "[Train][Época 1/30] Batch 220/226  Loss batch: 0.421333  Tiempo transcurrido: 117.7s\n",
            "[Train] Época 1 terminada en 120.3s  Loss media: 0.507141\n",
            "[Val][Época 1/30] Batch 0/65  Loss batch: 0.332640  Tiempo transcurrido: 0.3s\n",
            "[Val][Época 1/30] Batch 10/65  Loss batch: 2.611562  Tiempo transcurrido: 3.1s\n",
            "[Val][Época 1/30] Batch 20/65  Loss batch: 2.426908  Tiempo transcurrido: 5.5s\n",
            "[Val][Época 1/30] Batch 30/65  Loss batch: 7.517948  Tiempo transcurrido: 8.2s\n",
            "[Val][Época 1/30] Batch 40/65  Loss batch: 7.889425  Tiempo transcurrido: 11.0s\n",
            "[Val][Época 1/30] Batch 50/65  Loss batch: 10.304613  Tiempo transcurrido: 14.2s\n",
            "[Val][Época 1/30] Batch 60/65  Loss batch: 5.477428  Tiempo transcurrido: 17.1s\n",
            "[Val] Época 1 terminada en 17.9s  Loss media: 6.503704\n",
            "Época [1/30] Train Loss: 0.507141 | Val Loss: 6.503704\n",
            "  🟢 Nuevo mejor modelo guardado en: /content/cats_dataset/posenet_light_best.pth\n",
            "============================================================\n",
            "Comenzando época 2/30\n",
            "[Train][Época 2/30] Batch 0/226  Loss batch: 0.171183  Tiempo transcurrido: 0.5s\n",
            "[Train][Época 2/30] Batch 10/226  Loss batch: 0.069518  Tiempo transcurrido: 5.4s\n",
            "[Train][Época 2/30] Batch 20/226  Loss batch: 0.115279  Tiempo transcurrido: 11.3s\n",
            "[Train][Época 2/30] Batch 30/226  Loss batch: 0.401636  Tiempo transcurrido: 16.1s\n",
            "[Train][Época 2/30] Batch 40/226  Loss batch: 0.116713  Tiempo transcurrido: 21.4s\n",
            "[Train][Época 2/30] Batch 50/226  Loss batch: 0.094130  Tiempo transcurrido: 26.9s\n",
            "[Train][Época 2/30] Batch 60/226  Loss batch: 0.402614  Tiempo transcurrido: 31.8s\n",
            "[Train][Época 2/30] Batch 70/226  Loss batch: 0.063642  Tiempo transcurrido: 37.6s\n",
            "[Train][Época 2/30] Batch 80/226  Loss batch: 0.092201  Tiempo transcurrido: 42.5s\n",
            "[Train][Época 2/30] Batch 90/226  Loss batch: 0.107487  Tiempo transcurrido: 47.7s\n",
            "[Train][Época 2/30] Batch 100/226  Loss batch: 0.382689  Tiempo transcurrido: 53.2s\n",
            "[Train][Época 2/30] Batch 110/226  Loss batch: 0.092053  Tiempo transcurrido: 58.1s\n",
            "[Train][Época 2/30] Batch 120/226  Loss batch: 0.066784  Tiempo transcurrido: 64.0s\n",
            "[Train][Época 2/30] Batch 130/226  Loss batch: 0.395637  Tiempo transcurrido: 68.8s\n",
            "[Train][Época 2/30] Batch 140/226  Loss batch: 0.102002  Tiempo transcurrido: 74.0s\n",
            "[Train][Época 2/30] Batch 150/226  Loss batch: 0.143223  Tiempo transcurrido: 79.5s\n",
            "[Train][Época 2/30] Batch 160/226  Loss batch: 0.086580  Tiempo transcurrido: 84.4s\n",
            "[Train][Época 2/30] Batch 170/226  Loss batch: 0.717342  Tiempo transcurrido: 90.2s\n",
            "[Train][Época 2/30] Batch 180/226  Loss batch: 0.680166  Tiempo transcurrido: 95.1s\n",
            "[Train][Época 2/30] Batch 190/226  Loss batch: 0.071152  Tiempo transcurrido: 100.1s\n",
            "[Train][Época 2/30] Batch 200/226  Loss batch: 0.080518  Tiempo transcurrido: 105.7s\n",
            "[Train][Época 2/30] Batch 210/226  Loss batch: 0.129242  Tiempo transcurrido: 110.6s\n",
            "[Train][Época 2/30] Batch 220/226  Loss batch: 0.069231  Tiempo transcurrido: 116.4s\n",
            "[Train] Época 2 terminada en 118.5s  Loss media: 0.137619\n",
            "[Val][Época 2/30] Batch 0/65  Loss batch: 0.156926  Tiempo transcurrido: 0.2s\n",
            "[Val][Época 2/30] Batch 10/65  Loss batch: 0.928082  Tiempo transcurrido: 2.6s\n",
            "[Val][Época 2/30] Batch 20/65  Loss batch: 1.502469  Tiempo transcurrido: 5.0s\n",
            "[Val][Época 2/30] Batch 30/65  Loss batch: 5.745873  Tiempo transcurrido: 7.8s\n",
            "[Val][Época 2/30] Batch 40/65  Loss batch: 6.018734  Tiempo transcurrido: 11.3s\n",
            "[Val][Época 2/30] Batch 50/65  Loss batch: 10.462723  Tiempo transcurrido: 13.8s\n",
            "[Val][Época 2/30] Batch 60/65  Loss batch: 9.626455  Tiempo transcurrido: 16.4s\n",
            "[Val] Época 2 terminada en 17.3s  Loss media: 5.872232\n",
            "Época [2/30] Train Loss: 0.137619 | Val Loss: 5.872232\n",
            "  🟢 Nuevo mejor modelo guardado en: /content/cats_dataset/posenet_light_best.pth\n",
            "============================================================\n",
            "Comenzando época 3/30\n",
            "[Train][Época 3/30] Batch 0/226  Loss batch: 0.360525  Tiempo transcurrido: 0.6s\n",
            "[Train][Época 3/30] Batch 10/226  Loss batch: 0.082019  Tiempo transcurrido: 6.5s\n",
            "[Train][Época 3/30] Batch 20/226  Loss batch: 0.069719  Tiempo transcurrido: 11.4s\n",
            "[Train][Época 3/30] Batch 30/226  Loss batch: 0.096145  Tiempo transcurrido: 16.3s\n",
            "[Train][Época 3/30] Batch 40/226  Loss batch: 0.073029  Tiempo transcurrido: 22.6s\n",
            "[Train][Época 3/30] Batch 50/226  Loss batch: 0.068428  Tiempo transcurrido: 27.5s\n",
            "[Train][Época 3/30] Batch 60/226  Loss batch: 0.059905  Tiempo transcurrido: 33.3s\n",
            "[Train][Época 3/30] Batch 70/226  Loss batch: 0.535619  Tiempo transcurrido: 38.2s\n",
            "[Train][Época 3/30] Batch 80/226  Loss batch: 0.057116  Tiempo transcurrido: 43.2s\n",
            "[Train][Época 3/30] Batch 90/226  Loss batch: 0.062426  Tiempo transcurrido: 49.6s\n",
            "[Train][Época 3/30] Batch 100/226  Loss batch: 0.067343  Tiempo transcurrido: 54.5s\n",
            "[Train][Época 3/30] Batch 110/226  Loss batch: 0.051980  Tiempo transcurrido: 60.3s\n",
            "[Train][Época 3/30] Batch 120/226  Loss batch: 0.064623  Tiempo transcurrido: 65.2s\n",
            "[Train][Época 3/30] Batch 130/226  Loss batch: 0.084035  Tiempo transcurrido: 70.4s\n",
            "[Train][Época 3/30] Batch 140/226  Loss batch: 0.062603  Tiempo transcurrido: 76.0s\n",
            "[Train][Época 3/30] Batch 150/226  Loss batch: 0.083423  Tiempo transcurrido: 80.8s\n",
            "[Train][Época 3/30] Batch 160/226  Loss batch: 0.065742  Tiempo transcurrido: 86.6s\n",
            "[Train][Época 3/30] Batch 170/226  Loss batch: 0.054712  Tiempo transcurrido: 91.5s\n",
            "[Train][Época 3/30] Batch 180/226  Loss batch: 0.053314  Tiempo transcurrido: 96.6s\n",
            "[Train][Época 3/30] Batch 190/226  Loss batch: 0.065403  Tiempo transcurrido: 102.1s\n",
            "[Train][Época 3/30] Batch 200/226  Loss batch: 0.056351  Tiempo transcurrido: 107.0s\n",
            "[Train][Época 3/30] Batch 210/226  Loss batch: 0.042830  Tiempo transcurrido: 112.9s\n",
            "[Train][Época 3/30] Batch 220/226  Loss batch: 0.056150  Tiempo transcurrido: 117.8s\n",
            "[Train] Época 3 terminada en 120.0s  Loss media: 0.094870\n",
            "[Val][Época 3/30] Batch 0/65  Loss batch: 0.154658  Tiempo transcurrido: 0.2s\n",
            "[Val][Época 3/30] Batch 10/65  Loss batch: 0.807209  Tiempo transcurrido: 2.7s\n",
            "[Val][Época 3/30] Batch 20/65  Loss batch: 2.137246  Tiempo transcurrido: 5.9s\n",
            "[Val][Época 3/30] Batch 30/65  Loss batch: 5.444070  Tiempo transcurrido: 8.6s\n",
            "[Val][Época 3/30] Batch 40/65  Loss batch: 6.586136  Tiempo transcurrido: 11.3s\n",
            "[Val][Época 3/30] Batch 50/65  Loss batch: 10.418208  Tiempo transcurrido: 13.8s\n",
            "[Val][Época 3/30] Batch 60/65  Loss batch: 9.166639  Tiempo transcurrido: 16.9s\n",
            "[Val] Época 3 terminada en 18.0s  Loss media: 5.955932\n",
            "Época [3/30] Train Loss: 0.094870 | Val Loss: 5.955932\n",
            "============================================================\n",
            "Comenzando época 4/30\n",
            "[Train][Época 4/30] Batch 0/226  Loss batch: 0.061496  Tiempo transcurrido: 0.8s\n",
            "[Train][Época 4/30] Batch 10/226  Loss batch: 0.051100  Tiempo transcurrido: 5.7s\n",
            "[Train][Época 4/30] Batch 20/226  Loss batch: 0.045321  Tiempo transcurrido: 10.6s\n",
            "[Train][Época 4/30] Batch 30/226  Loss batch: 0.038996  Tiempo transcurrido: 16.4s\n",
            "[Train][Época 4/30] Batch 40/226  Loss batch: 0.048369  Tiempo transcurrido: 21.3s\n",
            "[Train][Época 4/30] Batch 50/226  Loss batch: 0.041672  Tiempo transcurrido: 27.1s\n",
            "[Train][Época 4/30] Batch 60/226  Loss batch: 0.052349  Tiempo transcurrido: 32.1s\n",
            "[Train][Época 4/30] Batch 70/226  Loss batch: 0.038896  Tiempo transcurrido: 37.0s\n",
            "[Train][Época 4/30] Batch 80/226  Loss batch: 0.059683  Tiempo transcurrido: 42.8s\n",
            "[Train][Época 4/30] Batch 90/226  Loss batch: 0.046271  Tiempo transcurrido: 47.8s\n",
            "[Train][Época 4/30] Batch 100/226  Loss batch: 0.031189  Tiempo transcurrido: 53.7s\n",
            "[Train][Época 4/30] Batch 110/226  Loss batch: 0.060246  Tiempo transcurrido: 58.6s\n",
            "[Train][Época 4/30] Batch 120/226  Loss batch: 0.048777  Tiempo transcurrido: 63.5s\n",
            "[Train][Época 4/30] Batch 130/226  Loss batch: 0.046819  Tiempo transcurrido: 69.3s\n",
            "[Train][Época 4/30] Batch 140/226  Loss batch: 0.147307  Tiempo transcurrido: 74.2s\n",
            "[Train][Época 4/30] Batch 150/226  Loss batch: 0.037585  Tiempo transcurrido: 79.9s\n",
            "[Train][Época 4/30] Batch 160/226  Loss batch: 0.037371  Tiempo transcurrido: 84.9s\n",
            "[Train][Época 4/30] Batch 170/226  Loss batch: 0.046772  Tiempo transcurrido: 89.7s\n",
            "[Train][Época 4/30] Batch 180/226  Loss batch: 0.053382  Tiempo transcurrido: 95.6s\n",
            "[Train][Época 4/30] Batch 190/226  Loss batch: 0.039878  Tiempo transcurrido: 100.4s\n",
            "[Train][Época 4/30] Batch 200/226  Loss batch: 0.052708  Tiempo transcurrido: 106.1s\n",
            "[Train][Época 4/30] Batch 210/226  Loss batch: 0.062782  Tiempo transcurrido: 111.1s\n",
            "[Train][Época 4/30] Batch 220/226  Loss batch: 0.047474  Tiempo transcurrido: 116.1s\n",
            "[Train] Época 4 terminada en 118.8s  Loss media: 0.060692\n",
            "[Val][Época 4/30] Batch 0/65  Loss batch: 0.140218  Tiempo transcurrido: 0.3s\n",
            "[Val][Época 4/30] Batch 10/65  Loss batch: 0.920726  Tiempo transcurrido: 2.9s\n",
            "[Val][Época 4/30] Batch 20/65  Loss batch: 2.113981  Tiempo transcurrido: 5.3s\n",
            "[Val][Época 4/30] Batch 30/65  Loss batch: 5.466330  Tiempo transcurrido: 8.0s\n",
            "[Val][Época 4/30] Batch 40/65  Loss batch: 6.573817  Tiempo transcurrido: 10.7s\n",
            "[Val][Época 4/30] Batch 50/65  Loss batch: 10.407835  Tiempo transcurrido: 14.1s\n",
            "[Val][Época 4/30] Batch 60/65  Loss batch: 9.651518  Tiempo transcurrido: 16.9s\n",
            "[Val] Época 4 terminada en 17.7s  Loss media: 5.928867\n",
            "Época [4/30] Train Loss: 0.060692 | Val Loss: 5.928867\n",
            "============================================================\n",
            "Comenzando época 5/30\n",
            "[Train][Época 5/30] Batch 0/226  Loss batch: 0.050318  Tiempo transcurrido: 0.5s\n",
            "[Train][Época 5/30] Batch 10/226  Loss batch: 0.064473  Tiempo transcurrido: 5.4s\n",
            "[Train][Época 5/30] Batch 20/226  Loss batch: 0.052477  Tiempo transcurrido: 11.3s\n",
            "[Train][Época 5/30] Batch 30/226  Loss batch: 0.047158  Tiempo transcurrido: 16.2s\n",
            "[Train][Época 5/30] Batch 40/226  Loss batch: 0.034508  Tiempo transcurrido: 21.5s\n",
            "[Train][Época 5/30] Batch 50/226  Loss batch: 0.042888  Tiempo transcurrido: 26.9s\n",
            "[Train][Época 5/30] Batch 60/226  Loss batch: 0.039833  Tiempo transcurrido: 31.8s\n",
            "[Train][Época 5/30] Batch 70/226  Loss batch: 0.032957  Tiempo transcurrido: 37.5s\n",
            "[Train][Época 5/30] Batch 80/226  Loss batch: 0.045355  Tiempo transcurrido: 42.5s\n",
            "[Train][Época 5/30] Batch 90/226  Loss batch: 0.044716  Tiempo transcurrido: 47.7s\n",
            "[Train][Época 5/30] Batch 100/226  Loss batch: 0.048227  Tiempo transcurrido: 53.1s\n",
            "[Train][Época 5/30] Batch 110/226  Loss batch: 0.039634  Tiempo transcurrido: 58.0s\n",
            "[Train][Época 5/30] Batch 120/226  Loss batch: 0.046207  Tiempo transcurrido: 63.9s\n",
            "[Train][Época 5/30] Batch 130/226  Loss batch: 0.054321  Tiempo transcurrido: 68.7s\n",
            "[Train][Época 5/30] Batch 140/226  Loss batch: 0.668295  Tiempo transcurrido: 73.9s\n",
            "[Train][Época 5/30] Batch 150/226  Loss batch: 0.041145  Tiempo transcurrido: 79.5s\n",
            "[Train][Época 5/30] Batch 160/226  Loss batch: 0.057121  Tiempo transcurrido: 84.4s\n",
            "[Train][Época 5/30] Batch 170/226  Loss batch: 0.044740  Tiempo transcurrido: 90.2s\n",
            "[Train][Época 5/30] Batch 180/226  Loss batch: 0.043716  Tiempo transcurrido: 95.0s\n",
            "[Train][Época 5/30] Batch 190/226  Loss batch: 0.044816  Tiempo transcurrido: 100.2s\n",
            "[Train][Época 5/30] Batch 200/226  Loss batch: 0.374178  Tiempo transcurrido: 105.7s\n",
            "[Train][Época 5/30] Batch 210/226  Loss batch: 0.047504  Tiempo transcurrido: 110.6s\n",
            "[Train][Época 5/30] Batch 220/226  Loss batch: 0.287492  Tiempo transcurrido: 116.4s\n",
            "[Train] Época 5 terminada en 118.5s  Loss media: 0.097756\n",
            "[Val][Época 5/30] Batch 0/65  Loss batch: 0.230915  Tiempo transcurrido: 0.2s\n",
            "[Val][Época 5/30] Batch 10/65  Loss batch: 1.766633  Tiempo transcurrido: 2.6s\n",
            "[Val][Época 5/30] Batch 20/65  Loss batch: 4.016076  Tiempo transcurrido: 5.0s\n",
            "[Val][Época 5/30] Batch 30/65  Loss batch: 5.351439  Tiempo transcurrido: 7.8s\n",
            "[Val][Época 5/30] Batch 40/65  Loss batch: 5.822831  Tiempo transcurrido: 11.3s\n",
            "[Val][Época 5/30] Batch 50/65  Loss batch: 10.395277  Tiempo transcurrido: 13.9s\n",
            "[Val][Época 5/30] Batch 60/65  Loss batch: 9.735556  Tiempo transcurrido: 16.5s\n",
            "[Val] Época 5 terminada en 17.3s  Loss media: 6.420779\n",
            "Época [5/30] Train Loss: 0.097756 | Val Loss: 6.420779\n",
            "============================================================\n",
            "Comenzando época 6/30\n",
            "[Train][Época 6/30] Batch 0/226  Loss batch: 0.077977  Tiempo transcurrido: 0.5s\n",
            "[Train][Época 6/30] Batch 10/226  Loss batch: 0.129275  Tiempo transcurrido: 6.5s\n",
            "[Train][Época 6/30] Batch 20/226  Loss batch: 0.057650  Tiempo transcurrido: 11.3s\n",
            "[Train][Época 6/30] Batch 30/226  Loss batch: 0.381744  Tiempo transcurrido: 16.2s\n",
            "[Train][Época 6/30] Batch 40/226  Loss batch: 0.075346  Tiempo transcurrido: 22.0s\n",
            "[Train][Época 6/30] Batch 50/226  Loss batch: 0.378477  Tiempo transcurrido: 27.0s\n",
            "[Train][Época 6/30] Batch 60/226  Loss batch: 0.075017  Tiempo transcurrido: 32.8s\n",
            "[Train][Época 6/30] Batch 70/226  Loss batch: 0.053847  Tiempo transcurrido: 37.7s\n",
            "[Train][Época 6/30] Batch 80/226  Loss batch: 0.073074  Tiempo transcurrido: 42.6s\n",
            "[Train][Época 6/30] Batch 90/226  Loss batch: 0.057901  Tiempo transcurrido: 48.4s\n",
            "[Train][Época 6/30] Batch 100/226  Loss batch: 0.052435  Tiempo transcurrido: 53.3s\n",
            "[Train][Época 6/30] Batch 110/226  Loss batch: 0.277295  Tiempo transcurrido: 59.1s\n",
            "[Train][Época 6/30] Batch 120/226  Loss batch: 0.043040  Tiempo transcurrido: 64.1s\n",
            "[Train][Época 6/30] Batch 130/226  Loss batch: 0.060225  Tiempo transcurrido: 68.9s\n",
            "[Train][Época 6/30] Batch 140/226  Loss batch: 0.050113  Tiempo transcurrido: 74.8s\n",
            "[Train][Época 6/30] Batch 150/226  Loss batch: 0.043506  Tiempo transcurrido: 79.6s\n",
            "[Train][Época 6/30] Batch 160/226  Loss batch: 0.054334  Tiempo transcurrido: 85.4s\n",
            "[Train][Época 6/30] Batch 170/226  Loss batch: 0.136392  Tiempo transcurrido: 90.5s\n",
            "[Train][Época 6/30] Batch 180/226  Loss batch: 0.040776  Tiempo transcurrido: 95.4s\n",
            "[Train][Época 6/30] Batch 190/226  Loss batch: 0.038352  Tiempo transcurrido: 101.3s\n",
            "[Train][Época 6/30] Batch 200/226  Loss batch: 0.036856  Tiempo transcurrido: 106.1s\n",
            "[Train][Época 6/30] Batch 210/226  Loss batch: 0.047297  Tiempo transcurrido: 111.8s\n",
            "[Train][Época 6/30] Batch 220/226  Loss batch: 0.056938  Tiempo transcurrido: 116.9s\n",
            "[Train] Época 6 terminada en 119.0s  Loss media: 0.103894\n",
            "[Val][Época 6/30] Batch 0/65  Loss batch: 0.081094  Tiempo transcurrido: 0.2s\n",
            "[Val][Época 6/30] Batch 10/65  Loss batch: 0.373119  Tiempo transcurrido: 2.6s\n",
            "[Val][Época 6/30] Batch 20/65  Loss batch: 1.578695  Tiempo transcurrido: 5.6s\n",
            "[Val][Época 6/30] Batch 30/65  Loss batch: 5.292295  Tiempo transcurrido: 8.5s\n",
            "[Val][Época 6/30] Batch 40/65  Loss batch: 5.813058  Tiempo transcurrido: 11.3s\n",
            "[Val][Época 6/30] Batch 50/65  Loss batch: 10.383829  Tiempo transcurrido: 13.8s\n",
            "[Val][Época 6/30] Batch 60/65  Loss batch: 9.979148  Tiempo transcurrido: 16.4s\n",
            "[Val] Época 6 terminada en 17.5s  Loss media: 5.854947\n",
            "Época [6/30] Train Loss: 0.103894 | Val Loss: 5.854947\n",
            "  🟢 Nuevo mejor modelo guardado en: /content/cats_dataset/posenet_light_best.pth\n",
            "============================================================\n",
            "Comenzando época 7/30\n",
            "[Train][Época 7/30] Batch 0/226  Loss batch: 0.055519  Tiempo transcurrido: 0.8s\n",
            "[Train][Época 7/30] Batch 10/226  Loss batch: 0.033993  Tiempo transcurrido: 6.0s\n",
            "[Train][Época 7/30] Batch 20/226  Loss batch: 0.029917  Tiempo transcurrido: 10.9s\n",
            "[Train][Época 7/30] Batch 30/226  Loss batch: 0.033185  Tiempo transcurrido: 16.8s\n",
            "[Train][Época 7/30] Batch 40/226  Loss batch: 0.053841  Tiempo transcurrido: 21.6s\n",
            "[Train][Época 7/30] Batch 50/226  Loss batch: 0.033404  Tiempo transcurrido: 27.0s\n",
            "[Train][Época 7/30] Batch 60/226  Loss batch: 0.031822  Tiempo transcurrido: 32.4s\n",
            "[Train][Época 7/30] Batch 70/226  Loss batch: 0.036546  Tiempo transcurrido: 37.2s\n",
            "[Train][Época 7/30] Batch 80/226  Loss batch: 0.042432  Tiempo transcurrido: 43.0s\n",
            "[Train][Época 7/30] Batch 90/226  Loss batch: 0.029256  Tiempo transcurrido: 47.9s\n",
            "[Train][Época 7/30] Batch 100/226  Loss batch: 0.040738  Tiempo transcurrido: 53.2s\n",
            "[Train][Época 7/30] Batch 110/226  Loss batch: 0.028916  Tiempo transcurrido: 58.6s\n",
            "[Train][Época 7/30] Batch 120/226  Loss batch: 0.076861  Tiempo transcurrido: 63.5s\n",
            "[Train][Época 7/30] Batch 130/226  Loss batch: 0.029873  Tiempo transcurrido: 69.5s\n",
            "[Train][Época 7/30] Batch 140/226  Loss batch: 0.028853  Tiempo transcurrido: 74.3s\n",
            "[Train][Época 7/30] Batch 150/226  Loss batch: 0.027633  Tiempo transcurrido: 79.6s\n",
            "[Train][Época 7/30] Batch 160/226  Loss batch: 0.030030  Tiempo transcurrido: 85.1s\n",
            "[Train][Época 7/30] Batch 170/226  Loss batch: 0.037937  Tiempo transcurrido: 90.0s\n",
            "[Train][Época 7/30] Batch 180/226  Loss batch: 0.039097  Tiempo transcurrido: 95.8s\n",
            "[Train][Época 7/30] Batch 190/226  Loss batch: 0.028129  Tiempo transcurrido: 100.7s\n",
            "[Train][Época 7/30] Batch 200/226  Loss batch: 0.041211  Tiempo transcurrido: 105.9s\n",
            "[Train][Época 7/30] Batch 210/226  Loss batch: 0.078944  Tiempo transcurrido: 111.4s\n",
            "[Train][Época 7/30] Batch 220/226  Loss batch: 0.034051  Tiempo transcurrido: 116.3s\n",
            "[Train] Época 7 terminada en 118.5s  Loss media: 0.042882\n",
            "[Val][Época 7/30] Batch 0/65  Loss batch: 0.091774  Tiempo transcurrido: 0.3s\n",
            "[Val][Época 7/30] Batch 10/65  Loss batch: 0.371984  Tiempo transcurrido: 3.4s\n",
            "[Val][Época 7/30] Batch 20/65  Loss batch: 1.941148  Tiempo transcurrido: 5.8s\n",
            "[Val][Época 7/30] Batch 30/65  Loss batch: 5.219092  Tiempo transcurrido: 8.4s\n",
            "[Val][Época 7/30] Batch 40/65  Loss batch: 6.093414  Tiempo transcurrido: 11.2s\n",
            "[Val][Época 7/30] Batch 50/65  Loss batch: 10.374488  Tiempo transcurrido: 14.2s\n",
            "[Val][Época 7/30] Batch 60/65  Loss batch: 9.954858  Tiempo transcurrido: 17.3s\n",
            "[Val] Época 7 terminada en 18.2s  Loss media: 5.851564\n",
            "Época [7/30] Train Loss: 0.042882 | Val Loss: 5.851564\n",
            "  🟢 Nuevo mejor modelo guardado en: /content/cats_dataset/posenet_light_best.pth\n",
            "============================================================\n",
            "Comenzando época 8/30\n",
            "[Train][Época 8/30] Batch 0/226  Loss batch: 0.035276  Tiempo transcurrido: 0.6s\n",
            "[Train][Época 8/30] Batch 10/226  Loss batch: 0.028755  Tiempo transcurrido: 5.4s\n",
            "[Train][Época 8/30] Batch 20/226  Loss batch: 0.031261  Tiempo transcurrido: 11.4s\n",
            "[Train][Época 8/30] Batch 30/226  Loss batch: 0.029737  Tiempo transcurrido: 16.3s\n",
            "[Train][Época 8/30] Batch 40/226  Loss batch: 0.028710  Tiempo transcurrido: 21.4s\n",
            "[Train][Época 8/30] Batch 50/226  Loss batch: 0.028423  Tiempo transcurrido: 27.1s\n",
            "[Train][Época 8/30] Batch 60/226  Loss batch: 0.028770  Tiempo transcurrido: 32.0s\n",
            "[Train][Época 8/30] Batch 70/226  Loss batch: 0.031718  Tiempo transcurrido: 37.9s\n",
            "[Train][Época 8/30] Batch 80/226  Loss batch: 0.029761  Tiempo transcurrido: 42.8s\n",
            "[Train][Época 8/30] Batch 90/226  Loss batch: 0.031142  Tiempo transcurrido: 47.7s\n",
            "[Train][Época 8/30] Batch 100/226  Loss batch: 0.030683  Tiempo transcurrido: 53.5s\n",
            "[Train][Época 8/30] Batch 110/226  Loss batch: 0.028202  Tiempo transcurrido: 58.3s\n",
            "[Train][Época 8/30] Batch 120/226  Loss batch: 0.027487  Tiempo transcurrido: 64.2s\n",
            "[Train][Época 8/30] Batch 130/226  Loss batch: 0.026386  Tiempo transcurrido: 69.1s\n",
            "[Train][Época 8/30] Batch 140/226  Loss batch: 0.036216  Tiempo transcurrido: 74.0s\n",
            "[Train][Época 8/30] Batch 150/226  Loss batch: 0.027526  Tiempo transcurrido: 79.8s\n",
            "[Train][Época 8/30] Batch 160/226  Loss batch: 0.031077  Tiempo transcurrido: 84.7s\n",
            "[Train][Época 8/30] Batch 170/226  Loss batch: 0.031652  Tiempo transcurrido: 90.6s\n",
            "[Train][Época 8/30] Batch 180/226  Loss batch: 0.035586  Tiempo transcurrido: 95.5s\n",
            "[Train][Época 8/30] Batch 190/226  Loss batch: 0.035117  Tiempo transcurrido: 100.3s\n",
            "[Train][Época 8/30] Batch 200/226  Loss batch: 0.027032  Tiempo transcurrido: 106.3s\n",
            "[Train][Época 8/30] Batch 210/226  Loss batch: 0.038416  Tiempo transcurrido: 111.1s\n",
            "[Train][Época 8/30] Batch 220/226  Loss batch: 0.031856  Tiempo transcurrido: 117.1s\n",
            "[Train] Época 8 terminada en 119.2s  Loss media: 0.039064\n",
            "[Val][Época 8/30] Batch 0/65  Loss batch: 0.115582  Tiempo transcurrido: 0.2s\n",
            "[Val][Época 8/30] Batch 10/65  Loss batch: 0.372711  Tiempo transcurrido: 2.6s\n",
            "[Val][Época 8/30] Batch 20/65  Loss batch: 2.191054  Tiempo transcurrido: 5.0s\n",
            "[Val][Época 8/30] Batch 30/65  Loss batch: 5.395714  Tiempo transcurrido: 7.6s\n",
            "[Val][Época 8/30] Batch 40/65  Loss batch: 6.338668  Tiempo transcurrido: 11.3s\n",
            "[Val][Época 8/30] Batch 50/65  Loss batch: 10.388132  Tiempo transcurrido: 13.8s\n",
            "[Val][Época 8/30] Batch 60/65  Loss batch: 9.930464  Tiempo transcurrido: 16.5s\n",
            "[Val] Época 8 terminada en 17.3s  Loss media: 5.848639\n",
            "Época [8/30] Train Loss: 0.039064 | Val Loss: 5.848639\n",
            "  🟢 Nuevo mejor modelo guardado en: /content/cats_dataset/posenet_light_best.pth\n",
            "============================================================\n",
            "Comenzando época 9/30\n",
            "[Train][Época 9/30] Batch 0/226  Loss batch: 0.023974  Tiempo transcurrido: 0.6s\n",
            "[Train][Época 9/30] Batch 10/226  Loss batch: 0.031118  Tiempo transcurrido: 6.3s\n",
            "[Train][Época 9/30] Batch 20/226  Loss batch: 0.028976  Tiempo transcurrido: 11.3s\n",
            "[Train][Época 9/30] Batch 30/226  Loss batch: 0.032532  Tiempo transcurrido: 16.2s\n",
            "[Train][Época 9/30] Batch 40/226  Loss batch: 0.023204  Tiempo transcurrido: 22.0s\n",
            "[Train][Época 9/30] Batch 50/226  Loss batch: 0.019653  Tiempo transcurrido: 26.9s\n",
            "[Train][Época 9/30] Batch 60/226  Loss batch: 0.037673  Tiempo transcurrido: 32.7s\n",
            "[Train][Época 9/30] Batch 70/226  Loss batch: 0.024902  Tiempo transcurrido: 37.8s\n",
            "[Train][Época 9/30] Batch 80/226  Loss batch: 0.029561  Tiempo transcurrido: 42.7s\n",
            "[Train][Época 9/30] Batch 90/226  Loss batch: 0.027896  Tiempo transcurrido: 48.5s\n",
            "[Train][Época 9/30] Batch 100/226  Loss batch: 0.026193  Tiempo transcurrido: 53.4s\n",
            "[Train][Época 9/30] Batch 110/226  Loss batch: 0.033241  Tiempo transcurrido: 59.1s\n",
            "[Train][Época 9/30] Batch 120/226  Loss batch: 0.024140  Tiempo transcurrido: 64.1s\n",
            "[Train][Época 9/30] Batch 130/226  Loss batch: 0.024017  Tiempo transcurrido: 69.0s\n",
            "[Train][Época 9/30] Batch 140/226  Loss batch: 0.044793  Tiempo transcurrido: 74.8s\n",
            "[Train][Época 9/30] Batch 150/226  Loss batch: 0.023408  Tiempo transcurrido: 79.7s\n",
            "[Train][Época 9/30] Batch 160/226  Loss batch: 0.022862  Tiempo transcurrido: 85.3s\n",
            "[Train][Época 9/30] Batch 170/226  Loss batch: 0.031061  Tiempo transcurrido: 90.4s\n",
            "[Train][Época 9/30] Batch 180/226  Loss batch: 0.037195  Tiempo transcurrido: 95.3s\n",
            "[Train][Época 9/30] Batch 190/226  Loss batch: 0.025788  Tiempo transcurrido: 101.2s\n",
            "[Train][Época 9/30] Batch 200/226  Loss batch: 0.028489  Tiempo transcurrido: 106.0s\n",
            "[Train][Época 9/30] Batch 210/226  Loss batch: 0.023052  Tiempo transcurrido: 111.5s\n",
            "[Train][Época 9/30] Batch 220/226  Loss batch: 0.029338  Tiempo transcurrido: 116.7s\n",
            "[Train] Época 9 terminada en 118.8s  Loss media: 0.031089\n",
            "[Val][Época 9/30] Batch 0/65  Loss batch: 0.125756  Tiempo transcurrido: 0.2s\n",
            "[Val][Época 9/30] Batch 10/65  Loss batch: 0.339367  Tiempo transcurrido: 2.6s\n",
            "[Val][Época 9/30] Batch 20/65  Loss batch: 1.887851  Tiempo transcurrido: 5.4s\n",
            "[Val][Época 9/30] Batch 30/65  Loss batch: 5.326202  Tiempo transcurrido: 8.5s\n",
            "[Val][Época 9/30] Batch 40/65  Loss batch: 6.035822  Tiempo transcurrido: 11.3s\n",
            "[Val][Época 9/30] Batch 50/65  Loss batch: 10.398729  Tiempo transcurrido: 13.8s\n",
            "[Val][Época 9/30] Batch 60/65  Loss batch: 9.940275  Tiempo transcurrido: 16.5s\n",
            "[Val] Época 9 terminada en 17.4s  Loss media: 5.803436\n",
            "Época [9/30] Train Loss: 0.031089 | Val Loss: 5.803436\n",
            "  🟢 Nuevo mejor modelo guardado en: /content/cats_dataset/posenet_light_best.pth\n",
            "============================================================\n",
            "Comenzando época 10/30\n",
            "[Train][Época 10/30] Batch 0/226  Loss batch: 0.036613  Tiempo transcurrido: 0.7s\n",
            "[Train][Época 10/30] Batch 10/226  Loss batch: 0.029262  Tiempo transcurrido: 6.2s\n",
            "[Train][Época 10/30] Batch 20/226  Loss batch: 0.023241  Tiempo transcurrido: 11.1s\n",
            "[Train][Época 10/30] Batch 30/226  Loss batch: 0.022632  Tiempo transcurrido: 16.9s\n",
            "[Train][Época 10/30] Batch 40/226  Loss batch: 0.027768  Tiempo transcurrido: 21.8s\n",
            "[Train][Época 10/30] Batch 50/226  Loss batch: 0.030813  Tiempo transcurrido: 27.1s\n",
            "[Train][Época 10/30] Batch 60/226  Loss batch: 0.030227  Tiempo transcurrido: 32.6s\n",
            "[Train][Época 10/30] Batch 70/226  Loss batch: 0.036276  Tiempo transcurrido: 37.4s\n",
            "[Train][Época 10/30] Batch 80/226  Loss batch: 0.017373  Tiempo transcurrido: 43.3s\n",
            "[Train][Época 10/30] Batch 90/226  Loss batch: 0.022522  Tiempo transcurrido: 48.2s\n",
            "[Train][Época 10/30] Batch 100/226  Loss batch: 0.023423  Tiempo transcurrido: 53.3s\n",
            "[Train][Época 10/30] Batch 110/226  Loss batch: 0.024516  Tiempo transcurrido: 58.9s\n",
            "[Train][Época 10/30] Batch 120/226  Loss batch: 0.026651  Tiempo transcurrido: 63.7s\n",
            "[Train][Época 10/30] Batch 130/226  Loss batch: 0.026044  Tiempo transcurrido: 69.6s\n",
            "[Train][Época 10/30] Batch 140/226  Loss batch: 0.026581  Tiempo transcurrido: 74.5s\n",
            "[Train][Época 10/30] Batch 150/226  Loss batch: 0.023438  Tiempo transcurrido: 79.6s\n",
            "[Train][Época 10/30] Batch 160/226  Loss batch: 0.025811  Tiempo transcurrido: 85.3s\n",
            "[Train][Época 10/30] Batch 170/226  Loss batch: 0.030460  Tiempo transcurrido: 90.2s\n",
            "[Train][Época 10/30] Batch 180/226  Loss batch: 0.031870  Tiempo transcurrido: 96.0s\n",
            "[Train][Época 10/30] Batch 190/226  Loss batch: 0.025248  Tiempo transcurrido: 100.9s\n",
            "[Train][Época 10/30] Batch 200/226  Loss batch: 0.024601  Tiempo transcurrido: 106.0s\n",
            "[Train][Época 10/30] Batch 210/226  Loss batch: 0.017799  Tiempo transcurrido: 111.6s\n",
            "[Train][Época 10/30] Batch 220/226  Loss batch: 0.025641  Tiempo transcurrido: 116.5s\n",
            "[Train] Época 10 terminada en 118.7s  Loss media: 0.029123\n",
            "[Val][Época 10/30] Batch 0/65  Loss batch: 0.115014  Tiempo transcurrido: 0.3s\n",
            "[Val][Época 10/30] Batch 10/65  Loss batch: 0.362738  Tiempo transcurrido: 3.6s\n",
            "[Val][Época 10/30] Batch 20/65  Loss batch: 1.926354  Tiempo transcurrido: 5.9s\n",
            "[Val][Época 10/30] Batch 30/65  Loss batch: 5.252580  Tiempo transcurrido: 8.6s\n",
            "[Val][Época 10/30] Batch 40/65  Loss batch: 5.950434  Tiempo transcurrido: 11.4s\n",
            "[Val][Época 10/30] Batch 50/65  Loss batch: 10.396614  Tiempo transcurrido: 14.2s\n",
            "[Val][Época 10/30] Batch 60/65  Loss batch: 9.871049  Tiempo transcurrido: 17.5s\n",
            "[Val] Época 10 terminada en 18.3s  Loss media: 5.850422\n",
            "Época [10/30] Train Loss: 0.029123 | Val Loss: 5.850422\n",
            "============================================================\n",
            "Comenzando época 11/30\n",
            "[Train][Época 11/30] Batch 0/226  Loss batch: 0.027656  Tiempo transcurrido: 0.5s\n",
            "[Train][Época 11/30] Batch 10/226  Loss batch: 0.025134  Tiempo transcurrido: 5.5s\n",
            "[Train][Época 11/30] Batch 20/226  Loss batch: 0.025219  Tiempo transcurrido: 11.3s\n",
            "[Train][Época 11/30] Batch 30/226  Loss batch: 0.024346  Tiempo transcurrido: 16.2s\n",
            "[Train][Época 11/30] Batch 40/226  Loss batch: 0.020900  Tiempo transcurrido: 21.2s\n",
            "[Train][Época 11/30] Batch 50/226  Loss batch: 0.016921  Tiempo transcurrido: 27.1s\n",
            "[Train][Época 11/30] Batch 60/226  Loss batch: 0.027875  Tiempo transcurrido: 32.0s\n",
            "[Train][Época 11/30] Batch 70/226  Loss batch: 0.030170  Tiempo transcurrido: 37.9s\n",
            "[Train][Época 11/30] Batch 80/226  Loss batch: 0.023589  Tiempo transcurrido: 42.8s\n",
            "[Train][Época 11/30] Batch 90/226  Loss batch: 0.021584  Tiempo transcurrido: 47.7s\n",
            "[Train][Época 11/30] Batch 100/226  Loss batch: 0.023964  Tiempo transcurrido: 53.6s\n",
            "[Train][Época 11/30] Batch 110/226  Loss batch: 0.020348  Tiempo transcurrido: 58.5s\n",
            "[Train][Época 11/30] Batch 120/226  Loss batch: 0.024755  Tiempo transcurrido: 64.3s\n",
            "[Train][Época 11/30] Batch 130/226  Loss batch: 0.031087  Tiempo transcurrido: 69.2s\n",
            "[Train][Época 11/30] Batch 140/226  Loss batch: 0.024577  Tiempo transcurrido: 74.1s\n",
            "[Train][Época 11/30] Batch 150/226  Loss batch: 0.019455  Tiempo transcurrido: 80.0s\n",
            "[Train][Época 11/30] Batch 160/226  Loss batch: 0.020742  Tiempo transcurrido: 84.9s\n",
            "[Train][Época 11/30] Batch 170/226  Loss batch: 0.017725  Tiempo transcurrido: 90.7s\n",
            "[Train][Época 11/30] Batch 180/226  Loss batch: 0.022315  Tiempo transcurrido: 95.6s\n",
            "[Train][Época 11/30] Batch 190/226  Loss batch: 0.029482  Tiempo transcurrido: 100.6s\n",
            "[Train][Época 11/30] Batch 200/226  Loss batch: 0.022149  Tiempo transcurrido: 106.4s\n",
            "[Train][Época 11/30] Batch 210/226  Loss batch: 0.020135  Tiempo transcurrido: 111.2s\n",
            "[Train][Época 11/30] Batch 220/226  Loss batch: 0.019835  Tiempo transcurrido: 117.0s\n",
            "[Train] Época 11 terminada en 119.2s  Loss media: 0.026772\n",
            "[Val][Época 11/30] Batch 0/65  Loss batch: 0.138054  Tiempo transcurrido: 0.2s\n",
            "[Val][Época 11/30] Batch 10/65  Loss batch: 0.375934  Tiempo transcurrido: 2.6s\n",
            "[Val][Época 11/30] Batch 20/65  Loss batch: 1.694529  Tiempo transcurrido: 5.0s\n",
            "[Val][Época 11/30] Batch 30/65  Loss batch: 5.184091  Tiempo transcurrido: 7.7s\n",
            "[Val][Época 11/30] Batch 40/65  Loss batch: 6.131042  Tiempo transcurrido: 11.5s\n",
            "[Val][Época 11/30] Batch 50/65  Loss batch: 10.375332  Tiempo transcurrido: 14.0s\n",
            "[Val][Época 11/30] Batch 60/65  Loss batch: 9.890192  Tiempo transcurrido: 16.7s\n",
            "[Val] Época 11 terminada en 17.5s  Loss media: 5.874293\n",
            "Época [11/30] Train Loss: 0.026772 | Val Loss: 5.874293\n",
            "============================================================\n",
            "Comenzando época 12/30\n",
            "[Train][Época 12/30] Batch 0/226  Loss batch: 0.028154  Tiempo transcurrido: 0.5s\n",
            "[Train][Época 12/30] Batch 10/226  Loss batch: 0.018142  Tiempo transcurrido: 6.1s\n",
            "[Train][Época 12/30] Batch 20/226  Loss batch: 0.028907  Tiempo transcurrido: 11.4s\n",
            "[Train][Época 12/30] Batch 30/226  Loss batch: 0.020536  Tiempo transcurrido: 16.4s\n",
            "[Train][Época 12/30] Batch 40/226  Loss batch: 0.027370  Tiempo transcurrido: 22.3s\n",
            "[Train][Época 12/30] Batch 50/226  Loss batch: 0.020313  Tiempo transcurrido: 27.3s\n",
            "[Train][Época 12/30] Batch 60/226  Loss batch: 0.019339  Tiempo transcurrido: 33.4s\n",
            "[Train][Época 12/30] Batch 70/226  Loss batch: 0.020775  Tiempo transcurrido: 38.6s\n",
            "[Train][Época 12/30] Batch 80/226  Loss batch: 0.023210  Tiempo transcurrido: 43.5s\n",
            "[Train][Época 12/30] Batch 90/226  Loss batch: 0.023900  Tiempo transcurrido: 49.4s\n",
            "[Train][Época 12/30] Batch 100/226  Loss batch: 0.024127  Tiempo transcurrido: 54.4s\n",
            "[Train][Época 12/30] Batch 110/226  Loss batch: 0.019773  Tiempo transcurrido: 60.5s\n",
            "[Train][Época 12/30] Batch 120/226  Loss batch: 0.020121  Tiempo transcurrido: 65.4s\n",
            "[Train][Época 12/30] Batch 130/226  Loss batch: 0.024435  Tiempo transcurrido: 70.4s\n",
            "[Train][Época 12/30] Batch 140/226  Loss batch: 0.018357  Tiempo transcurrido: 76.3s\n",
            "[Train][Época 12/30] Batch 150/226  Loss batch: 0.019178  Tiempo transcurrido: 81.4s\n",
            "[Train][Época 12/30] Batch 160/226  Loss batch: 0.019786  Tiempo transcurrido: 87.3s\n",
            "[Train][Época 12/30] Batch 170/226  Loss batch: 0.024274  Tiempo transcurrido: 92.3s\n",
            "[Train][Época 12/30] Batch 180/226  Loss batch: 0.025564  Tiempo transcurrido: 97.3s\n",
            "[Train][Época 12/30] Batch 190/226  Loss batch: 0.025290  Tiempo transcurrido: 103.1s\n",
            "[Train][Época 12/30] Batch 200/226  Loss batch: 0.019185  Tiempo transcurrido: 108.0s\n",
            "[Train][Época 12/30] Batch 210/226  Loss batch: 0.020446  Tiempo transcurrido: 114.0s\n",
            "[Train][Época 12/30] Batch 220/226  Loss batch: 0.025875  Tiempo transcurrido: 118.9s\n",
            "[Train] Época 12 terminada en 121.1s  Loss media: 0.025162\n",
            "[Val][Época 12/30] Batch 0/65  Loss batch: 0.157435  Tiempo transcurrido: 0.2s\n",
            "[Val][Época 12/30] Batch 10/65  Loss batch: 0.367464  Tiempo transcurrido: 2.8s\n",
            "[Val][Época 12/30] Batch 20/65  Loss batch: 2.006740  Tiempo transcurrido: 6.0s\n",
            "[Val][Época 12/30] Batch 30/65  Loss batch: 5.267393  Tiempo transcurrido: 8.6s\n",
            "[Val][Época 12/30] Batch 40/65  Loss batch: 6.474967  Tiempo transcurrido: 11.4s\n",
            "[Val][Época 12/30] Batch 50/65  Loss batch: 10.379109  Tiempo transcurrido: 14.0s\n",
            "[Val][Época 12/30] Batch 60/65  Loss batch: 9.781829  Tiempo transcurrido: 17.1s\n",
            "[Val] Época 12 terminada en 18.3s  Loss media: 5.923626\n",
            "Época [12/30] Train Loss: 0.025162 | Val Loss: 5.923626\n",
            "============================================================\n",
            "Comenzando época 13/30\n",
            "[Train][Época 13/30] Batch 0/226  Loss batch: 0.023008  Tiempo transcurrido: 0.7s\n",
            "[Train][Época 13/30] Batch 10/226  Loss batch: 0.017320  Tiempo transcurrido: 5.8s\n",
            "[Train][Época 13/30] Batch 20/226  Loss batch: 0.028953  Tiempo transcurrido: 10.9s\n",
            "[Train][Época 13/30] Batch 30/226  Loss batch: 0.019682  Tiempo transcurrido: 16.9s\n",
            "[Train][Época 13/30] Batch 40/226  Loss batch: 0.022725  Tiempo transcurrido: 21.8s\n",
            "[Train][Época 13/30] Batch 50/226  Loss batch: 0.025683  Tiempo transcurrido: 27.8s\n",
            "[Train][Época 13/30] Batch 60/226  Loss batch: 0.024615  Tiempo transcurrido: 32.8s\n",
            "[Train][Época 13/30] Batch 70/226  Loss batch: 0.017525  Tiempo transcurrido: 37.9s\n",
            "[Train][Época 13/30] Batch 80/226  Loss batch: 0.022177  Tiempo transcurrido: 43.7s\n",
            "[Train][Época 13/30] Batch 90/226  Loss batch: 0.014951  Tiempo transcurrido: 48.7s\n",
            "[Train][Época 13/30] Batch 100/226  Loss batch: 0.021981  Tiempo transcurrido: 54.6s\n",
            "[Train][Época 13/30] Batch 110/226  Loss batch: 0.017556  Tiempo transcurrido: 59.5s\n",
            "[Train][Época 13/30] Batch 120/226  Loss batch: 0.019036  Tiempo transcurrido: 64.6s\n",
            "[Train][Época 13/30] Batch 130/226  Loss batch: 0.020203  Tiempo transcurrido: 70.2s\n",
            "[Train][Época 13/30] Batch 140/226  Loss batch: 0.023387  Tiempo transcurrido: 75.1s\n",
            "[Train][Época 13/30] Batch 150/226  Loss batch: 0.021170  Tiempo transcurrido: 81.0s\n",
            "[Train][Época 13/30] Batch 160/226  Loss batch: 0.021566  Tiempo transcurrido: 86.0s\n",
            "[Train][Época 13/30] Batch 170/226  Loss batch: 0.018758  Tiempo transcurrido: 91.2s\n",
            "[Train][Época 13/30] Batch 180/226  Loss batch: 0.024995  Tiempo transcurrido: 96.8s\n",
            "[Train][Época 13/30] Batch 190/226  Loss batch: 0.019374  Tiempo transcurrido: 101.7s\n",
            "[Train][Época 13/30] Batch 200/226  Loss batch: 0.020798  Tiempo transcurrido: 107.6s\n",
            "[Train][Época 13/30] Batch 210/226  Loss batch: 0.019803  Tiempo transcurrido: 112.5s\n",
            "[Train][Época 13/30] Batch 220/226  Loss batch: 0.021288  Tiempo transcurrido: 117.6s\n",
            "[Train] Época 13 terminada en 120.5s  Loss media: 0.024046\n",
            "[Val][Época 13/30] Batch 0/65  Loss batch: 0.153919  Tiempo transcurrido: 0.3s\n",
            "[Val][Época 13/30] Batch 10/65  Loss batch: 0.331346  Tiempo transcurrido: 2.6s\n",
            "[Val][Época 13/30] Batch 20/65  Loss batch: 1.640482  Tiempo transcurrido: 5.0s\n",
            "[Val][Época 13/30] Batch 30/65  Loss batch: 5.380092  Tiempo transcurrido: 7.7s\n",
            "[Val][Época 13/30] Batch 40/65  Loss batch: 6.146667  Tiempo transcurrido: 10.8s\n",
            "[Val][Época 13/30] Batch 50/65  Loss batch: 10.420990  Tiempo transcurrido: 13.9s\n",
            "[Val][Época 13/30] Batch 60/65  Loss batch: 9.830614  Tiempo transcurrido: 16.6s\n",
            "[Val] Época 13 terminada en 17.4s  Loss media: 5.879757\n",
            "Época [13/30] Train Loss: 0.024046 | Val Loss: 5.879757\n",
            "============================================================\n",
            "Comenzando época 14/30\n",
            "[Train][Época 14/30] Batch 0/226  Loss batch: 0.019679  Tiempo transcurrido: 0.5s\n",
            "[Train][Época 14/30] Batch 10/226  Loss batch: 0.022897  Tiempo transcurrido: 5.5s\n",
            "[Train][Época 14/30] Batch 20/226  Loss batch: 0.020373  Tiempo transcurrido: 11.4s\n",
            "[Train][Época 14/30] Batch 30/226  Loss batch: 0.025485  Tiempo transcurrido: 16.3s\n",
            "[Train][Época 14/30] Batch 40/226  Loss batch: 0.024227  Tiempo transcurrido: 22.2s\n",
            "[Train][Época 14/30] Batch 50/226  Loss batch: 0.017121  Tiempo transcurrido: 27.0s\n",
            "[Train][Época 14/30] Batch 60/226  Loss batch: 0.022967  Tiempo transcurrido: 32.0s\n",
            "[Train][Época 14/30] Batch 70/226  Loss batch: 0.026137  Tiempo transcurrido: 38.0s\n",
            "[Train][Época 14/30] Batch 80/226  Loss batch: 0.021495  Tiempo transcurrido: 43.0s\n",
            "[Train][Época 14/30] Batch 90/226  Loss batch: 0.026017  Tiempo transcurrido: 49.0s\n",
            "[Train][Época 14/30] Batch 100/226  Loss batch: 0.022449  Tiempo transcurrido: 54.0s\n",
            "[Train][Época 14/30] Batch 110/226  Loss batch: 0.028389  Tiempo transcurrido: 59.0s\n",
            "[Train][Época 14/30] Batch 120/226  Loss batch: 0.329982  Tiempo transcurrido: 64.8s\n",
            "[Train][Época 14/30] Batch 130/226  Loss batch: 0.016921  Tiempo transcurrido: 69.7s\n",
            "[Train][Época 14/30] Batch 140/226  Loss batch: 0.020174  Tiempo transcurrido: 75.6s\n",
            "[Train][Época 14/30] Batch 150/226  Loss batch: 0.022635  Tiempo transcurrido: 80.5s\n",
            "[Train][Época 14/30] Batch 160/226  Loss batch: 0.015436  Tiempo transcurrido: 85.6s\n",
            "[Train][Época 14/30] Batch 170/226  Loss batch: 0.645624  Tiempo transcurrido: 91.3s\n",
            "[Train][Época 14/30] Batch 180/226  Loss batch: 0.664815  Tiempo transcurrido: 96.2s\n",
            "[Train][Época 14/30] Batch 190/226  Loss batch: 0.092971  Tiempo transcurrido: 102.1s\n",
            "[Train][Época 14/30] Batch 200/226  Loss batch: 0.067464  Tiempo transcurrido: 107.1s\n",
            "[Train][Época 14/30] Batch 210/226  Loss batch: 0.056480  Tiempo transcurrido: 112.2s\n",
            "[Train][Época 14/30] Batch 220/226  Loss batch: 0.048799  Tiempo transcurrido: 118.0s\n",
            "[Train] Época 14 terminada en 120.1s  Loss media: 0.071643\n",
            "[Val][Época 14/30] Batch 0/65  Loss batch: 0.143271  Tiempo transcurrido: 0.2s\n",
            "[Val][Época 14/30] Batch 10/65  Loss batch: 0.422529  Tiempo transcurrido: 2.6s\n",
            "[Val][Época 14/30] Batch 20/65  Loss batch: 2.264980  Tiempo transcurrido: 5.0s\n",
            "[Val][Época 14/30] Batch 30/65  Loss batch: 5.649641  Tiempo transcurrido: 8.5s\n",
            "[Val][Época 14/30] Batch 40/65  Loss batch: 7.541670  Tiempo transcurrido: 11.3s\n",
            "[Val][Época 14/30] Batch 50/65  Loss batch: 10.480526  Tiempo transcurrido: 13.8s\n",
            "[Val][Época 14/30] Batch 60/65  Loss batch: 9.337457  Tiempo transcurrido: 16.5s\n",
            "[Val] Época 14 terminada en 17.3s  Loss media: 5.808417\n",
            "Época [14/30] Train Loss: 0.071643 | Val Loss: 5.808417\n",
            "============================================================\n",
            "Comenzando época 15/30\n",
            "[Train][Época 15/30] Batch 0/226  Loss batch: 0.048519  Tiempo transcurrido: 0.5s\n",
            "[Train][Época 15/30] Batch 10/226  Loss batch: 0.034813  Tiempo transcurrido: 6.5s\n",
            "[Train][Época 15/30] Batch 20/226  Loss batch: 0.035866  Tiempo transcurrido: 11.4s\n",
            "[Train][Época 15/30] Batch 30/226  Loss batch: 0.127334  Tiempo transcurrido: 17.3s\n",
            "[Train][Época 15/30] Batch 40/226  Loss batch: 0.062224  Tiempo transcurrido: 22.2s\n",
            "[Train][Época 15/30] Batch 50/226  Loss batch: 0.080155  Tiempo transcurrido: 27.1s\n",
            "[Train][Época 15/30] Batch 60/226  Loss batch: 0.036045  Tiempo transcurrido: 33.0s\n",
            "[Train][Época 15/30] Batch 70/226  Loss batch: 0.036455  Tiempo transcurrido: 38.0s\n",
            "[Train][Época 15/30] Batch 80/226  Loss batch: 0.038240  Tiempo transcurrido: 43.9s\n",
            "[Train][Época 15/30] Batch 90/226  Loss batch: 0.042082  Tiempo transcurrido: 48.8s\n",
            "[Train][Época 15/30] Batch 100/226  Loss batch: 0.348631  Tiempo transcurrido: 53.8s\n",
            "[Train][Época 15/30] Batch 110/226  Loss batch: 0.029982  Tiempo transcurrido: 59.7s\n",
            "[Train][Época 15/30] Batch 120/226  Loss batch: 0.125681  Tiempo transcurrido: 64.6s\n",
            "[Train][Época 15/30] Batch 130/226  Loss batch: 0.140237  Tiempo transcurrido: 70.5s\n",
            "[Train][Época 15/30] Batch 140/226  Loss batch: 0.041745  Tiempo transcurrido: 75.4s\n",
            "[Train][Época 15/30] Batch 150/226  Loss batch: 0.037379  Tiempo transcurrido: 80.5s\n",
            "[Train][Época 15/30] Batch 160/226  Loss batch: 0.042366  Tiempo transcurrido: 86.3s\n",
            "[Train][Época 15/30] Batch 170/226  Loss batch: 0.033112  Tiempo transcurrido: 91.3s\n",
            "[Train][Época 15/30] Batch 180/226  Loss batch: 0.030011  Tiempo transcurrido: 97.3s\n",
            "[Train][Época 15/30] Batch 190/226  Loss batch: 0.039548  Tiempo transcurrido: 102.2s\n",
            "[Train][Época 15/30] Batch 200/226  Loss batch: 0.049792  Tiempo transcurrido: 107.2s\n",
            "[Train][Época 15/30] Batch 210/226  Loss batch: 0.348629  Tiempo transcurrido: 113.0s\n",
            "[Train][Época 15/30] Batch 220/226  Loss batch: 0.042995  Tiempo transcurrido: 117.9s\n",
            "[Train] Época 15 terminada en 119.9s  Loss media: 0.115047\n",
            "[Val][Época 15/30] Batch 0/65  Loss batch: 0.214214  Tiempo transcurrido: 0.3s\n",
            "[Val][Época 15/30] Batch 10/65  Loss batch: 0.396543  Tiempo transcurrido: 3.5s\n",
            "[Val][Época 15/30] Batch 20/65  Loss batch: 2.906818  Tiempo transcurrido: 5.9s\n",
            "[Val][Época 15/30] Batch 30/65  Loss batch: 5.727781  Tiempo transcurrido: 8.5s\n",
            "[Val][Época 15/30] Batch 40/65  Loss batch: 6.456607  Tiempo transcurrido: 11.3s\n",
            "[Val][Época 15/30] Batch 50/65  Loss batch: 10.380547  Tiempo transcurrido: 14.0s\n",
            "[Val][Época 15/30] Batch 60/65  Loss batch: 8.035336  Tiempo transcurrido: 17.3s\n",
            "[Val] Época 15 terminada en 18.2s  Loss media: 6.049400\n",
            "Época [15/30] Train Loss: 0.115047 | Val Loss: 6.049400\n",
            "============================================================\n",
            "Comenzando época 16/30\n",
            "[Train][Época 16/30] Batch 0/226  Loss batch: 0.067646  Tiempo transcurrido: 0.5s\n",
            "[Train][Época 16/30] Batch 10/226  Loss batch: 0.327590  Tiempo transcurrido: 5.4s\n",
            "[Train][Época 16/30] Batch 20/226  Loss batch: 0.045428  Tiempo transcurrido: 11.1s\n",
            "[Train][Época 16/30] Batch 30/226  Loss batch: 0.037570  Tiempo transcurrido: 16.1s\n",
            "[Train][Época 16/30] Batch 40/226  Loss batch: 0.059101  Tiempo transcurrido: 21.1s\n",
            "[Train][Época 16/30] Batch 50/226  Loss batch: 0.043113  Tiempo transcurrido: 26.9s\n",
            "[Train][Época 16/30] Batch 60/226  Loss batch: 0.060228  Tiempo transcurrido: 31.9s\n",
            "[Train][Época 16/30] Batch 70/226  Loss batch: 0.045455  Tiempo transcurrido: 37.6s\n",
            "[Train][Época 16/30] Batch 80/226  Loss batch: 0.046556  Tiempo transcurrido: 42.6s\n",
            "[Train][Época 16/30] Batch 90/226  Loss batch: 0.025273  Tiempo transcurrido: 47.5s\n",
            "[Train][Época 16/30] Batch 100/226  Loss batch: 0.050876  Tiempo transcurrido: 53.2s\n",
            "[Train][Época 16/30] Batch 110/226  Loss batch: 0.037874  Tiempo transcurrido: 58.1s\n",
            "[Train][Época 16/30] Batch 120/226  Loss batch: 0.625505  Tiempo transcurrido: 63.6s\n",
            "[Train][Época 16/30] Batch 130/226  Loss batch: 0.044715  Tiempo transcurrido: 68.7s\n",
            "[Train][Época 16/30] Batch 140/226  Loss batch: 0.057435  Tiempo transcurrido: 73.6s\n",
            "[Train][Época 16/30] Batch 150/226  Loss batch: 0.026334  Tiempo transcurrido: 79.5s\n",
            "[Train][Época 16/30] Batch 160/226  Loss batch: 0.332136  Tiempo transcurrido: 84.4s\n",
            "[Train][Época 16/30] Batch 170/226  Loss batch: 0.024139  Tiempo transcurrido: 89.9s\n",
            "[Train][Época 16/30] Batch 180/226  Loss batch: 0.037043  Tiempo transcurrido: 95.1s\n",
            "[Train][Época 16/30] Batch 190/226  Loss batch: 0.027665  Tiempo transcurrido: 100.0s\n",
            "[Train][Época 16/30] Batch 200/226  Loss batch: 0.035030  Tiempo transcurrido: 105.8s\n",
            "[Train][Época 16/30] Batch 210/226  Loss batch: 0.024684  Tiempo transcurrido: 110.7s\n",
            "[Train][Época 16/30] Batch 220/226  Loss batch: 0.037185  Tiempo transcurrido: 116.1s\n",
            "[Train] Época 16 terminada en 118.6s  Loss media: 0.094501\n",
            "[Val][Época 16/30] Batch 0/65  Loss batch: 0.197500  Tiempo transcurrido: 0.2s\n",
            "[Val][Época 16/30] Batch 10/65  Loss batch: 0.350512  Tiempo transcurrido: 2.6s\n",
            "[Val][Época 16/30] Batch 20/65  Loss batch: 1.374076  Tiempo transcurrido: 4.9s\n",
            "[Val][Época 16/30] Batch 30/65  Loss batch: 5.368323  Tiempo transcurrido: 7.5s\n",
            "[Val][Época 16/30] Batch 40/65  Loss batch: 5.756379  Tiempo transcurrido: 10.9s\n",
            "[Val][Época 16/30] Batch 50/65  Loss batch: 10.365838  Tiempo transcurrido: 13.8s\n",
            "[Val][Época 16/30] Batch 60/65  Loss batch: 9.756096  Tiempo transcurrido: 16.4s\n",
            "[Val] Época 16 terminada en 17.3s  Loss media: 5.687026\n",
            "Época [16/30] Train Loss: 0.094501 | Val Loss: 5.687026\n",
            "  🟢 Nuevo mejor modelo guardado en: /content/cats_dataset/posenet_light_best.pth\n",
            "============================================================\n",
            "Comenzando época 17/30\n",
            "[Train][Época 17/30] Batch 0/226  Loss batch: 0.025935  Tiempo transcurrido: 0.5s\n",
            "[Train][Época 17/30] Batch 10/226  Loss batch: 0.031998  Tiempo transcurrido: 5.8s\n",
            "[Train][Época 17/30] Batch 20/226  Loss batch: 0.025171  Tiempo transcurrido: 11.2s\n",
            "[Train][Época 17/30] Batch 30/226  Loss batch: 0.032744  Tiempo transcurrido: 16.1s\n",
            "[Train][Época 17/30] Batch 40/226  Loss batch: 0.336605  Tiempo transcurrido: 21.9s\n",
            "[Train][Época 17/30] Batch 50/226  Loss batch: 0.022053  Tiempo transcurrido: 26.8s\n",
            "[Train][Época 17/30] Batch 60/226  Loss batch: 0.021567  Tiempo transcurrido: 31.9s\n",
            "[Train][Época 17/30] Batch 70/226  Loss batch: 0.339101  Tiempo transcurrido: 37.5s\n",
            "[Train][Época 17/30] Batch 80/226  Loss batch: 0.019525  Tiempo transcurrido: 42.5s\n",
            "[Train][Época 17/30] Batch 90/226  Loss batch: 0.017861  Tiempo transcurrido: 48.2s\n",
            "[Train][Época 17/30] Batch 100/226  Loss batch: 0.020514  Tiempo transcurrido: 53.1s\n",
            "[Train][Época 17/30] Batch 110/226  Loss batch: 0.020385  Tiempo transcurrido: 58.3s\n",
            "[Train][Época 17/30] Batch 120/226  Loss batch: 0.024752  Tiempo transcurrido: 64.0s\n",
            "[Train][Época 17/30] Batch 130/226  Loss batch: 0.020110  Tiempo transcurrido: 69.0s\n",
            "[Train][Época 17/30] Batch 140/226  Loss batch: 0.022978  Tiempo transcurrido: 74.7s\n",
            "[Train][Época 17/30] Batch 150/226  Loss batch: 0.021435  Tiempo transcurrido: 79.7s\n",
            "[Train][Época 17/30] Batch 160/226  Loss batch: 0.026891  Tiempo transcurrido: 84.9s\n",
            "[Train][Época 17/30] Batch 170/226  Loss batch: 0.022817  Tiempo transcurrido: 90.5s\n",
            "[Train][Época 17/30] Batch 180/226  Loss batch: 0.024298  Tiempo transcurrido: 95.5s\n",
            "[Train][Época 17/30] Batch 190/226  Loss batch: 0.019269  Tiempo transcurrido: 101.3s\n",
            "[Train][Época 17/30] Batch 200/226  Loss batch: 0.024047  Tiempo transcurrido: 106.1s\n",
            "[Train][Época 17/30] Batch 210/226  Loss batch: 0.020256  Tiempo transcurrido: 111.2s\n",
            "[Train][Época 17/30] Batch 220/226  Loss batch: 0.021526  Tiempo transcurrido: 116.9s\n",
            "[Train] Época 17 terminada en 119.0s  Loss media: 0.061714\n",
            "[Val][Época 17/30] Batch 0/65  Loss batch: 0.210271  Tiempo transcurrido: 0.2s\n",
            "[Val][Época 17/30] Batch 10/65  Loss batch: 0.334737  Tiempo transcurrido: 2.6s\n",
            "[Val][Época 17/30] Batch 20/65  Loss batch: 1.218096  Tiempo transcurrido: 5.1s\n",
            "[Val][Época 17/30] Batch 30/65  Loss batch: 5.594238  Tiempo transcurrido: 8.7s\n",
            "[Val][Época 17/30] Batch 40/65  Loss batch: 6.035761  Tiempo transcurrido: 11.5s\n",
            "[Val][Época 17/30] Batch 50/65  Loss batch: 10.409515  Tiempo transcurrido: 14.0s\n",
            "[Val][Época 17/30] Batch 60/65  Loss batch: 9.885395  Tiempo transcurrido: 16.7s\n",
            "[Val] Época 17 terminada en 17.5s  Loss media: 5.755545\n",
            "Época [17/30] Train Loss: 0.061714 | Val Loss: 5.755545\n",
            "============================================================\n",
            "Comenzando época 18/30\n",
            "[Train][Época 18/30] Batch 0/226  Loss batch: 0.335314  Tiempo transcurrido: 0.5s\n",
            "[Train][Época 18/30] Batch 10/226  Loss batch: 0.021527  Tiempo transcurrido: 6.5s\n",
            "[Train][Época 18/30] Batch 20/226  Loss batch: 0.329459  Tiempo transcurrido: 11.4s\n",
            "[Train][Época 18/30] Batch 30/226  Loss batch: 0.030792  Tiempo transcurrido: 17.3s\n",
            "[Train][Época 18/30] Batch 40/226  Loss batch: 0.027206  Tiempo transcurrido: 22.1s\n",
            "[Train][Época 18/30] Batch 50/226  Loss batch: 0.332454  Tiempo transcurrido: 27.0s\n",
            "[Train][Época 18/30] Batch 60/226  Loss batch: 0.015061  Tiempo transcurrido: 32.8s\n",
            "[Train][Época 18/30] Batch 70/226  Loss batch: 0.018798  Tiempo transcurrido: 37.7s\n",
            "[Train][Época 18/30] Batch 80/226  Loss batch: 0.019044  Tiempo transcurrido: 43.6s\n",
            "[Train][Época 18/30] Batch 90/226  Loss batch: 0.028891  Tiempo transcurrido: 48.5s\n",
            "[Train][Época 18/30] Batch 100/226  Loss batch: 0.327834  Tiempo transcurrido: 53.4s\n",
            "[Train][Época 18/30] Batch 110/226  Loss batch: 0.093451  Tiempo transcurrido: 59.3s\n",
            "[Train][Época 18/30] Batch 120/226  Loss batch: 0.024212  Tiempo transcurrido: 64.2s\n",
            "[Train][Época 18/30] Batch 130/226  Loss batch: 0.022688  Tiempo transcurrido: 70.1s\n",
            "[Train][Época 18/30] Batch 140/226  Loss batch: 0.029017  Tiempo transcurrido: 75.0s\n",
            "[Train][Época 18/30] Batch 150/226  Loss batch: 0.029916  Tiempo transcurrido: 79.9s\n",
            "[Train][Época 18/30] Batch 160/226  Loss batch: 0.020248  Tiempo transcurrido: 85.8s\n",
            "[Train][Época 18/30] Batch 170/226  Loss batch: 0.020574  Tiempo transcurrido: 90.7s\n",
            "[Train][Época 18/30] Batch 180/226  Loss batch: 0.018871  Tiempo transcurrido: 96.6s\n",
            "[Train][Época 18/30] Batch 190/226  Loss batch: 0.022321  Tiempo transcurrido: 101.5s\n",
            "[Train][Época 18/30] Batch 200/226  Loss batch: 0.059791  Tiempo transcurrido: 106.4s\n",
            "[Train][Época 18/30] Batch 210/226  Loss batch: 0.026047  Tiempo transcurrido: 112.3s\n",
            "[Train][Época 18/30] Batch 220/226  Loss batch: 0.660662  Tiempo transcurrido: 117.1s\n",
            "[Train] Época 18 terminada en 119.2s  Loss media: 0.054016\n",
            "[Val][Época 18/30] Batch 0/65  Loss batch: 0.125562  Tiempo transcurrido: 0.2s\n",
            "[Val][Época 18/30] Batch 10/65  Loss batch: 0.211708  Tiempo transcurrido: 3.5s\n",
            "[Val][Época 18/30] Batch 20/65  Loss batch: 1.197538  Tiempo transcurrido: 5.9s\n",
            "[Val][Época 18/30] Batch 30/65  Loss batch: 5.383192  Tiempo transcurrido: 8.6s\n",
            "[Val][Época 18/30] Batch 40/65  Loss batch: 6.866117  Tiempo transcurrido: 11.4s\n",
            "[Val][Época 18/30] Batch 50/65  Loss batch: 10.418176  Tiempo transcurrido: 14.0s\n",
            "[Val][Época 18/30] Batch 60/65  Loss batch: 10.032685  Tiempo transcurrido: 17.5s\n",
            "[Val] Época 18 terminada en 18.3s  Loss media: 5.713798\n",
            "Época [18/30] Train Loss: 0.054016 | Val Loss: 5.713798\n",
            "============================================================\n",
            "Comenzando época 19/30\n",
            "[Train][Época 19/30] Batch 0/226  Loss batch: 0.051271  Tiempo transcurrido: 0.5s\n",
            "[Train][Época 19/30] Batch 10/226  Loss batch: 0.048802  Tiempo transcurrido: 5.5s\n",
            "[Train][Época 19/30] Batch 20/226  Loss batch: 0.055132  Tiempo transcurrido: 11.1s\n",
            "[Train][Época 19/30] Batch 30/226  Loss batch: 0.035132  Tiempo transcurrido: 16.2s\n",
            "[Train][Época 19/30] Batch 40/226  Loss batch: 0.058267  Tiempo transcurrido: 21.2s\n",
            "[Train][Época 19/30] Batch 50/226  Loss batch: 0.046415  Tiempo transcurrido: 27.1s\n",
            "[Train][Época 19/30] Batch 60/226  Loss batch: 0.028922  Tiempo transcurrido: 32.0s\n",
            "[Train][Época 19/30] Batch 70/226  Loss batch: 0.334391  Tiempo transcurrido: 37.7s\n",
            "[Train][Época 19/30] Batch 80/226  Loss batch: 0.020114  Tiempo transcurrido: 42.8s\n",
            "[Train][Época 19/30] Batch 90/226  Loss batch: 0.030593  Tiempo transcurrido: 47.7s\n",
            "[Train][Época 19/30] Batch 100/226  Loss batch: 0.021058  Tiempo transcurrido: 53.6s\n",
            "[Train][Época 19/30] Batch 110/226  Loss batch: 0.022527  Tiempo transcurrido: 58.4s\n",
            "[Train][Época 19/30] Batch 120/226  Loss batch: 0.024483  Tiempo transcurrido: 64.1s\n",
            "[Train][Época 19/30] Batch 130/226  Loss batch: 0.028106  Tiempo transcurrido: 69.2s\n",
            "[Train][Época 19/30] Batch 140/226  Loss batch: 0.020521  Tiempo transcurrido: 74.0s\n",
            "[Train][Época 19/30] Batch 150/226  Loss batch: 0.019524  Tiempo transcurrido: 79.9s\n",
            "[Train][Época 19/30] Batch 160/226  Loss batch: 0.021372  Tiempo transcurrido: 84.8s\n",
            "[Train][Época 19/30] Batch 170/226  Loss batch: 0.031778  Tiempo transcurrido: 90.3s\n",
            "[Train][Época 19/30] Batch 180/226  Loss batch: 0.029373  Tiempo transcurrido: 95.5s\n",
            "[Train][Época 19/30] Batch 190/226  Loss batch: 0.025500  Tiempo transcurrido: 100.4s\n",
            "[Train][Época 19/30] Batch 200/226  Loss batch: 0.023207  Tiempo transcurrido: 106.2s\n",
            "[Train][Época 19/30] Batch 210/226  Loss batch: 0.030161  Tiempo transcurrido: 111.1s\n",
            "[Train][Época 19/30] Batch 220/226  Loss batch: 0.350761  Tiempo transcurrido: 116.6s\n",
            "[Train] Época 19 terminada en 119.1s  Loss media: 0.066456\n",
            "[Val][Época 19/30] Batch 0/65  Loss batch: 0.194627  Tiempo transcurrido: 0.2s\n",
            "[Val][Época 19/30] Batch 10/65  Loss batch: 0.386841  Tiempo transcurrido: 2.6s\n",
            "[Val][Época 19/30] Batch 20/65  Loss batch: 2.426677  Tiempo transcurrido: 5.0s\n",
            "[Val][Época 19/30] Batch 30/65  Loss batch: 5.379809  Tiempo transcurrido: 7.6s\n",
            "[Val][Época 19/30] Batch 40/65  Loss batch: 7.389968  Tiempo transcurrido: 11.1s\n",
            "[Val][Época 19/30] Batch 50/65  Loss batch: 10.400545  Tiempo transcurrido: 13.8s\n",
            "[Val][Época 19/30] Batch 60/65  Loss batch: 9.870444  Tiempo transcurrido: 16.5s\n",
            "[Val] Época 19 terminada en 17.3s  Loss media: 5.806929\n",
            "Época [19/30] Train Loss: 0.066456 | Val Loss: 5.806929\n",
            "============================================================\n",
            "Comenzando época 20/30\n",
            "[Train][Época 20/30] Batch 0/226  Loss batch: 0.020254  Tiempo transcurrido: 0.5s\n",
            "[Train][Época 20/30] Batch 10/226  Loss batch: 0.020979  Tiempo transcurrido: 5.8s\n",
            "[Train][Época 20/30] Batch 20/226  Loss batch: 0.017862  Tiempo transcurrido: 11.3s\n",
            "[Train][Época 20/30] Batch 30/226  Loss batch: 0.026595  Tiempo transcurrido: 16.2s\n",
            "[Train][Época 20/30] Batch 40/226  Loss batch: 0.036586  Tiempo transcurrido: 22.1s\n",
            "[Train][Época 20/30] Batch 50/226  Loss batch: 0.018851  Tiempo transcurrido: 27.0s\n",
            "[Train][Época 20/30] Batch 60/226  Loss batch: 0.015066  Tiempo transcurrido: 32.3s\n",
            "[Train][Época 20/30] Batch 70/226  Loss batch: 0.022207  Tiempo transcurrido: 37.8s\n",
            "[Train][Época 20/30] Batch 80/226  Loss batch: 0.030230  Tiempo transcurrido: 42.7s\n",
            "[Train][Época 20/30] Batch 90/226  Loss batch: 0.025430  Tiempo transcurrido: 48.7s\n",
            "[Train][Época 20/30] Batch 100/226  Loss batch: 0.021104  Tiempo transcurrido: 53.5s\n",
            "[Train][Época 20/30] Batch 110/226  Loss batch: 0.025623  Tiempo transcurrido: 58.7s\n",
            "[Train][Época 20/30] Batch 120/226  Loss batch: 0.015780  Tiempo transcurrido: 64.2s\n",
            "[Train][Época 20/30] Batch 130/226  Loss batch: 0.024123  Tiempo transcurrido: 69.0s\n",
            "[Train][Época 20/30] Batch 140/226  Loss batch: 0.019469  Tiempo transcurrido: 74.9s\n",
            "[Train][Época 20/30] Batch 150/226  Loss batch: 0.024654  Tiempo transcurrido: 79.9s\n",
            "[Train][Época 20/30] Batch 160/226  Loss batch: 0.019676  Tiempo transcurrido: 85.0s\n",
            "[Train][Época 20/30] Batch 170/226  Loss batch: 0.035725  Tiempo transcurrido: 90.5s\n",
            "[Train][Época 20/30] Batch 180/226  Loss batch: 0.022712  Tiempo transcurrido: 95.5s\n",
            "[Train][Época 20/30] Batch 190/226  Loss batch: 0.026113  Tiempo transcurrido: 101.3s\n",
            "[Train][Época 20/30] Batch 200/226  Loss batch: 0.030921  Tiempo transcurrido: 106.2s\n",
            "[Train][Época 20/30] Batch 210/226  Loss batch: 0.016933  Tiempo transcurrido: 111.3s\n",
            "[Train][Época 20/30] Batch 220/226  Loss batch: 0.020814  Tiempo transcurrido: 116.9s\n",
            "[Train] Época 20 terminada en 119.1s  Loss media: 0.036049\n",
            "[Val][Época 20/30] Batch 0/65  Loss batch: 0.173454  Tiempo transcurrido: 0.2s\n",
            "[Val][Época 20/30] Batch 10/65  Loss batch: 0.397001  Tiempo transcurrido: 2.6s\n",
            "[Val][Época 20/30] Batch 20/65  Loss batch: 2.785678  Tiempo transcurrido: 5.0s\n",
            "[Val][Época 20/30] Batch 30/65  Loss batch: 5.693391  Tiempo transcurrido: 8.5s\n",
            "[Val][Época 20/30] Batch 40/65  Loss batch: 8.548877  Tiempo transcurrido: 11.3s\n",
            "[Val][Época 20/30] Batch 50/65  Loss batch: 10.398235  Tiempo transcurrido: 13.9s\n",
            "[Val][Época 20/30] Batch 60/65  Loss batch: 9.535095  Tiempo transcurrido: 16.5s\n",
            "[Val] Época 20 terminada en 17.3s  Loss media: 5.904120\n",
            "Época [20/30] Train Loss: 0.036049 | Val Loss: 5.904120\n",
            "============================================================\n",
            "Comenzando época 21/30\n",
            "[Train][Época 21/30] Batch 0/226  Loss batch: 0.018022  Tiempo transcurrido: 0.5s\n",
            "[Train][Época 21/30] Batch 10/226  Loss batch: 0.017946  Tiempo transcurrido: 6.5s\n",
            "[Train][Época 21/30] Batch 20/226  Loss batch: 0.024541  Tiempo transcurrido: 11.4s\n",
            "[Train][Época 21/30] Batch 30/226  Loss batch: 0.026066  Tiempo transcurrido: 17.3s\n",
            "[Train][Época 21/30] Batch 40/226  Loss batch: 0.017480  Tiempo transcurrido: 22.2s\n",
            "[Train][Época 21/30] Batch 50/226  Loss batch: 0.024614  Tiempo transcurrido: 27.1s\n",
            "[Train][Época 21/30] Batch 60/226  Loss batch: 0.018481  Tiempo transcurrido: 32.9s\n",
            "[Train][Época 21/30] Batch 70/226  Loss batch: 0.020170  Tiempo transcurrido: 37.9s\n",
            "[Train][Época 21/30] Batch 80/226  Loss batch: 0.020791  Tiempo transcurrido: 43.9s\n",
            "[Train][Época 21/30] Batch 90/226  Loss batch: 0.016771  Tiempo transcurrido: 48.9s\n",
            "[Train][Época 21/30] Batch 100/226  Loss batch: 0.024217  Tiempo transcurrido: 53.9s\n",
            "[Train][Época 21/30] Batch 110/226  Loss batch: 0.018505  Tiempo transcurrido: 59.7s\n",
            "[Train][Época 21/30] Batch 120/226  Loss batch: 0.020660  Tiempo transcurrido: 64.6s\n",
            "[Train][Época 21/30] Batch 130/226  Loss batch: 0.016070  Tiempo transcurrido: 70.6s\n",
            "[Train][Época 21/30] Batch 140/226  Loss batch: 0.018340  Tiempo transcurrido: 75.5s\n",
            "[Train][Época 21/30] Batch 150/226  Loss batch: 0.024181  Tiempo transcurrido: 80.5s\n",
            "[Train][Época 21/30] Batch 160/226  Loss batch: 0.021581  Tiempo transcurrido: 86.3s\n",
            "[Train][Época 21/30] Batch 170/226  Loss batch: 0.023366  Tiempo transcurrido: 91.2s\n",
            "[Train][Época 21/30] Batch 180/226  Loss batch: 0.013607  Tiempo transcurrido: 97.0s\n",
            "[Train][Época 21/30] Batch 190/226  Loss batch: 0.016366  Tiempo transcurrido: 101.9s\n",
            "[Train][Época 21/30] Batch 200/226  Loss batch: 0.014108  Tiempo transcurrido: 106.8s\n",
            "[Train][Época 21/30] Batch 210/226  Loss batch: 0.021445  Tiempo transcurrido: 112.7s\n",
            "[Train][Época 21/30] Batch 220/226  Loss batch: 0.019029  Tiempo transcurrido: 117.5s\n",
            "[Train] Época 21 terminada en 119.6s  Loss media: 0.022704\n",
            "[Val][Época 21/30] Batch 0/65  Loss batch: 0.170593  Tiempo transcurrido: 0.3s\n",
            "[Val][Época 21/30] Batch 10/65  Loss batch: 0.357819  Tiempo transcurrido: 3.5s\n",
            "[Val][Época 21/30] Batch 20/65  Loss batch: 2.748031  Tiempo transcurrido: 5.9s\n",
            "[Val][Época 21/30] Batch 30/65  Loss batch: 5.645946  Tiempo transcurrido: 8.6s\n",
            "[Val][Época 21/30] Batch 40/65  Loss batch: 8.090014  Tiempo transcurrido: 11.4s\n",
            "[Val][Época 21/30] Batch 50/65  Loss batch: 10.396525  Tiempo transcurrido: 14.2s\n",
            "[Val][Época 21/30] Batch 60/65  Loss batch: 9.677350  Tiempo transcurrido: 17.5s\n",
            "[Val] Época 21 terminada en 18.4s  Loss media: 5.881087\n",
            "Época [21/30] Train Loss: 0.022704 | Val Loss: 5.881087\n",
            "============================================================\n",
            "Comenzando época 22/30\n",
            "[Train][Época 22/30] Batch 0/226  Loss batch: 0.016680  Tiempo transcurrido: 0.5s\n",
            "[Train][Época 22/30] Batch 10/226  Loss batch: 0.023980  Tiempo transcurrido: 5.5s\n",
            "[Train][Época 22/30] Batch 20/226  Loss batch: 0.017420  Tiempo transcurrido: 11.3s\n",
            "[Train][Época 22/30] Batch 30/226  Loss batch: 0.022302  Tiempo transcurrido: 16.3s\n",
            "[Train][Época 22/30] Batch 40/226  Loss batch: 0.024352  Tiempo transcurrido: 21.1s\n",
            "[Train][Época 22/30] Batch 50/226  Loss batch: 0.021781  Tiempo transcurrido: 27.0s\n",
            "[Train][Época 22/30] Batch 60/226  Loss batch: 0.020193  Tiempo transcurrido: 32.0s\n",
            "[Train][Época 22/30] Batch 70/226  Loss batch: 0.014754  Tiempo transcurrido: 37.8s\n",
            "[Train][Época 22/30] Batch 80/226  Loss batch: 0.015394  Tiempo transcurrido: 42.8s\n",
            "[Train][Época 22/30] Batch 90/226  Loss batch: 0.014797  Tiempo transcurrido: 47.8s\n",
            "[Train][Época 22/30] Batch 100/226  Loss batch: 0.017880  Tiempo transcurrido: 53.7s\n",
            "[Train][Época 22/30] Batch 110/226  Loss batch: 0.026309  Tiempo transcurrido: 58.6s\n",
            "[Train][Época 22/30] Batch 120/226  Loss batch: 0.025012  Tiempo transcurrido: 64.6s\n",
            "[Train][Época 22/30] Batch 130/226  Loss batch: 0.017084  Tiempo transcurrido: 69.6s\n",
            "[Train][Época 22/30] Batch 140/226  Loss batch: 0.012943  Tiempo transcurrido: 74.4s\n",
            "[Train][Época 22/30] Batch 150/226  Loss batch: 0.017550  Tiempo transcurrido: 80.3s\n",
            "[Train][Época 22/30] Batch 160/226  Loss batch: 0.012160  Tiempo transcurrido: 85.1s\n",
            "[Train][Época 22/30] Batch 170/226  Loss batch: 0.018264  Tiempo transcurrido: 91.0s\n",
            "[Train][Época 22/30] Batch 180/226  Loss batch: 0.021412  Tiempo transcurrido: 96.0s\n",
            "[Train][Época 22/30] Batch 190/226  Loss batch: 0.012557  Tiempo transcurrido: 100.9s\n",
            "[Train][Época 22/30] Batch 200/226  Loss batch: 0.014789  Tiempo transcurrido: 106.8s\n",
            "[Train][Época 22/30] Batch 210/226  Loss batch: 0.018749  Tiempo transcurrido: 111.7s\n",
            "[Train][Época 22/30] Batch 220/226  Loss batch: 0.013802  Tiempo transcurrido: 117.5s\n",
            "[Train] Época 22 terminada en 119.6s  Loss media: 0.019847\n",
            "[Val][Época 22/30] Batch 0/65  Loss batch: 0.202446  Tiempo transcurrido: 0.2s\n",
            "[Val][Época 22/30] Batch 10/65  Loss batch: 0.414981  Tiempo transcurrido: 2.6s\n",
            "[Val][Época 22/30] Batch 20/65  Loss batch: 2.858961  Tiempo transcurrido: 4.9s\n",
            "[Val][Época 22/30] Batch 30/65  Loss batch: 5.741426  Tiempo transcurrido: 7.6s\n",
            "[Val][Época 22/30] Batch 40/65  Loss batch: 8.335218  Tiempo transcurrido: 11.3s\n",
            "[Val][Época 22/30] Batch 50/65  Loss batch: 10.380666  Tiempo transcurrido: 13.9s\n",
            "[Val][Época 22/30] Batch 60/65  Loss batch: 9.827124  Tiempo transcurrido: 16.5s\n",
            "[Val] Época 22 terminada en 17.3s  Loss media: 5.926742\n",
            "Época [22/30] Train Loss: 0.019847 | Val Loss: 5.926742\n",
            "============================================================\n",
            "Comenzando época 23/30\n",
            "[Train][Época 23/30] Batch 0/226  Loss batch: 0.015149  Tiempo transcurrido: 0.5s\n",
            "[Train][Época 23/30] Batch 10/226  Loss batch: 0.025094  Tiempo transcurrido: 6.1s\n",
            "[Train][Época 23/30] Batch 20/226  Loss batch: 0.016608  Tiempo transcurrido: 11.3s\n",
            "[Train][Época 23/30] Batch 30/226  Loss batch: 0.020536  Tiempo transcurrido: 16.3s\n",
            "[Train][Época 23/30] Batch 40/226  Loss batch: 0.016167  Tiempo transcurrido: 22.1s\n",
            "[Train][Época 23/30] Batch 50/226  Loss batch: 0.021712  Tiempo transcurrido: 27.0s\n",
            "[Train][Época 23/30] Batch 60/226  Loss batch: 0.013884  Tiempo transcurrido: 32.6s\n",
            "[Train][Época 23/30] Batch 70/226  Loss batch: 0.017946  Tiempo transcurrido: 37.8s\n",
            "[Train][Época 23/30] Batch 80/226  Loss batch: 0.018021  Tiempo transcurrido: 42.7s\n",
            "[Train][Época 23/30] Batch 90/226  Loss batch: 0.017811  Tiempo transcurrido: 48.6s\n",
            "[Train][Época 23/30] Batch 100/226  Loss batch: 0.018450  Tiempo transcurrido: 53.5s\n",
            "[Train][Época 23/30] Batch 110/226  Loss batch: 0.022675  Tiempo transcurrido: 59.1s\n",
            "[Train][Época 23/30] Batch 120/226  Loss batch: 0.019825  Tiempo transcurrido: 64.3s\n",
            "[Train][Época 23/30] Batch 130/226  Loss batch: 0.022821  Tiempo transcurrido: 69.3s\n",
            "[Train][Época 23/30] Batch 140/226  Loss batch: 0.150693  Tiempo transcurrido: 75.2s\n",
            "[Train][Época 23/30] Batch 150/226  Loss batch: 0.328183  Tiempo transcurrido: 80.1s\n",
            "[Train][Época 23/30] Batch 160/226  Loss batch: 0.030158  Tiempo transcurrido: 85.8s\n",
            "[Train][Época 23/30] Batch 170/226  Loss batch: 0.018670  Tiempo transcurrido: 91.0s\n",
            "[Train][Época 23/30] Batch 180/226  Loss batch: 0.015183  Tiempo transcurrido: 95.9s\n",
            "[Train][Época 23/30] Batch 190/226  Loss batch: 0.024807  Tiempo transcurrido: 101.8s\n",
            "[Train][Época 23/30] Batch 200/226  Loss batch: 0.019640  Tiempo transcurrido: 106.7s\n",
            "[Train][Época 23/30] Batch 210/226  Loss batch: 0.015613  Tiempo transcurrido: 112.5s\n",
            "[Train][Época 23/30] Batch 220/226  Loss batch: 0.022043  Tiempo transcurrido: 117.6s\n",
            "[Train] Época 23 terminada en 119.7s  Loss media: 0.048745\n",
            "[Val][Época 23/30] Batch 0/65  Loss batch: 0.204977  Tiempo transcurrido: 0.2s\n",
            "[Val][Época 23/30] Batch 10/65  Loss batch: 0.325179  Tiempo transcurrido: 2.6s\n",
            "[Val][Época 23/30] Batch 20/65  Loss batch: 1.934038  Tiempo transcurrido: 5.6s\n",
            "[Val][Época 23/30] Batch 30/65  Loss batch: 5.424673  Tiempo transcurrido: 8.5s\n",
            "[Val][Época 23/30] Batch 40/65  Loss batch: 5.781343  Tiempo transcurrido: 11.3s\n",
            "[Val][Época 23/30] Batch 50/65  Loss batch: 10.389074  Tiempo transcurrido: 13.8s\n",
            "[Val][Época 23/30] Batch 60/65  Loss batch: 9.273749  Tiempo transcurrido: 16.5s\n",
            "[Val] Época 23 terminada en 17.6s  Loss media: 5.766561\n",
            "Época [23/30] Train Loss: 0.048745 | Val Loss: 5.766561\n",
            "============================================================\n",
            "Comenzando época 24/30\n",
            "[Train][Época 24/30] Batch 0/226  Loss batch: 0.013477  Tiempo transcurrido: 0.7s\n",
            "[Train][Época 24/30] Batch 10/226  Loss batch: 0.021213  Tiempo transcurrido: 6.1s\n",
            "[Train][Época 24/30] Batch 20/226  Loss batch: 0.019639  Tiempo transcurrido: 11.0s\n",
            "[Train][Época 24/30] Batch 30/226  Loss batch: 0.028299  Tiempo transcurrido: 16.9s\n",
            "[Train][Época 24/30] Batch 40/226  Loss batch: 0.016936  Tiempo transcurrido: 21.9s\n",
            "[Train][Época 24/30] Batch 50/226  Loss batch: 0.022927  Tiempo transcurrido: 27.5s\n",
            "[Train][Época 24/30] Batch 60/226  Loss batch: 0.022730  Tiempo transcurrido: 32.8s\n",
            "[Train][Época 24/30] Batch 70/226  Loss batch: 0.018701  Tiempo transcurrido: 37.8s\n",
            "[Train][Época 24/30] Batch 80/226  Loss batch: 0.021420  Tiempo transcurrido: 43.6s\n",
            "[Train][Época 24/30] Batch 90/226  Loss batch: 0.016040  Tiempo transcurrido: 48.5s\n",
            "[Train][Época 24/30] Batch 100/226  Loss batch: 0.020453  Tiempo transcurrido: 54.1s\n",
            "[Train][Época 24/30] Batch 110/226  Loss batch: 0.016521  Tiempo transcurrido: 59.4s\n",
            "[Train][Época 24/30] Batch 120/226  Loss batch: 0.016679  Tiempo transcurrido: 64.3s\n",
            "[Train][Época 24/30] Batch 130/226  Loss batch: 0.016632  Tiempo transcurrido: 70.2s\n",
            "[Train][Época 24/30] Batch 140/226  Loss batch: 0.029807  Tiempo transcurrido: 75.1s\n",
            "[Train][Época 24/30] Batch 150/226  Loss batch: 0.014509  Tiempo transcurrido: 80.8s\n",
            "[Train][Época 24/30] Batch 160/226  Loss batch: 0.016831  Tiempo transcurrido: 86.0s\n",
            "[Train][Época 24/30] Batch 170/226  Loss batch: 0.011744  Tiempo transcurrido: 90.9s\n",
            "[Train][Época 24/30] Batch 180/226  Loss batch: 0.028761  Tiempo transcurrido: 96.8s\n",
            "[Train][Época 24/30] Batch 190/226  Loss batch: 0.019371  Tiempo transcurrido: 101.7s\n",
            "[Train][Época 24/30] Batch 200/226  Loss batch: 0.018990  Tiempo transcurrido: 107.3s\n",
            "[Train][Época 24/30] Batch 210/226  Loss batch: 0.020499  Tiempo transcurrido: 112.4s\n",
            "[Train][Época 24/30] Batch 220/226  Loss batch: 0.025588  Tiempo transcurrido: 117.3s\n",
            "[Train] Época 24 terminada en 119.9s  Loss media: 0.035963\n",
            "[Val][Época 24/30] Batch 0/65  Loss batch: 0.205251  Tiempo transcurrido: 0.3s\n",
            "[Val][Época 24/30] Batch 10/65  Loss batch: 0.330603  Tiempo transcurrido: 3.1s\n",
            "[Val][Época 24/30] Batch 20/65  Loss batch: 1.723611  Tiempo transcurrido: 5.4s\n",
            "[Val][Época 24/30] Batch 30/65  Loss batch: 5.397615  Tiempo transcurrido: 8.1s\n",
            "[Val][Época 24/30] Batch 40/65  Loss batch: 6.122934  Tiempo transcurrido: 10.9s\n",
            "[Val][Época 24/30] Batch 50/65  Loss batch: 10.442327  Tiempo transcurrido: 14.1s\n",
            "[Val][Época 24/30] Batch 60/65  Loss batch: 10.099110  Tiempo transcurrido: 17.0s\n",
            "[Val] Época 24 terminada en 17.8s  Loss media: 5.656888\n",
            "Época [24/30] Train Loss: 0.035963 | Val Loss: 5.656888\n",
            "  🟢 Nuevo mejor modelo guardado en: /content/cats_dataset/posenet_light_best.pth\n",
            "============================================================\n",
            "Comenzando época 25/30\n",
            "[Train][Época 25/30] Batch 0/226  Loss batch: 0.325427  Tiempo transcurrido: 0.6s\n",
            "[Train][Época 25/30] Batch 10/226  Loss batch: 0.021397  Tiempo transcurrido: 5.6s\n",
            "[Train][Época 25/30] Batch 20/226  Loss batch: 0.019078  Tiempo transcurrido: 11.5s\n",
            "[Train][Época 25/30] Batch 30/226  Loss batch: 0.018049  Tiempo transcurrido: 16.5s\n",
            "[Train][Época 25/30] Batch 40/226  Loss batch: 0.013299  Tiempo transcurrido: 22.0s\n",
            "[Train][Época 25/30] Batch 50/226  Loss batch: 0.017849  Tiempo transcurrido: 27.3s\n",
            "[Train][Época 25/30] Batch 60/226  Loss batch: 0.017282  Tiempo transcurrido: 32.2s\n",
            "[Train][Época 25/30] Batch 70/226  Loss batch: 0.019205  Tiempo transcurrido: 38.2s\n",
            "[Train][Época 25/30] Batch 80/226  Loss batch: 0.015516  Tiempo transcurrido: 43.1s\n",
            "[Train][Época 25/30] Batch 90/226  Loss batch: 0.020068  Tiempo transcurrido: 48.8s\n",
            "[Train][Época 25/30] Batch 100/226  Loss batch: 0.014495  Tiempo transcurrido: 53.9s\n",
            "[Train][Época 25/30] Batch 110/226  Loss batch: 0.020023  Tiempo transcurrido: 58.9s\n",
            "[Train][Época 25/30] Batch 120/226  Loss batch: 0.015167  Tiempo transcurrido: 64.7s\n",
            "[Train][Época 25/30] Batch 130/226  Loss batch: 0.023015  Tiempo transcurrido: 69.7s\n",
            "[Train][Época 25/30] Batch 140/226  Loss batch: 0.366682  Tiempo transcurrido: 75.3s\n",
            "[Train][Época 25/30] Batch 150/226  Loss batch: 0.060619  Tiempo transcurrido: 80.5s\n",
            "[Train][Época 25/30] Batch 160/226  Loss batch: 0.021571  Tiempo transcurrido: 85.4s\n",
            "[Train][Época 25/30] Batch 170/226  Loss batch: 0.033048  Tiempo transcurrido: 91.2s\n",
            "[Train][Época 25/30] Batch 180/226  Loss batch: 0.023261  Tiempo transcurrido: 96.1s\n",
            "[Train][Época 25/30] Batch 190/226  Loss batch: 0.021576  Tiempo transcurrido: 101.8s\n",
            "[Train][Época 25/30] Batch 200/226  Loss batch: 0.019980  Tiempo transcurrido: 107.1s\n",
            "[Train][Época 25/30] Batch 210/226  Loss batch: 0.028545  Tiempo transcurrido: 112.0s\n",
            "[Train][Época 25/30] Batch 220/226  Loss batch: 0.025460  Tiempo transcurrido: 117.8s\n",
            "[Train] Época 25 terminada en 119.9s  Loss media: 0.029073\n",
            "[Val][Época 25/30] Batch 0/65  Loss batch: 0.210462  Tiempo transcurrido: 0.2s\n",
            "[Val][Época 25/30] Batch 10/65  Loss batch: 0.327431  Tiempo transcurrido: 2.6s\n",
            "[Val][Época 25/30] Batch 20/65  Loss batch: 2.736293  Tiempo transcurrido: 4.9s\n",
            "[Val][Época 25/30] Batch 30/65  Loss batch: 5.374366  Tiempo transcurrido: 8.2s\n",
            "[Val][Época 25/30] Batch 40/65  Loss batch: 6.604724  Tiempo transcurrido: 11.3s\n",
            "[Val][Época 25/30] Batch 50/65  Loss batch: 10.389668  Tiempo transcurrido: 13.9s\n",
            "[Val][Época 25/30] Batch 60/65  Loss batch: 10.144985  Tiempo transcurrido: 16.5s\n",
            "[Val] Época 25 terminada en 17.4s  Loss media: 5.717534\n",
            "Época [25/30] Train Loss: 0.029073 | Val Loss: 5.717534\n",
            "============================================================\n",
            "Comenzando época 26/30\n",
            "[Train][Época 26/30] Batch 0/226  Loss batch: 0.016980  Tiempo transcurrido: 0.5s\n",
            "[Train][Época 26/30] Batch 10/226  Loss batch: 0.024133  Tiempo transcurrido: 6.5s\n",
            "[Train][Época 26/30] Batch 20/226  Loss batch: 0.016449  Tiempo transcurrido: 11.5s\n",
            "[Train][Época 26/30] Batch 30/226  Loss batch: 0.020554  Tiempo transcurrido: 17.0s\n",
            "[Train][Época 26/30] Batch 40/226  Loss batch: 0.014048  Tiempo transcurrido: 22.4s\n",
            "[Train][Época 26/30] Batch 50/226  Loss batch: 0.024064  Tiempo transcurrido: 27.4s\n",
            "[Train][Época 26/30] Batch 60/226  Loss batch: 0.021347  Tiempo transcurrido: 33.4s\n",
            "[Train][Época 26/30] Batch 70/226  Loss batch: 0.029511  Tiempo transcurrido: 38.3s\n",
            "[Train][Época 26/30] Batch 80/226  Loss batch: 0.018422  Tiempo transcurrido: 44.2s\n",
            "[Train][Época 26/30] Batch 90/226  Loss batch: 0.018895  Tiempo transcurrido: 49.4s\n",
            "[Train][Época 26/30] Batch 100/226  Loss batch: 0.029961  Tiempo transcurrido: 54.4s\n",
            "[Train][Época 26/30] Batch 110/226  Loss batch: 0.021502  Tiempo transcurrido: 60.5s\n",
            "[Train][Época 26/30] Batch 120/226  Loss batch: 0.017674  Tiempo transcurrido: 65.5s\n",
            "[Train][Época 26/30] Batch 130/226  Loss batch: 0.018958  Tiempo transcurrido: 71.5s\n",
            "[Train][Época 26/30] Batch 140/226  Loss batch: 0.024808  Tiempo transcurrido: 76.5s\n",
            "[Train][Época 26/30] Batch 150/226  Loss batch: 0.024080  Tiempo transcurrido: 81.5s\n",
            "[Train][Época 26/30] Batch 160/226  Loss batch: 0.047304  Tiempo transcurrido: 87.5s\n",
            "[Train][Época 26/30] Batch 170/226  Loss batch: 0.023439  Tiempo transcurrido: 92.5s\n",
            "[Train][Época 26/30] Batch 180/226  Loss batch: 0.025640  Tiempo transcurrido: 98.6s\n",
            "[Train][Época 26/30] Batch 190/226  Loss batch: 0.033803  Tiempo transcurrido: 103.7s\n",
            "[Train][Época 26/30] Batch 200/226  Loss batch: 0.021683  Tiempo transcurrido: 109.0s\n",
            "[Train][Época 26/30] Batch 210/226  Loss batch: 0.027214  Tiempo transcurrido: 114.7s\n",
            "[Train][Época 26/30] Batch 220/226  Loss batch: 0.025733  Tiempo transcurrido: 119.7s\n",
            "[Train] Época 26 terminada en 122.2s  Loss media: 0.040916\n",
            "[Val][Época 26/30] Batch 0/65  Loss batch: 0.180358  Tiempo transcurrido: 0.3s\n",
            "[Val][Época 26/30] Batch 10/65  Loss batch: 0.439282  Tiempo transcurrido: 3.4s\n",
            "[Val][Época 26/30] Batch 20/65  Loss batch: 1.771364  Tiempo transcurrido: 5.9s\n",
            "[Val][Época 26/30] Batch 30/65  Loss batch: 5.585883  Tiempo transcurrido: 8.6s\n",
            "[Val][Época 26/30] Batch 40/65  Loss batch: 6.150845  Tiempo transcurrido: 11.4s\n",
            "[Val][Época 26/30] Batch 50/65  Loss batch: 10.388472  Tiempo transcurrido: 14.6s\n",
            "[Val][Época 26/30] Batch 60/65  Loss batch: 10.107708  Tiempo transcurrido: 17.6s\n",
            "[Val] Época 26 terminada en 18.5s  Loss media: 5.799202\n",
            "Época [26/30] Train Loss: 0.040916 | Val Loss: 5.799202\n",
            "============================================================\n",
            "Comenzando época 27/30\n",
            "[Train][Época 27/30] Batch 0/226  Loss batch: 0.018984  Tiempo transcurrido: 0.6s\n",
            "[Train][Época 27/30] Batch 10/226  Loss batch: 0.021286  Tiempo transcurrido: 5.5s\n",
            "[Train][Época 27/30] Batch 20/226  Loss batch: 0.017300  Tiempo transcurrido: 11.4s\n",
            "[Train][Época 27/30] Batch 30/226  Loss batch: 0.031733  Tiempo transcurrido: 16.3s\n",
            "[Train][Época 27/30] Batch 40/226  Loss batch: 0.019401  Tiempo transcurrido: 21.5s\n",
            "[Train][Época 27/30] Batch 50/226  Loss batch: 0.025043  Tiempo transcurrido: 27.2s\n",
            "[Train][Época 27/30] Batch 60/226  Loss batch: 0.033646  Tiempo transcurrido: 32.1s\n",
            "[Train][Época 27/30] Batch 70/226  Loss batch: 0.023653  Tiempo transcurrido: 38.0s\n",
            "[Train][Época 27/30] Batch 80/226  Loss batch: 0.026532  Tiempo transcurrido: 42.9s\n",
            "[Train][Época 27/30] Batch 90/226  Loss batch: 0.019262  Tiempo transcurrido: 48.3s\n",
            "[Train][Época 27/30] Batch 100/226  Loss batch: 0.016041  Tiempo transcurrido: 53.9s\n",
            "[Train][Época 27/30] Batch 110/226  Loss batch: 0.027423  Tiempo transcurrido: 58.8s\n",
            "[Train][Época 27/30] Batch 120/226  Loss batch: 0.022154  Tiempo transcurrido: 64.7s\n",
            "[Train][Época 27/30] Batch 130/226  Loss batch: 0.018895  Tiempo transcurrido: 69.6s\n",
            "[Train][Época 27/30] Batch 140/226  Loss batch: 0.015683  Tiempo transcurrido: 75.0s\n",
            "[Train][Época 27/30] Batch 150/226  Loss batch: 0.012252  Tiempo transcurrido: 80.5s\n",
            "[Train][Época 27/30] Batch 160/226  Loss batch: 0.020329  Tiempo transcurrido: 85.4s\n",
            "[Train][Época 27/30] Batch 170/226  Loss batch: 0.323507  Tiempo transcurrido: 91.3s\n",
            "[Train][Época 27/30] Batch 180/226  Loss batch: 0.014694  Tiempo transcurrido: 96.2s\n",
            "[Train][Época 27/30] Batch 190/226  Loss batch: 0.028242  Tiempo transcurrido: 101.6s\n",
            "[Train][Época 27/30] Batch 200/226  Loss batch: 0.014467  Tiempo transcurrido: 107.0s\n",
            "[Train][Época 27/30] Batch 210/226  Loss batch: 0.016804  Tiempo transcurrido: 111.9s\n",
            "[Train][Época 27/30] Batch 220/226  Loss batch: 0.013479  Tiempo transcurrido: 117.8s\n",
            "[Train] Época 27 terminada en 119.9s  Loss media: 0.027004\n",
            "[Val][Época 27/30] Batch 0/65  Loss batch: 0.190728  Tiempo transcurrido: 0.2s\n",
            "[Val][Época 27/30] Batch 10/65  Loss batch: 0.374863  Tiempo transcurrido: 2.6s\n",
            "[Val][Época 27/30] Batch 20/65  Loss batch: 1.479673  Tiempo transcurrido: 5.0s\n",
            "[Val][Época 27/30] Batch 30/65  Loss batch: 5.483764  Tiempo transcurrido: 8.0s\n",
            "[Val][Época 27/30] Batch 40/65  Loss batch: 6.189192  Tiempo transcurrido: 11.5s\n",
            "[Val][Época 27/30] Batch 50/65  Loss batch: 10.355163  Tiempo transcurrido: 14.0s\n",
            "[Val][Época 27/30] Batch 60/65  Loss batch: 10.056498  Tiempo transcurrido: 16.6s\n",
            "[Val] Época 27 terminada en 17.5s  Loss media: 5.778338\n",
            "Época [27/30] Train Loss: 0.027004 | Val Loss: 5.778338\n",
            "============================================================\n",
            "Comenzando época 28/30\n",
            "[Train][Época 28/30] Batch 0/226  Loss batch: 0.016416  Tiempo transcurrido: 0.5s\n",
            "[Train][Época 28/30] Batch 10/226  Loss batch: 0.022842  Tiempo transcurrido: 6.5s\n",
            "[Train][Época 28/30] Batch 20/226  Loss batch: 0.015311  Tiempo transcurrido: 11.4s\n",
            "[Train][Época 28/30] Batch 30/226  Loss batch: 0.022143  Tiempo transcurrido: 16.5s\n",
            "[Train][Época 28/30] Batch 40/226  Loss batch: 0.020081  Tiempo transcurrido: 22.2s\n",
            "[Train][Época 28/30] Batch 50/226  Loss batch: 0.019278  Tiempo transcurrido: 27.1s\n",
            "[Train][Época 28/30] Batch 60/226  Loss batch: 0.018584  Tiempo transcurrido: 33.1s\n",
            "[Train][Época 28/30] Batch 70/226  Loss batch: 0.015924  Tiempo transcurrido: 38.0s\n",
            "[Train][Época 28/30] Batch 80/226  Loss batch: 0.016767  Tiempo transcurrido: 43.0s\n",
            "[Train][Época 28/30] Batch 90/226  Loss batch: 0.015594  Tiempo transcurrido: 48.8s\n",
            "[Train][Época 28/30] Batch 100/226  Loss batch: 0.014332  Tiempo transcurrido: 53.7s\n",
            "[Train][Época 28/30] Batch 110/226  Loss batch: 0.017431  Tiempo transcurrido: 59.6s\n",
            "[Train][Época 28/30] Batch 120/226  Loss batch: 0.020761  Tiempo transcurrido: 64.5s\n",
            "[Train][Época 28/30] Batch 130/226  Loss batch: 0.013792  Tiempo transcurrido: 69.6s\n",
            "[Train][Época 28/30] Batch 140/226  Loss batch: 0.013132  Tiempo transcurrido: 75.4s\n",
            "[Train][Época 28/30] Batch 150/226  Loss batch: 0.016889  Tiempo transcurrido: 80.2s\n",
            "[Train][Época 28/30] Batch 160/226  Loss batch: 0.013188  Tiempo transcurrido: 86.2s\n",
            "[Train][Época 28/30] Batch 170/226  Loss batch: 0.017331  Tiempo transcurrido: 91.1s\n",
            "[Train][Época 28/30] Batch 180/226  Loss batch: 0.013369  Tiempo transcurrido: 96.3s\n",
            "[Train][Época 28/30] Batch 190/226  Loss batch: 0.025004  Tiempo transcurrido: 101.9s\n",
            "[Train][Época 28/30] Batch 200/226  Loss batch: 0.015232  Tiempo transcurrido: 106.8s\n",
            "[Train][Época 28/30] Batch 210/226  Loss batch: 0.017726  Tiempo transcurrido: 112.7s\n",
            "[Train][Época 28/30] Batch 220/226  Loss batch: 0.020050  Tiempo transcurrido: 117.6s\n",
            "[Train] Época 28 terminada en 119.7s  Loss media: 0.026005\n",
            "[Val][Época 28/30] Batch 0/65  Loss batch: 0.195028  Tiempo transcurrido: 0.2s\n",
            "[Val][Época 28/30] Batch 10/65  Loss batch: 0.400060  Tiempo transcurrido: 2.7s\n",
            "[Val][Época 28/30] Batch 20/65  Loss batch: 1.698799  Tiempo transcurrido: 5.9s\n",
            "[Val][Época 28/30] Batch 30/65  Loss batch: 5.493861  Tiempo transcurrido: 8.5s\n",
            "[Val][Época 28/30] Batch 40/65  Loss batch: 6.557351  Tiempo transcurrido: 11.3s\n",
            "[Val][Época 28/30] Batch 50/65  Loss batch: 10.407563  Tiempo transcurrido: 13.8s\n",
            "[Val][Época 28/30] Batch 60/65  Loss batch: 10.081594  Tiempo transcurrido: 16.9s\n",
            "[Val] Época 28 terminada en 18.0s  Loss media: 5.872006\n",
            "Época [28/30] Train Loss: 0.026005 | Val Loss: 5.872006\n",
            "============================================================\n",
            "Comenzando época 29/30\n",
            "[Train][Época 29/30] Batch 0/226  Loss batch: 0.013688  Tiempo transcurrido: 0.8s\n",
            "[Train][Época 29/30] Batch 10/226  Loss batch: 0.019042  Tiempo transcurrido: 5.7s\n",
            "[Train][Época 29/30] Batch 20/226  Loss batch: 0.017845  Tiempo transcurrido: 10.5s\n",
            "[Train][Época 29/30] Batch 30/226  Loss batch: 0.023934  Tiempo transcurrido: 16.4s\n",
            "[Train][Época 29/30] Batch 40/226  Loss batch: 0.019528  Tiempo transcurrido: 21.4s\n",
            "[Train][Época 29/30] Batch 50/226  Loss batch: 0.018091  Tiempo transcurrido: 27.3s\n",
            "[Train][Época 29/30] Batch 60/226  Loss batch: 0.021273  Tiempo transcurrido: 32.3s\n",
            "[Train][Época 29/30] Batch 70/226  Loss batch: 0.016190  Tiempo transcurrido: 37.2s\n",
            "[Train][Época 29/30] Batch 80/226  Loss batch: 0.027284  Tiempo transcurrido: 43.1s\n",
            "[Train][Época 29/30] Batch 90/226  Loss batch: 0.325784  Tiempo transcurrido: 48.0s\n",
            "[Train][Época 29/30] Batch 100/226  Loss batch: 0.019917  Tiempo transcurrido: 53.9s\n",
            "[Train][Época 29/30] Batch 110/226  Loss batch: 0.021071  Tiempo transcurrido: 58.8s\n",
            "[Train][Época 29/30] Batch 120/226  Loss batch: 0.019459  Tiempo transcurrido: 63.7s\n",
            "[Train][Época 29/30] Batch 130/226  Loss batch: 0.021665  Tiempo transcurrido: 69.6s\n",
            "[Train][Época 29/30] Batch 140/226  Loss batch: 0.025967  Tiempo transcurrido: 74.5s\n",
            "[Train][Época 29/30] Batch 150/226  Loss batch: 0.020943  Tiempo transcurrido: 80.4s\n",
            "[Train][Época 29/30] Batch 160/226  Loss batch: 0.019573  Tiempo transcurrido: 85.3s\n",
            "[Train][Época 29/30] Batch 170/226  Loss batch: 0.021488  Tiempo transcurrido: 90.2s\n",
            "[Train][Época 29/30] Batch 180/226  Loss batch: 0.017639  Tiempo transcurrido: 96.2s\n",
            "[Train][Época 29/30] Batch 190/226  Loss batch: 0.019561  Tiempo transcurrido: 101.1s\n",
            "[Train][Época 29/30] Batch 200/226  Loss batch: 0.024108  Tiempo transcurrido: 106.9s\n",
            "[Train][Época 29/30] Batch 210/226  Loss batch: 0.019631  Tiempo transcurrido: 111.8s\n",
            "[Train][Época 29/30] Batch 220/226  Loss batch: 0.013951  Tiempo transcurrido: 116.7s\n",
            "[Train] Época 29 terminada en 119.7s  Loss media: 0.032427\n",
            "[Val][Época 29/30] Batch 0/65  Loss batch: 0.201530  Tiempo transcurrido: 0.3s\n",
            "[Val][Época 29/30] Batch 10/65  Loss batch: 0.325540  Tiempo transcurrido: 2.8s\n",
            "[Val][Época 29/30] Batch 20/65  Loss batch: 1.584224  Tiempo transcurrido: 5.1s\n",
            "[Val][Época 29/30] Batch 30/65  Loss batch: 5.653080  Tiempo transcurrido: 7.8s\n",
            "[Val][Época 29/30] Batch 40/65  Loss batch: 7.390569  Tiempo transcurrido: 10.6s\n",
            "[Val][Época 29/30] Batch 50/65  Loss batch: 10.417765  Tiempo transcurrido: 14.0s\n",
            "[Val][Época 29/30] Batch 60/65  Loss batch: 9.875134  Tiempo transcurrido: 16.6s\n",
            "[Val] Época 29 terminada en 17.4s  Loss media: 5.876757\n",
            "Época [29/30] Train Loss: 0.032427 | Val Loss: 5.876757\n",
            "============================================================\n",
            "Comenzando época 30/30\n",
            "[Train][Época 30/30] Batch 0/226  Loss batch: 0.016302  Tiempo transcurrido: 0.5s\n",
            "[Train][Época 30/30] Batch 10/226  Loss batch: 0.019667  Tiempo transcurrido: 5.5s\n",
            "[Train][Época 30/30] Batch 20/226  Loss batch: 0.017311  Tiempo transcurrido: 11.3s\n",
            "[Train][Época 30/30] Batch 30/226  Loss batch: 0.013996  Tiempo transcurrido: 16.3s\n",
            "[Train][Época 30/30] Batch 40/226  Loss batch: 0.013628  Tiempo transcurrido: 21.9s\n",
            "[Train][Época 30/30] Batch 50/226  Loss batch: 0.017581  Tiempo transcurrido: 27.0s\n",
            "[Train][Época 30/30] Batch 60/226  Loss batch: 0.015351  Tiempo transcurrido: 31.9s\n",
            "[Train][Época 30/30] Batch 70/226  Loss batch: 0.020495  Tiempo transcurrido: 37.8s\n",
            "[Train][Época 30/30] Batch 80/226  Loss batch: 0.019428  Tiempo transcurrido: 42.7s\n",
            "[Train][Época 30/30] Batch 90/226  Loss batch: 0.013951  Tiempo transcurrido: 48.4s\n",
            "[Train][Época 30/30] Batch 100/226  Loss batch: 0.017693  Tiempo transcurrido: 53.5s\n",
            "[Train][Época 30/30] Batch 110/226  Loss batch: 0.017971  Tiempo transcurrido: 58.4s\n",
            "[Train][Época 30/30] Batch 120/226  Loss batch: 0.015592  Tiempo transcurrido: 64.2s\n",
            "[Train][Época 30/30] Batch 130/226  Loss batch: 0.018051  Tiempo transcurrido: 69.1s\n",
            "[Train][Época 30/30] Batch 140/226  Loss batch: 0.007263  Tiempo transcurrido: 74.6s\n",
            "[Train][Época 30/30] Batch 150/226  Loss batch: 0.024603  Tiempo transcurrido: 79.8s\n",
            "[Train][Época 30/30] Batch 160/226  Loss batch: 0.015560  Tiempo transcurrido: 84.7s\n",
            "[Train][Época 30/30] Batch 170/226  Loss batch: 0.014865  Tiempo transcurrido: 90.7s\n",
            "[Train][Época 30/30] Batch 180/226  Loss batch: 0.018225  Tiempo transcurrido: 95.6s\n",
            "[Train][Época 30/30] Batch 190/226  Loss batch: 0.017497  Tiempo transcurrido: 101.1s\n",
            "[Train][Época 30/30] Batch 200/226  Loss batch: 0.016320  Tiempo transcurrido: 106.4s\n",
            "[Train][Época 30/30] Batch 210/226  Loss batch: 0.012174  Tiempo transcurrido: 111.3s\n",
            "[Train][Época 30/30] Batch 220/226  Loss batch: 0.012166  Tiempo transcurrido: 117.1s\n",
            "[Train] Época 30 terminada en 119.2s  Loss media: 0.021066\n",
            "[Val][Época 30/30] Batch 0/65  Loss batch: 0.224005  Tiempo transcurrido: 0.3s\n",
            "[Val][Época 30/30] Batch 10/65  Loss batch: 0.403851  Tiempo transcurrido: 2.7s\n",
            "[Val][Época 30/30] Batch 20/65  Loss batch: 1.739544  Tiempo transcurrido: 5.0s\n",
            "[Val][Época 30/30] Batch 30/65  Loss batch: 5.607131  Tiempo transcurrido: 8.2s\n",
            "[Val][Época 30/30] Batch 40/65  Loss batch: 7.630148  Tiempo transcurrido: 11.4s\n",
            "[Val][Época 30/30] Batch 50/65  Loss batch: 10.435119  Tiempo transcurrido: 13.9s\n",
            "[Val][Época 30/30] Batch 60/65  Loss batch: 9.848343  Tiempo transcurrido: 16.5s\n",
            "[Val] Época 30 terminada en 17.4s  Loss media: 5.907465\n",
            "Época [30/30] Train Loss: 0.021066 | Val Loss: 5.907465\n",
            "Entrenamiento terminado. Mejor val_loss: 5.656887865089709\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 8\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def quaternion_to_angle_deg(q1, q2):\n",
        "    q1 = q1 / (np.linalg.norm(q1) + 1e-8)\n",
        "    q2 = q2 / (np.linalg.norm(q2) + 1e-8)\n",
        "    dot = np.clip(np.dot(q1, q2), -1.0, 1.0)\n",
        "    angle_rad = 2 * np.arccos(abs(dot))\n",
        "    return np.degrees(angle_rad)\n",
        "\n",
        "\n",
        "# Cargar el mejor modelo guardado\n",
        "checkpoint = torch.load(best_model_path, map_location=DEVICE)\n",
        "best_model = PoseNetLight(pretrained=False).to(DEVICE)\n",
        "best_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "best_model.eval()\n",
        "\n",
        "print(\"Mejor modelo cargado:\")\n",
        "print(\"  Época:\", checkpoint[\"epoch\"])\n",
        "print(\"  Val Loss:\", checkpoint[\"val_loss\"])\n",
        "\n",
        "# Seleccionar dataset a evaluar\n",
        "eval_dataset = val_dataset  # cambia a test_dataset cuando lo tengas\n",
        "eval_loader = DataLoader(eval_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
        "\n",
        "print(f\"Evaluando en split con {len(eval_dataset)} muestras...\")\n",
        "\n",
        "# Escala para desnormalizar a metros\n",
        "pos_scale_np = POS_SCALE.cpu().numpy()\n",
        "\n",
        "transl_errors = []\n",
        "transl_err_x = []\n",
        "transl_err_y = []\n",
        "transl_err_z = []\n",
        "rot_errors_deg = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, poses_gt in eval_loader:\n",
        "        images = images.to(DEVICE)\n",
        "        poses_gt = poses_gt.to(DEVICE)\n",
        "\n",
        "        poses_pred = best_model(images)\n",
        "\n",
        "        # separar posición y cuaterniones (OJO: posición está normalizada)\n",
        "        t_pred_n = poses_pred[:, :3].cpu().numpy()\n",
        "        q_pred   = poses_pred[:, 3:].cpu().numpy()\n",
        "        t_gt_n   = poses_gt[:, :3].cpu().numpy()\n",
        "        q_gt     = poses_gt[:, 3:].cpu().numpy()\n",
        "\n",
        "        # desnormalizar a metros\n",
        "        t_pred = t_pred_n * pos_scale_np\n",
        "        t_gt   = t_gt_n * pos_scale_np\n",
        "\n",
        "        # normalizar cuaterniones\n",
        "        q_pred = q_pred / (np.linalg.norm(q_pred, axis=1, keepdims=True) + 1e-8)\n",
        "        q_gt   = q_gt   / (np.linalg.norm(q_gt,   axis=1, keepdims=True) + 1e-8)\n",
        "\n",
        "        # errores de traslación en metros\n",
        "        diff = t_pred - t_gt\n",
        "        dist = np.linalg.norm(diff, axis=1)\n",
        "\n",
        "        transl_errors.extend(dist)\n",
        "        transl_err_x.extend(np.abs(diff[:, 0]))\n",
        "        transl_err_y.extend(np.abs(diff[:, 1]))\n",
        "        transl_err_z.extend(np.abs(diff[:, 2]))\n",
        "\n",
        "        # errores de rotación\n",
        "        for qp, qg in zip(q_pred, q_gt):\n",
        "            rot_errors_deg.append(quaternion_to_angle_deg(qp, qg))\n",
        "\n",
        "transl_errors = np.array(transl_errors)\n",
        "transl_err_x = np.array(transl_err_x)\n",
        "transl_err_y = np.array(transl_err_y)\n",
        "transl_err_z = np.array(transl_err_z)\n",
        "rot_errors_deg = np.array(rot_errors_deg)\n",
        "\n",
        "print(\"\\n====================================================\")\n",
        "print(\"          📊 RESULTADOS DE EVALUACIÓN\")\n",
        "print(\"====================================================\\n\")\n",
        "\n",
        "print(f\"Total muestras evaluadas: {len(transl_errors)}\")\n",
        "\n",
        "print(\"\\n--- TRASLACIÓN (m) ---\")\n",
        "print(f\"Error medio (L2):        {np.mean(transl_errors):.4f} m\")\n",
        "print(f\"Error mediano:           {np.median(transl_errors):.4f} m\")\n",
        "print(f\"RMSE:                    {np.sqrt(np.mean(transl_errors**2)):.4f} m\")\n",
        "print(f\"Error medio |dx|:        {np.mean(transl_err_x):.4f} m\")\n",
        "print(f\"Error medio |dy|:        {np.mean(transl_err_y):.4f} m\")\n",
        "print(f\"Error medio |dz|:        {np.mean(transl_err_z):.4f} m\")\n",
        "\n",
        "print(\"\\n--- ORIENTACIÓN (grados) ---\")\n",
        "print(f\"Error angular medio:     {np.mean(rot_errors_deg):.3f}°\")\n",
        "print(f\"Error angular mediano:   {np.median(rot_errors_deg):.3f}°\")\n",
        "print(f\"Error angular RMS:       {np.sqrt(np.mean(rot_errors_deg**2)):.3f}°\")\n",
        "\n",
        "print(\"\\n====================================================\")\n",
        "print(\"Evaluación completada.\")\n",
        "print(\"====================================================\")\n"
      ],
      "metadata": {
        "id": "oO0jD-0MH3yA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "873dcfb7-007b-4c20-c784-f59b4d2ecf54"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mejor modelo cargado:\n",
            "  Época: 24\n",
            "  Val Loss: 5.656887865089709\n",
            "Evaluando en split con 2057 muestras...\n",
            "\n",
            "====================================================\n",
            "          📊 RESULTADOS DE EVALUACIÓN\n",
            "====================================================\n",
            "\n",
            "Total muestras evaluadas: 2057\n",
            "\n",
            "--- TRASLACIÓN (m) ---\n",
            "Error medio (L2):        5.7236 m\n",
            "Error mediano:           5.6072 m\n",
            "RMSE:                    5.9959 m\n",
            "Error medio |dx|:        5.3094 m\n",
            "Error medio |dy|:        1.7095 m\n",
            "Error medio |dz|:        0.2678 m\n",
            "\n",
            "--- ORIENTACIÓN (grados) ---\n",
            "Error angular medio:     66.970°\n",
            "Error angular mediano:   33.161°\n",
            "Error angular RMS:       93.451°\n",
            "\n",
            "====================================================\n",
            "Evaluación completada.\n",
            "====================================================\n"
          ]
        }
      ]
    }
  ]
}